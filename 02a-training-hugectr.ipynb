{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Training Large Deep Learning Recommender Models with NVIDIA HugeCTR and Vertex AI Training\n",
    "\n",
    "This notebook demonstrates how to use Vertex AI Training to operationalize training and hyperparameter tuning of large scale deep learning models developed with NVIDIA HugeCTR framework.\n",
    "\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "- Building a custom Vertex training container derived from NVIDIA NGC Merlin Training image\n",
    "- Configuring, submitting and monitoring a Vertex custom training job\n",
    "- Configuring, submitting and monitoring a Vertex hyperparameter tuning job\n",
    "- Retrieving and analyzing results of a hyperparameter tuning job\n",
    "\n",
    "The deep learning model used in this sample is [DeepFM](https://arxiv.org/abs/1703.04247) - a Factorization-Machine based Neural Network for CTR Prediction. The HugeCTR implementation of this model used in this notebook has been configured for the [Criteo dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/). \n",
    "\n",
    "## Model implementation\n",
    "\n",
    "[HugeCTR](https://github.com/NVIDIA-Merlin/HugeCTR) is NVIDIA's GPU-accelerated, highly scalable recommender framework. We highly encourage reviewing the [HugeCTR User Guide](https://github.com/NVIDIA-Merlin/HugeCTR/blob/master/docs/hugectr_user_guide.md) before proceeding with this notebook.\n",
    "\n",
    "NVIDIA HugeCTR facilitates highly scalable implementations of leading deep learning recommender models including Google's [Wide and Deep](https://arxiv.org/abs/1606.07792), Facebook's [DLRM](https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/), and the [DeepFM](https://arxiv.org/abs/1703.04247) model used in this notebook.\n",
    "\n",
    "A unique feature of HugeCTR is support for model-parallel embedding tables. Applying model-parallelizm for embedding tables enables massive scalability. Industrial grade deep learning recommendation systems most often employ very large embedding tables. User and item embedding tables - cornerstones of any recommender - can easily exceed tens of millions of rows. Without model-parallelizm it would be impossible to fit embedding tables this large in the memory of a single device - especially when using large embeddng vectors. In HugeCTR, embedding tables can span device memory across multiple GPUs on a single node or even across GPUs in a large distributed cluster. \n",
    "\n",
    "HugeCTR supports multiple model-parallelizm configurations for embedding tables. Refer to [the HugeCTR API reference](https://github.com/NVIDIA-Merlin/HugeCTR/blob/master/docs/hugectr_layer_book.md#embedding-types-detail) for detailed descriptions. In the DeepFM implementation used in this notebook, we utilize the `LocalizedSlotSparseEmbeddingHash` embedding type. In this embedding type, an embedding table is segmented into multiple slots or feature fields. Each slot stores embeddings for a single categorical feature. A given slot is allocated to a single GPU and it does not span multiple GPUs. However; in a multi GPU environment different slots can be allocated to different GPUs. \n",
    "\n",
    "The following diagram demonstrates an example configuration on a single node with multiple GPU - a hardware topology used by Vertex jobs in this notebook.\n",
    "\n",
    "\n",
    "<img src=\"./images/deepfm.png\" alt=\"Model parallel embeddings\" />\n",
    "\n",
    "\n",
    "The Criteo dataset has 24 categorical features so there are 24 slots in the embedding table. Cardinalities of categorical variables vary from tens of milions to low teens so the dimensions of slots vary accordingly. Each slot in the embedding table utilizes an embedding vector of the same size. Note that the distribution of slots across GPUs is handled by HugeCTR; you don't have to explicitly pin a slot to a GPU.\n",
    "\n",
    "Dense layers of the DeepFM models are replicated on all GPUs using a canonical data-parallel pattern. \n",
    "\n",
    "A choice of an optimizer is critical when training large deep learning recommender systems. Different optimizers may result in significantly different convergence rates impacting both time (cost) to train and a final model performance. Since large recommender systems are often retrained on frequent basis minimizing time to train is one of the key design objectives for a training workflow. In this notebook we use the Adam optimizer that has been proved to work well with many deep learning recommeder system architectures.\n",
    "\n",
    "You can find the code that implements the DeepFM model in the `src/training/hugectr/trainer/model.py` file. \n",
    "\n",
    "\n",
    "## Training workflow\n",
    "\n",
    "The training workflow has been optimized for Vertex AI Training. \n",
    "- Google Cloud Storage (GCS) and Vertex Training GCS Fuse are used for accessing training and validation data\n",
    "- A single node, multiple GPUs worker pool is used for Vertex Training jobs.\n",
    "- Training code has been instrumented to support hyperparameter tuning using Vertex Training Hyperparameter Tuning Job. \n",
    "\n",
    "You can find the code that implements the training workflow in the `src/training/hugectr/trainer/task.py` file.\n",
    "\n",
    "### Training data access\n",
    "\n",
    "Large deep learning  recommender systems are trained on massive datasets often hundreds of terabytes in size. Maintaining high-throughput when streaming training data to GPU workers is of critical importance. HugeCTR features a highly efficient multi-threaded data reader that parallelizes data reading and model computations. The reader accesses training data through a file system interface. The reader cannot directly access object storage systems like Google Cloud Storage, which is a canonical storage system for large scale training and validation datasets in Vertex AI Training. To expose Google Cloud Storage through a file system interface, the notebook uses an integrated feature of Vertex AI  - Google Cloud Storage FUSE. Vertex AI GCS FUSE provides a high performance file system interface layer to GCS that is self-tuning and requires minimal configuration. The following diagram depicts the training data access configuration:\n",
    "\n",
    "<img src=\"./images/gcsfuse.png\" alt=\"GCS Fuse\" />\n",
    "\n",
    "\n",
    "### Vertex AI Training worker pool configuration\n",
    "\n",
    "HugeCTR supports both single node, multiple GPU configurations and multiple node, multiple GPU distributed cluster topologies. In this sample, we use a single node, multiple GPU configuration. Due to the computational complexity of modern deep learning recommender models we recommend using Vertex Training A2 series machines for large models implemented with HugeCTR. The A2 machines can be configured with up to 16 A100 GPUs, 96 vCPUs, and 1,360GBs RAM. Each A100 GPU has 40GB of device memory. These are powerful configurations that can handle complex models with large embeddings.\n",
    "\n",
    "In this sample we use the `a2-highgpu-4g` machine type. \n",
    "\n",
    "Both custom training and hyperparameter tuning Vertex AI jobs demonstrated in this notebook are configured to use a [custom training container](https://cloud.google.com/vertex-ai/docs/training/containers-overview). The container is a derivative of [NVIDIA NGC Merlin training container](https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-training). \n",
    "\n",
    "### HugeCTR hyperparameter tuning with Vertex AI\n",
    "\n",
    "The training module has been instrumented to support [hyperparameter tuning with Vertex AI](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview). The custom container includes the [cloudml-hypertune package](https://github.com/GoogleCloudPlatform/cloudml-hypertune), which is used to report the results of model evaluations to Vertex AI hypertuning service. The following diagram depicts the training flow implemeted by the training module.\n",
    "\n",
    "\n",
    "<img src=\"./images/hugectrtrainer.png\" alt=\"Training regimen\" style=\"height:15%; width:40%\"/>\n",
    "\n",
    "\n",
    "Note that as of HugeCTR v3.2 release, the `hugectr.inference.InferenceSession.evaluate` method used in the trainer module only supports the *AUC* evaluation metric.\n",
    "\n",
    "\n",
    "## Notebook flow\n",
    "\n",
    "This notebook assumes that the Criteo dataset has been preprocessed using the preprocessing workflow detailed in the `01-dataset-preprocessing.ipynb` notebook and the resulting Parquet training and validation splits, and the processed data schema have been stored in Google Cloud Storage.\n",
    "\n",
    "As you walk through the notebook you will execute the following steps:\n",
    "- Configure notebook environment settings like GCP project, compute region, and the GCS locations of training and validation data splits.\n",
    "- Build a custom Vertex training container based on NVIDIA NGC Merlin Training container\n",
    "- Configure and submit a Vertex custom training job\n",
    "- Configure and submit a Vertex hyperparameter training job\n",
    "- Retrieve the results of the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c864e5c-0319-4a2e-804b-fcac44e7e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import nvtabular as nvt\n",
    "import shutil\n",
    "\n",
    "from nvtabular.columns.schema import ColumnSchema, Schema\n",
    "from nvtabular.tags import Tags\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ecf96d-4649-4772-bcc3-b6170f3f8c7e",
   "metadata": {},
   "source": [
    "## Configure notebook settings\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a GCP compute region, a Vertex AI service account and a Vertex AI staging bucket. You also set the locations of training and validation splits, and their schema as created in the `01-dataset-preprocessing.ipynb` notebook.\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment.\n",
    "\n",
    "### Set project, region, and Vertex AI settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb263c2-b3c2-4739-911c-cc88470241bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "VERTEX_STAGING_BUCKET = 'gs://jk-vertex-merlin'\n",
    "VERTEX_SA = 'vertex-sa@jk-mlops-dev.iam.gserviceaccount.com'\n",
    "LOCAL_STAGING_PATH = '/home/jupyter/staging'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bb72e-e164-43c3-9487-1c779abb7cc8",
   "metadata": {},
   "source": [
    "### Set paths to training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d3d313-276e-4871-88ad-18a505a1c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'gs://jk-criteo-bucket/criteo_processed_parquet'\n",
    "TRAIN_DATA = f'{DATA_ROOT}/train/_file_list.txt'\n",
    "VALID_DATA = f'{DATA_ROOT}/valid/_file_list.txt'\n",
    "SCHEMA_PATH = f'{DATA_ROOT}/train/schema.pbtxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0720d5f-a368-4c44-b31e-f3869f02ec39",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6370bb-96a3-4432-8899-3b4b723fe5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=VERTEX_STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845048c6-68cf-435b-8ef5-8a3281c8632d",
   "metadata": {},
   "source": [
    "### Prepare a local staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8838523-b876-4d44-8598-8afc0f929e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(LOCAL_STAGING_PATH):\n",
    "    shutil.rmtree(LOCAL_STAGING_PATH)\n",
    "os.makedirs(LOCAL_STAGING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69c76b-9720-4168-8073-243d9d9ae06b",
   "metadata": {},
   "source": [
    "## Submit a Vertex custom training job\n",
    "\n",
    "In this section of the notebook you define, submit and monitor a Vertex custom training job. As noted in the introduction, the job uses a custom training container that is a derivative of [NVIDIA NGC Merlin training container image](https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-training). The custom container image packages the training module which includes a DeepFM model definition - `src/training/hugectr/model.py` and a training and evaluation workflow\" - `src/training/hugectr/task.py`. The custom container image also installs the `cloudml-hypertune` package for integration with Vertex AI hypertuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de3552-4302-4d07-a830-3aeb1bdf4ca8",
   "metadata": {},
   "source": [
    "### Build a custom training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08f9980-ab36-4879-ab09-8df702032912",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'hugectr_deepfm'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT}/{IMAGE_NAME}'\n",
    "DOCKERFILE = 'src/Dockerfile.hugectr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbcc6f2-0ae1-4ebd-9dfd-75677294025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM nvcr.io/nvidia/merlin/merlin-training:21.09\n",
      "\n",
      "RUN pip3 install cloudml-hypertune\n",
      "\n",
      "WORKDIR /src\n",
      "\n",
      "COPY training/hugectr/ ./\n"
     ]
    }
   ],
   "source": [
    "! cat {DOCKERFILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad9f5ac6-0372-4601-a209-9b4a0123c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! cp {DOCKERFILE} src/Dockerfile\n",
    "#! gcloud builds submit --tag {IMAGE_URI} src\n",
    "#! rm src/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546fe200-8638-4cda-936f-bd091a2c9cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  476.7kB\n",
      "Step 1/4 : FROM nvcr.io/nvidia/merlin/merlin-training:21.09\n",
      " ---> 8f6ef763d770\n",
      "Step 2/4 : RUN pip3 install cloudml-hypertune\n",
      " ---> Using cache\n",
      " ---> 7be51676d034\n",
      "Step 3/4 : WORKDIR /src\n",
      " ---> Using cache\n",
      " ---> 8e80619f2291\n",
      "Step 4/4 : COPY training/hugectr/ ./\n",
      " ---> Using cache\n",
      " ---> 623c6915619a\n",
      "Successfully built 623c6915619a\n",
      "Successfully tagged gcr.io/jk-mlops-dev/hugectr_deepfm:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/hugectr_deepfm]\n",
      "\n",
      "\u001b[1Bed57b667: Preparing \n",
      "\u001b[1B8170b157: Preparing \n",
      "\u001b[1B96e6939c: Preparing \n",
      "\u001b[1B1f36f93d: Preparing \n",
      "\u001b[1Be0fa48ac: Preparing \n",
      "\u001b[1Bd4cd8fbb: Preparing \n",
      "\u001b[1B2cf6a026: Preparing \n",
      "\u001b[1B26a95f43: Preparing \n",
      "\u001b[1Bba5d6668: Preparing \n",
      "\u001b[1Ba9641b18: Preparing \n",
      "\u001b[1B42012c23: Preparing \n",
      "\u001b[1Bbd3e801f: Preparing \n",
      "\u001b[1B7039f00c: Preparing \n",
      "\u001b[1B023e019c: Preparing \n",
      "\u001b[1Bf9e4afbc: Preparing \n",
      "\u001b[1Be9115178: Preparing \n",
      "\u001b[1B220871e3: Preparing \n",
      "\u001b[1B76810591: Preparing \n",
      "\u001b[1B7b7db32e: Preparing \n",
      "\u001b[1B176606fb: Preparing \n",
      "\u001b[1B44765ddc: Preparing \n",
      "\u001b[1B295911d7: Preparing \n",
      "\u001b[1B1a643992: Preparing \n",
      "\u001b[1B6f8bf009: Preparing \n",
      "\u001b[1Bd3ad3992: Preparing \n",
      "\u001b[1Bd0bdb6b2: Preparing \n",
      "\u001b[1B59647bc1: Preparing \n",
      "\u001b[1B9b0b029e: Preparing \n",
      "\u001b[1B3c6245a7: Preparing \n",
      "\u001b[1B9f6effa5: Preparing \n",
      "\u001b[1B43806c89: Preparing \n",
      "\u001b[1B31c5510f: Preparing \n",
      "\u001b[1B4c5d4460: Preparing \n",
      "\u001b[1Be3d1aa10: Preparing \n",
      "\u001b[29Bcf6a026: Waiting g \n",
      "\u001b[21B9115178: Waiting g \n",
      "\u001b[1B58a72969: Preparing \n",
      "\u001b[29B9641b18: Waiting g \n",
      "\u001b[1B109a3ea2: Preparing \n",
      "\u001b[30B2012c23: Waiting g \n",
      "\u001b[1Bdeda2561: Preparing \n",
      "\u001b[31Bd3e801f: Waiting g \n",
      "\u001b[1B3117f7ee: Preparing \n",
      "\u001b[32B039f00c: Waiting g \n",
      "\u001b[27Bb7db32e: Waiting g \n",
      "\u001b[1Bd1994cdb: Preparing \n",
      "\u001b[34B23e019c: Waiting g \n",
      "\u001b[28B4765ddc: Waiting g \n",
      "\u001b[1B54a6be62: Preparing \n",
      "\u001b[9B7e454744: Waiting g \n",
      "\u001b[30B95911d7: Waiting g \n",
      "\u001b[20Bc5d4460: Waiting g \n",
      "\u001b[20B3d1aa10: Waiting g \n",
      "\u001b[19Bfc769b7: Waiting g \n",
      "\u001b[30B0bdb6b2: Waiting g \n",
      "\u001b[1B664f5ac5: Preparing \n",
      "\u001b[1B6513dba7: Preparing \n",
      "\u001b[12B8e05279: Waiting g \n",
      "\u001b[1B35ff9141: Preparing \n",
      "\u001b[30B3806c89: Waiting g \n",
      "\u001b[24Bb179a54: Waiting g \n",
      "\u001b[11Bb4d2292: Waiting g \n",
      "\u001b[7B6513dba7: Waiting g \n",
      "\u001b[9B664f5ac5: Waiting g \n",
      "\u001b[13Bce6b542: Waiting g \n",
      "\u001b[58Ba5d6668: Waiting g \n",
      "\u001b[1Bd201b9e7: Preparing \n",
      "\u001b[11B47ab1c6: Waiting g \n",
      "\u001b[6B4e358856: Waiting g \n",
      "\u001b[9Bf3851c58: Waiting g \n",
      "\u001b[12B16378c2: Waiting g \n",
      "\u001b[1Bac1c2c2d: Preparing \n",
      "\u001b[1B73877dab: Preparing \n",
      "\u001b[5Ba3065d69: Waiting g \n",
      "\u001b[1B19677a82: Preparing \n",
      "\u001b[5Bac1c2c2d: Waiting g \n",
      "\u001b[1B37aa9a0a: Preparing \n",
      "\u001b[6B73877dab: Waiting g \n",
      "\u001b[5B19677a82: Waiting g \n",
      "\u001b[4B37aa9a0a: Waiting g \n",
      "\u001b[1B5d7876c0: Preparing \n",
      "\u001b[1Bf6dd0bf3: Preparing \n",
      "\u001b[1B7995c44f: Preparing \n",
      "\u001b[1B96a127ae: Preparing \n",
      "\u001b[1Bf69cc2e7: Preparing \n",
      "\u001b[1B6a01493d: Preparing \n",
      "\u001b[79Ba5d6668: Pushed   602.8MB/598.5MB1A\u001b[2K\u001b[77A\u001b[2K\u001b[75A\u001b[2K\u001b[74A\u001b[2K\u001b[72A\u001b[2K\u001b[68A\u001b[2K\u001b[66A\u001b[2K\u001b[65A\u001b[2K\u001b[62A\u001b[2K\u001b[63A\u001b[2K\u001b[61A\u001b[2K\u001b[59A\u001b[2K\u001b[56A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[51A\u001b[2K\u001b[49A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[41A\u001b[2K\u001b[39A\u001b[2K\u001b[36A\u001b[2K\u001b[33A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[27A\u001b[2K\u001b[26A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[87A\u001b[2K\u001b[87A\u001b[2K\u001b[70A\u001b[2K\u001b[70A\u001b[2K\u001b[70A\u001b[2K\u001b[70A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[87A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2K\u001b[79A\u001b[2Klatest: digest: sha256:933ae816428078e79a6fc300fff0172b3f69a5922b5643dcfd248dbd5a68a468 size: 18610\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {IMAGE_URI} -f {DOCKERFILE} src\n",
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c8bd9-01f0-470e-8031-c72fabeae6ac",
   "metadata": {},
   "source": [
    "### Configure a custom training job\n",
    "\n",
    "The training module accepts a set of parameters that allow you to fine tune the DeepFM model implementation and configure the training workflow. Most of the  paramaters exposed by the training module map directly to the settings used in [HugeCTR Python Inteface](https://github.com/NVIDIA-Merlin/HugeCTR/blob/master/docs/python_interface.md#createsolver-method). \n",
    "\n",
    "`NUM_EPOCHS` and `MAX_ITERATIONS`: The training workflow can run in either an epoch mode or a non-epoch mode. When the constant `NUM_EPOCHS` is set to a value greater than zero the model will be trained on the `NUM_EPOCHS` number of full epochs, where an epoch is defined as a single pass through all examples in the training data. If `NUM_EPOCHS` is set to zero you must set `MAX_ITERATIONS` to a value greater than zero. `MAX_ITERATIONA` defines the number of batches to train the model on. When `NUM_EPOCHS` is greater than zero `MAX_ITERATIONS` is ignored.\n",
    "\n",
    "`EVAL_INTERVAL` and `EVAL_BATCHES`: The model will be evaluated every `EVAL_INTERVAL` training batches using the `EVAL_BATCHES` validation batches during the main training loop. In the current implementation the evaluation metric is `AUC`.\n",
    "\n",
    "`EVAL_BATCHES_FINAL`: After the main training loop completes, a final evaluation will be run using the `EVAL_BATCHES_FINAL`. The `AUC` value returned is reported to Vertex AI hypertuner.\n",
    "\n",
    "`DISPLAY_INTERVAL`: Training progress will be reported every `DISPLAY_INTERVAL` batches.\n",
    "\n",
    "`SNAPSHOT_INTERVAL`: When set to a value greater than zero, a snapshot will be saved every `SNAPSHOT_INTERVAL` batches.\n",
    "\n",
    "`PER_GPU_BATCH_SIZE`: Per GPU batch size. This value should be set through experimentation and depends on model architecture, training features, and GPU type. It is highly dependent on device memory available in a particular GPU. In our scenario - DeepFM, Criteo datasets, and A100 GPU - a batch size of 2048 works well.\n",
    "\n",
    "`LR`: The base learning rate for the HugeCTR solver.\n",
    "\n",
    "`DROPOUT_RATE`: The base dropout rate used in DeepFM dense layers.\n",
    "\n",
    "`NUM_WORKERS`: The number of HugeCTR data reader workers that concurrently load data. This value should be estimated through experimentation. **TBD** This is a per GPU value. The default, which works well on A100 GPUs is 12. For the optimal performance `NUM_WORKERS` should be aligned with the number of files (shards) in the training data. \n",
    "\n",
    "`SLOT_SIZE_ARRAY`: The cardinality array of categorical features. This value is required when using Parquet format for training data, which we do in this sample. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0180acd-e969-4523-baf6-f4c30b5d0e5c",
   "metadata": {},
   "source": [
    "#### Set HugeCTR model and trainer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "977f768e-3d0d-4367-9e12-5e20cbc3aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MODULE = 'trainer.task'\n",
    "\n",
    "NUM_EPOCHS = 0\n",
    "MAX_ITERATIONS = 50000\n",
    "EVAL_INTERVAL = 1000\n",
    "EVAL_BATCHES = 500\n",
    "EVAL_BATCHES_FINAL = 2500\n",
    "DISPLAY_INTERVAL = 200\n",
    "SNAPSHOT_INTERVAL = 0\n",
    "PER_GPU_BATCHSIZE = 2048\n",
    "LR = 0.001\n",
    "DROPOUT_RATE = 0.5\n",
    "NUM_WORKERS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111e199-25ff-42b9-aac5-875ca32cd8a2",
   "metadata": {},
   "source": [
    "#### Retrieve cardinalities for categorical columns\n",
    "\n",
    "To set the `SLOT_SIZE_ARRAY` parameter we need to retrieve cardinalities of all categorical variables in the training data. This information is captured in the dataset schema file - `schema.pbtxt` - generated by the data preprocessing workflow. The schema file uses a protocol buffer format and can be parsed using the `nvtabular.columns.schema.ColumnSchema`, `nvtabular.columns.schema.Schema` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9719b18b-fedf-40d3-8f4a-5f59b38d0d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://jk-criteo-bucket/criteo_processed_parquet/train/schema.pbtxt...\n",
      "/ [1 files][ 20.8 KiB/ 20.8 KiB]                                                \n",
      "Operation completed over 1 objects/20.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "LOCAL_SCHEMA_PATH = f'{LOCAL_STAGING_PATH}/schema.pbtxt'\n",
    "\n",
    "!gsutil cp {SCHEMA_PATH} {LOCAL_SCHEMA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac85ff7-8dd0-4692-9c48-d8c255ce75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema.load_protobuf(LOCAL_SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f048e4-0846-4a93-84c9-677258220741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[18792578,35176,17091,7383,20154,4,7075,1403,63,12687136,1054830,297377,11,2209,10933,113,4,972,15,19550853,5602712,16779972,375290,12292,101,35]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_cardinalities(schema):\n",
    "    cardinalities = {key: value.properties['embedding_sizes']['cardinality'] \n",
    "                     for key, value in schema.column_schemas.items()\n",
    "                     if Tags.CATEGORICAL in value.tags}\n",
    "    \n",
    "    return cardinalities\n",
    "    \n",
    "    \n",
    "cardinalities = retrieve_cardinalities(schema)\n",
    "SLOT_SIZE_ARRAY = json.dumps(\n",
    "    [int(cardinality) for cardinality in cardinalities.values()]).replace(' ', '')\n",
    "SLOT_SIZE_ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa0148-c742-49e3-9e7e-758ab513d555",
   "metadata": {},
   "source": [
    "#### Set training node configuration\n",
    "\n",
    "As described in the overview, we use a single node, multiple GPU worker pool configuration. For a complex deep learning model like DeepFM, we recommend using A2 machines that are equipped with A100 GPUs. \n",
    "\n",
    "In this sample we use a `a2-highgpu-4g` machine. For production systems, you may consider even more powerful configurations - `a2-highgpu-8g` with 8 A100 GPUs  or `a2-megagpu-16g` with 16 A100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcdcf631-d25a-4190-9429-1c19ad70ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'a2-highgpu-4g'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "ACCELERATOR_NUM = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece569ba-9faf-46ff-be12-4ae1c64af83b",
   "metadata": {},
   "source": [
    "#### Configure worker pool specifications\n",
    "\n",
    "In this cell we configure a worker pool specification for a Vertex Custom Training job. Refer to [Vertex AI Training documentation](https://cloud.google.com/vertex-ai/docs/training/create-custom-job) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a5bfc83-506a-459a-89d9-6a6963722a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = PER_GPU_BATCHSIZE * ACCELERATOR_NUM\n",
    "gpus = json.dumps([list(range(ACCELERATOR_NUM))]).replace(' ','')\n",
    "                 \n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": ACCELERATOR_NUM,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"-m\", TRAINING_MODULE],\n",
    "            \"args\": [\n",
    "                '--batchsize=' + str(batchsize),\n",
    "                '--train_data=' + TRAIN_DATA.replace('gs://', '/gcs/'), \n",
    "                '--valid_data=' + VALID_DATA.replace('gs://', '/gcs/'),\n",
    "                '--slot_size_array=' + SLOT_SIZE_ARRAY,\n",
    "                '--max_iter=' + str(MAX_ITERATIONS),\n",
    "                '--max_eval_batches=' + str(EVAL_BATCHES),\n",
    "                '--eval_batches=' + str(EVAL_BATCHES_FINAL),\n",
    "                '--dropout_rate=' + str(DROPOUT_RATE),\n",
    "                '--lr=' + str(LR),\n",
    "                '--num_workers=' + str(NUM_WORKERS),\n",
    "                '--num_epochs=' + str(NUM_EPOCHS),\n",
    "                '--eval_interval=' + str(EVAL_INTERVAL),\n",
    "                '--snapshot=' + str(SNAPSHOT_INTERVAL),\n",
    "                '--display_interval=' + str(DISPLAY_INTERVAL),\n",
    "                '--gpus=' + gpus,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9e340-5733-42fe-bceb-8d338e31bec0",
   "metadata": {},
   "source": [
    "### Submit and monitor the job\n",
    "\n",
    "When submitting a training job using the `aiplatfom.CustomJob` API you can configure the `job.run` function to block till the job completes or return control to the notebook immediately after the job is submitted. You control it with the `sync` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1301906-919b-4dc7-af5c-402e19c8b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/895222332033/locations/us-central1/customJobs/77412215665197056\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/895222332033/locations/us-central1/customJobs/77412215665197056')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/77412215665197056?project=895222332033\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/77412215665197056 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/895222332033/locations/us-central1/customJobs/77412215665197056\n"
     ]
    }
   ],
   "source": [
    "job_name = 'HUGECTR_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = f'{VERTEX_STAGING_BUCKET}/job_dir/{job_name}'\n",
    "\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=base_output_dir\n",
    ")\n",
    "job.run(\n",
    "    sync=True,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68064a1a-c71a-4b86-82a4-7a723c0b55e2",
   "metadata": {},
   "source": [
    "## Submit and monitor a Vertex hyperparameter tuning job\n",
    "\n",
    "In this section of the notebook, you configure, submit and monitor a [Vertex AI raining hyperparameter tuning](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) job. We will demonstrate how to use Vertex Training hyperparameter tuning to find optimal values for the base learning rate and the dropout ratio. This example can be easily extended to other parameters - e.g. the batch size or even the optimizer type.\n",
    "\n",
    "As noted in the overview, the training module has been instrumented to integrate with Vertex AI Training hypertuning. After the final evaluation is completed, the AUC value calculated on the `EVAL_BATCHES_FINAL` number of batches from the validation dataset is reported to Vertex AI Training using the `report_hyperparameter_tuning_metric`. When the training module is executed in the context of a Vertex Custom Job this code path has no effect. When used with a Vertex AI Training hyperparameter job, the job is configured to use the AUC as a metric to optimize.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704e299-c23f-4a3c-a097-2ba3ce42e5b9",
   "metadata": {},
   "source": [
    "### Configure a hyperparameter job\n",
    "\n",
    "To prepare a Vertex Training hyperparameter tuning job you need to configure a worker pool specificatin and a hyperparameter study configuration. Configuring a worker pool is virtually the same as with a Custom Job. The only difference is that you don't explicitly pass values of hyperparameters being tuned to a training container. They will be provided by the hyperparameter tuning service. In our case, we don't set the `lr` and `dropout_ratio`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353bdec-e3e7-410a-a723-8c002f3c4f57",
   "metadata": {},
   "source": [
    "#### Set HugeCTR model and trainer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f20ce01-d020-4d8a-bdf0-0102bd9d5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MODULE = 'trainer.task'\n",
    "\n",
    "NUM_EPOCHS = 0\n",
    "MAX_ITERATIONS = 10000\n",
    "EVAL_INTERVAL = 1000\n",
    "EVAL_BATCHES = 500\n",
    "EVAL_BATCHES_FINAL = 2500\n",
    "DISPLAY_INTERVAL = 200\n",
    "SNAPSHOT_INTERVAL = 0\n",
    "WORKSPACE_SIZE_PER_GPU = 61\n",
    "PER_GPU_BATCHSIZE = 2048\n",
    "NUM_WORKERS = 12\n",
    "SLOT_SIZE_ARRAY = json.dumps(\n",
    "    [int(cardinality) for cardinality in cardinalities.values()]).replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc77717-8200-40d4-a093-7a9204bcebd8",
   "metadata": {},
   "source": [
    "#### Set training node configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e813368a-4e56-4753-9a25-ea068565a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'a2-highgpu-4g'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "ACCELERATOR_NUM = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac41445-6dc1-45e5-bc17-98667056af2f",
   "metadata": {},
   "source": [
    "#### Configure worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51db43af-3ed1-47ee-baca-e17677f7e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = PER_GPU_BATCHSIZE * ACCELERATOR_NUM\n",
    "gpus = json.dumps([list(range(ACCELERATOR_NUM))]).replace(' ','')\n",
    "                 \n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": ACCELERATOR_NUM,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"-m\", TRAINING_MODULE],\n",
    "            \"args\": [\n",
    "                '--batchsize=' + str(batchsize),\n",
    "                '--train_data=' + TRAIN_DATA.replace('gs://', '/gcs/'), \n",
    "                '--valid_data=' + VALID_DATA.replace('gs://', '/gcs/'),\n",
    "                '--slot_size_array=' + SLOT_SIZE_ARRAY,\n",
    "                '--max_iter=' + str(MAX_ITERATIONS),\n",
    "                '--max_eval_batches=' + str(EVAL_BATCHES),\n",
    "                '--eval_batches=' + str(EVAL_BATCHES_FINAL),\n",
    "                '--num_workers=' + str(NUM_WORKERS),\n",
    "                '--num_epochs=' + str(NUM_EPOCHS),\n",
    "                '--eval_interval=' + str(EVAL_INTERVAL),\n",
    "                '--snapshot=' + str(SNAPSHOT_INTERVAL),\n",
    "                '--display_interval=' + str(DISPLAY_INTERVAL),\n",
    "                '--workspace_size_per_gpu=' + str(WORKSPACE_SIZE_PER_GPU),\n",
    "                '--gpus=' + gpus,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507bcd7-7789-4630-b4c6-69d0c8aa9c93",
   "metadata": {},
   "source": [
    "#### Configure hyperparameter and metric specs\n",
    "\n",
    "To configure a hyperparameter study you need to define a metric to optimize, an optimization goal, and a set of configurations for hyperparameters to tune.\n",
    "\n",
    "In our case the metric is AUC, the optimization goal is to maximize AUC, and the hyperparameters are `lr`, and `dropout_rate`. Notice that you have to match the name of the metric with the name used to report the metric in the training module. You also have to match the names of the hyperparameters with the respective names of command line parameters in your training container.\n",
    "\n",
    "For each hyperparameter you specify a strategy to apply for sampling values from the hyperparameter's domain. For the `lr` hyperparameter we configure the tuning service to sample values from a continuous range between 0.001 to 0.01 using a logarithmic scale. For the `dropout_rate` we provide a list of discrete values to choose from.\n",
    "\n",
    "For more information about configuring a hyperparameter study refer to [Vertex AI Hyperparameter job configuration](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0425cdf-07bc-4cf5-b3f7-858fdbc40fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_spec = {'AUC': 'maximize'}\n",
    "\n",
    "parameter_spec = {\n",
    "    'lr': hpt.DoubleParameterSpec(min=0.001, max=0.01, scale='log'),\n",
    "    'dropout_rate': hpt.DiscreteParameterSpec(values=[0.4, 0.5, 0.6], scale=None),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbadf45-f41b-4c99-a620-fd73062b072d",
   "metadata": {},
   "source": [
    "### Submit and monitor the job\n",
    "\n",
    "We can now submit a hyperparameter tuning job. When submitting the job you specify a maximum number of trials to attempt and how many trials to run in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00435e4d-6833-4034-9bad-e2f46a1c75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating HyperparameterTuningJob\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob created. Resource name: projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704\n",
      "INFO:google.cloud.aiplatform.jobs:To use this HyperparameterTuningJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:hpt_job = aiplatform.HyperparameterTuningJob.get('projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704')\n",
      "INFO:google.cloud.aiplatform.jobs:View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5821753540376264704?project=895222332033\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/895222332033/locations/us-central1/hyperparameterTuningJobs/5821753540376264704 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job_name = 'HUGECTR_HTUNING_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = f'{VERTEX_STAGING_BUCKET}/job_dir/{job_name}'\n",
    "\n",
    "\n",
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=base_output_dir\n",
    ")\n",
    "\n",
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=job_name,\n",
    "    custom_job=custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=4,\n",
    "    parallel_trial_count=2,\n",
    "    search_algorithm=None)\n",
    "\n",
    "hp_job.run(\n",
    "    sync=True,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832fbaa-f32e-4bac-b53b-5b0639cd28bc",
   "metadata": {},
   "source": [
    "### Retrieve trial results\n",
    "\n",
    "After a hyperparameter tuning job completes you can retrieve the trial results from the job object. The results are returned as a list of trial records. To retrieve the trial with the best value of a metric - AUC - you need to scan through all trial records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcda9bf-3c5e-48ee-a75c-8dc197f96c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_job.trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f589c-4fc2-4a85-a56a-d099abb1853a",
   "metadata": {},
   "source": [
    "#### Find the best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e15b4-e033-42d8-9363-848d69557102",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = sorted(hp_job.trials, \n",
    "                    key=lambda trial: trial.final_measurement.metrics[0].value, \n",
    "                    reverse=True)[0]\n",
    "\n",
    "print(\"Best trial ID:\", best_trial.id)\n",
    "print(\"   AUC:\", best_trial.final_measurement.metrics[0].value)\n",
    "print(\"   LR:\", best_trial.parameters[1].value)\n",
    "print(\"   Dropout rate:\", best_trial.parameters[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9397c3-9f79-4f93-9691-bd40113fefa8",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251c1cc-46f9-4a0d-adee-0e6bb37b81e7",
   "metadata": {},
   "source": [
    "After completing this notebook you can proceed to the `03a-inference-triton-huygectr.ipynb` notebook that demonstrates how to deploy the DeepFM model trained in this notebook using NVIDIA Triton Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f1841-6bb7-4cfa-bee0-20c574ce206f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m80"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
