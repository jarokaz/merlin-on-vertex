{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NVTabular for large scale feature engineering on CSV files in Google Cloud Storage\n",
    "\n",
    "This notebook demonstrates how to do data preprocessing with NVIDIA NVTabular on Vertex AI Pipeline steps using Google Cloud Storage as the data source.  \n",
    "You will create a pipeline with the following steps:\n",
    " - Read CSV files from Google Cloud Storage (GCS)\n",
    " - Convert these files to parquet format and write to GCS\n",
    " - Define the DAG with transformation steps and create a Workflow\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output transformed parquet files to GCS\n",
    "\n",
    "The dataset used for this tutorial is the [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).  \n",
    "\n",
    "Architecture overview:\n",
    "\n",
    "<img src=\"./images/pipeline_1.png\" alt=\"Pipeline\" style=\"height: 60%; width:60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we dive into the details on how to execute the preprocessing pipeline, you need to accomplish the following steps:\n",
    " - Define variables: project ID, region, bucket name and location.\n",
    " - Build the container that will execute each preprocessing step\n",
    " - Import your pipeline and some libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, it is assumed that you already created your project and bucket in GCS.\n",
    "# Fill the following variables with the information you collected from the previous notebook.\n",
    "PROJECT_ID = 'renatoleite-mldemos' # ID of your project\n",
    "REGION = 'us-central1' # Region where the pipeline will be executed\n",
    "BUCKET = 'renatoleite-criteo-partial' # Bucket name to read the Criteo dataset\n",
    "LOCATION = 'us' # Location to Bigquery resources\n",
    "STAGING_BUCKET = f'gs://{BUCKET}/temp' # Temp location for Vertex AI Pipeline files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container image and push to gcr.io (Google Container Registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker image name and Dockerfile location\n",
    "IMAGE_NAME = 'nvt_preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERFILE = 'src/preprocessing'\n",
    "\n",
    "import os\n",
    "os.environ['IMAGE_URI'] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build can take up to 1 hour\n",
    "! gcloud builds submit --timeout \"2h\" --tag {IMAGE_URI} {DOCKERFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# Import components and pipeline definition\n",
    "from src.pipelines.pipeline_preprocessing_gcs import preprocessing_pipeline_gcs\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertex AI Pipelines definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Components\n",
    "A pipeline component is a self-contained set of code that performs one step in your ML workflow.  \n",
    "The components are defined as python functions in the file `src/preprocessing/kfp_components.py`.  \n",
    "Each component is annotated with Inputs and Outputs to keep track of lineage metadata.\n",
    "\n",
    "The `base_image` used to execute the components is the same docker image you built a few steps back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline steps configuration\n",
    "All the NVTabular preprocessing are executed as steps in the Pipeline.  \n",
    "Some steps require a more robust runtime configuration with more CPU, memory and GPU.  \n",
    "\n",
    "Each pipeline definition has its own runtime configurations that can be set directly to the component execution.  \n",
    "In Vertex AI Pipelines, you can set the amount of CPU, memory and GPU in the pipeline specification, like this:\n",
    "\n",
    "```\n",
    "component_being_executed.set_cpu_limit(\"8\") # Number of CPUs\n",
    "component_being_executed.set_memory_limit(\"32G\") # Memory quantity\n",
    "component_being_executed.set_gpu_limit(\"1\") # Number of GPUs\n",
    "component_being_executed.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-t4') # GPU type\n",
    "```\n",
    "\n",
    "More information on how to set these parameters [HERE](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#specify-machine-type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Convert CSV files to Parquet with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original Criteo dataset is in CSV format, but the recommended data format to run the NVTabular preprocessing task and get the best possible performance is Parquet; a compressed, column-oriented file structure format. While NVTabular also supports reading from CSV files, it can be over twice as slow as reading from Parquet.  \n",
    "\n",
    "This step in the pipeline will read the data from GCS, convert from CSV to PARQUET using the `nvtabular.Dataset.to_parquet` method, and write the converted data back to GCS.\n",
    "\n",
    "To convert the CSV to PARQUET you need to pass a dictionary mapping the column names to its data type.  \n",
    "This data will be used to instantiate the classe `nvtabular.Dataset` and consequently call the `.to_parquet` method.\n",
    "\n",
    "This dataset was two different types: `int32` and `hex` (hexadecimal strings that should be converted to int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Specify column dtypes. Note that \"hex\" means that\n",
    "# the values will be hexadecimal strings that should\n",
    "# be converted to int32\n",
    "col_dtypes = {}\n",
    "\n",
    "col_dtypes[\"label\"] = np.int32\n",
    "for x in [\"I\" + str(i) for i in range(1, 14)]:\n",
    "    col_dtypes[x] = np.int32\n",
    "for x in [\"C\" + str(i) for i in range(1, 27)]:\n",
    "    col_dtypes[x] = 'hex'\n",
    "\n",
    "return col_dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Create Workflow DAG definition for statistics calculation and data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the data, first we need to define a transformation pipeline as a DAG (directed acyclic graph).  \n",
    "Each transformation step in the transformation pipeline executes multiple calculations, called `ops`.  \n",
    "An op can be applied to a ColumnGroup from an overloaded \">>\" operator, which in turn returns a new ColumnGroup. A ColumnGroup is a list of column names as text.\n",
    "\n",
    "Example:\n",
    "features = [ column_name, ...] >> op1 >> op2 >> ...\n",
    "\n",
    "Here are some examples of ops implemented in NVTabular:\n",
    " - Filtering outliers or missing values, or creating new features indicating that a value is missing;\n",
    " - Imputing and filling in missing data;\n",
    " - Discretization or bucketing of continuous features;\n",
    " - Creating features by splitting or combining existing features, for example, breaking down a date column into day-of-week, month-of-year, day-of-month features;\n",
    " - Normalizing numerical features to have zero mean and unit variance or applying transformations, for example with log transform;\n",
    " - Encoding discrete features using one-hot vectors or converting them to continuous integer indices.  \n",
    " \n",
    "The list of all ops can be found [HERE](https://nvidia.github.io/NVTabular/main/api/ops/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the DAG performs the following transformations:\n",
    " - Categorical features (columns that start with C): Apply [Categorify](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/categorify.html)\n",
    " - Continuous features (columns that start with I): Apply [FillMissing](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/fillmissing.html), [Clip](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/clip.html) and [Normalize](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/normalize.html) (in this order).\n",
    "\n",
    " The definition of the `nvt.Workflow` will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " It will be uploaded to GCS for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the transformation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Transformation pipeline\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(max_size=num_buckets)\n",
    "cat_features = cat_names >> categorify_op\n",
    "cont_features = cont_names >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "features = cat_features + cont_features + ['label']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dag_preprocessing.png\" alt=\"Pipeline\" style=\"height: 40%; width:40%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVTabular is designed to minimize the number of passes through the data. This is achieved with a lazy execution strategy. Data operations are not executed until an explicit apply phase.  \n",
    "In the first phase, these operations are only registered into the workflow. This allows NVTabular to optimize the collection of statistics that require iteration over the entire dataset.\n",
    "\n",
    "When processing terabyte-scale datasets, it is critical to plan this statistics-gathering phase as well as transformation phase carefully in advance and avoid unnecessary passes through the data.  \n",
    "NVTabular requires at most N passes through the data, where N is the number of chained operations. This is often less as lazy execution allows for careful planning and optimization of the workflow.  \n",
    "Other libraries, such as cuDF and pandas, due to their eager execution nature, do not allow workflow optimization and can iterate through the whole dataset as many times as the number of operations.\n",
    "\n",
    "\n",
    "The Workflow first ``fit`` by calculating statistics on the dataset, and then once fit it can ``transform`` the datasets by applying these statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters values for pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_PATH = f'{BUCKET}/workflow' # Where to write the calculated workflow\n",
    "OUTPUT_CONVERTED = f'{BUCKET}/converted' # Location to write the transformed data\n",
    "OUTPUT_TRANSFORMED = f'{BUCKET}/transformed_data' # Location to write the transformed data\n",
    "\n",
    "train_paths = ['gs://workshop-datasets/criteo/day_0'] # Sample training CSV file to be converted to parquet\n",
    "valid_paths = ['gs://workshop-datasets/criteo/day_1'] # Sample validation CSV file to be converted to parquet\n",
    "\n",
    "sep = '\\t' # Separator for the CSV file\n",
    "recursive = False # If the train/valid paths should be navigated recursivelly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NVTabular, NVIDIA provides an option to shuffle the dataset before storing to disk.  \n",
    "The uniformly shuffled dataset enables the data loader to read in contiguous chunks of data that are already randomized across the entire dataset.\n",
    "NVTabular provides the option to control the number of chunks that are combined into a batch, allowing the end user flexibility when trading off between performance and true randomization.  \n",
    "This mechanism is critical when dealing with datasets that exceed CPU memory and per-epoch shuffling is desired during training.  \n",
    "Full shuffling of such a dataset can exceed training time for the epoch by several orders of magnitude.\n",
    "\n",
    "In the next cell, choose between: PER_PARTITION, PER_WORKER, FULL. `None` will not shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = None # How to shuffle the dataset both in the conversion from CSV to PARQUET and during transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary with parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with all the parameters defined until now\n",
    "parameter_values = {\n",
    "    'train_paths': json.dumps(train_paths),\n",
    "    'valid_paths': json.dumps(valid_paths),\n",
    "    'output_converted': OUTPUT_CONVERTED,\n",
    "    'output_transformed': OUTPUT_TRANSFORMED,\n",
    "    'workflow_path': WORKFLOW_PATH,\n",
    "    'sep': sep,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFP pipeline compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline.\n",
    "# This command will validate the pipeline and generate a JSON file with its specifications\n",
    "PACKAGE_PATH = 'nvt_gcs_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_gcs,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize aiplatform SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize aiplatform SDK client\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f'{TIMESTAMP}_nvt_gcs_pipeline',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
