{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using NVTabular for large scale feature engineering on CSV files in Cloud Storage\n",
    "\n",
    "This notebook demonstrates how to preprocess CSV data files in Cloud Storage using NVIDIA NVTabular and Vertex AI. The data preprocessing is implemented using Vertex AI Pipelines, which covers the following steps.  \n",
    "\n",
    " 1. Read CSV files from Cloud Storage.\n",
    " 2. Convert the CSV files to parquet format and write it Cloud Storage.\n",
    " 3. Fit a pre-defined NVTabular workflow to the training data split to calculate transformation statistics.\n",
    " 4. Transform the training and validation data splits using the fitted workflow.\n",
    " 5. Output transformed parquet files to Cloud Storage.\n",
    "\n",
    "\n",
    "<img src=\"./images/pipeline_1.png\" alt=\"Pipeline\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NVTabular Overview\n",
    "\n",
    "[Merlin NVTabular](https://developer.nvidia.com/nvidia-merlin/nvtabular) is a feature engineering and preprocessing library designed to effectively manipulate \n",
    "large datasets and significantly reduce data preparation time, as follows:\n",
    "\n",
    "* Processes large datasets not bound by CPU or GPU memory.\n",
    "* Accelerates data preprocessing computation on GPUs using the RAPIDS cuDF library.\n",
    "* Supports multi-node scaling and multi-GPU with DASK-CUDA distributed parallelism.\n",
    "* Supports tabular data formats, including comma-separated values (CSV) files, Apache Parquet, Apache Orc, and Apache Avro.\n",
    "* Provides data loaders that are optimized for TensorFlow, PyTorch, and Merlin HugeCTR.\n",
    "* Includes multi-hot categoricals and vector continuous passing support to ease feature engineering.\n",
    "\n",
    "\n",
    "To preprocess the data, we need to define a transformation `workflow`.  \n",
    "Each transformation step in the transformation pipeline executes multiple calculations, called `ops`. \n",
    "NVTabular provides a [set of ops](https://nvidia.github.io/NVTabular/main/api/ops/index.html), which include:\n",
    "\n",
    " - Filtering outliers or missing values, or creating new features indicating that a value is missing;\n",
    " - Imputing and filling in missing data;\n",
    " - Discretization or bucketing of continuous features;\n",
    " - Creating features by splitting or combining existing features, for example, breaking down a date column into day-of-week, month-of-year, day-of-month features;\n",
    " - Normalizing numerical features to have zero mean and unit variance or applying transformations, for example with log transform;\n",
    " - Encoding discrete features using one-hot vectors or converting them to continuous integer indices.  \n",
    "\n",
    "NVTabular processes a dataset, given a pre-defined workflow, in two steps:\n",
    "\n",
    "1. The `fit` step, where NVTabular compute the statistics required for transforming the data. Such a step requires at most `N` passes through the data, where `N` is the number of chained operations in the workflow.\n",
    "2. The `apply` step, where NVTabular uses the fitted workflow to process the data. \n",
    "\n",
    "NVTabular is designed to minimize the number of passes through the data. This is achieved with a lazy execution strategy. Data operations are not executed until an explicit apply phase. This allows NVTabular to optimize the workflow that requires iteration over the entire dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing Criteo dataset\n",
    "\n",
    "The Criteo dataset contains over four billion samples spanning 24 CSV files. Each record contains 40 columns: 13 columns are numerical, 26 columns are categorical, and 1 binary target column. See [00-dataset-management.ipynb](00-dataset-management.ipynb) for more details.\n",
    "\n",
    "\n",
    "### NVTabular preprocessing Workflow for Criteo dataset\n",
    "\n",
    "In this example, the preprocessing `nvt.Workflow` consists for the following operations:\n",
    " - [Categorify](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/categorify.html): applied to categorical columns (columns that start with C). \n",
    " - [FillMissing](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/fillmissing.html): applied to continuous columns (columns that start with I).\n",
    " - [Clip](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/clip.html):  applied to continuous columns after FillMissing.\n",
    " - [Normalize](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/normalize.html): applied to continuous columns after Clip.\n",
    " \n",
    " <img src=\"./images/dag_preprocessing.png\" alt=\"Pipeline\" style=\"height: 50%; width:50%;\"/>\n",
    " \n",
    " The `nvt.Workflow` is createdin in the `create_criteo_nvt_workflow` method, which can be found in [src/preprocessing/etl.py](src/preprocessing/etl.py) module. \n",
    " This `nvt.Workflow` will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " \n",
    " \n",
    "### Converting CSV files to Parquet with NVTabular\n",
    "\n",
    "The Criteo dataset is provides in CSV format, but the recommended data format to run the NVTabular preprocessing task and get the best possible performance is [Parquet](http://parquet.apache.org/documentation/latest/); a compressed, column-oriented file structure format. While NVTabular also supports reading from CSV files, reading  \n",
    "Parquet files can 2X faster than reading CSV files.  \n",
    "\n",
    "To convert the Criteo CSV data to Parquet, the following steps are performed:\n",
    "\n",
    "1. Create a `nvt.Dataset` object the CSV data using the `create_csv_dataset` method in [src/preprocessing/etl.py](src/preprocessing/etl.py).\n",
    "2. Convert the CSV data to Parquet, and write it to Cloud Storahe using the `convert_csv_to_parquet` method in [src/preprocessing/etl.py](src/preprocessing/etl.py).\n",
    "\n",
    "### Implementing the preprocessing pipeline using KFP\n",
    "\n",
    "[src/pipelines/preprocessing_gcs.py](src/pipelines/preprocessing_gcs.py) defines the KFP pipeline to preprocess the Criteo CSV data. \n",
    "A pipeline component is a self-contained set of code that performs one step in your ML workflow.  \n",
    "The pipeline uses the following components defined in [src/pipelines/components.py](src/pipelines/components.py):\n",
    "\n",
    "1. `convert_csv_to_parquet_op`: this component converts raw CSV files to Parquet files, and store them to Cloud Storage. \n",
    "2. `analyze_dataset_op`: this component creates a Criteo preprocessing `nvt.Workflow`, fit it to the training data split, and store it to Cloud Storage.\n",
    "3. `transform_dataset_op`: this component loads the fitted `nvt.Workflow` from Cloud Storage, uses it to transform and input datas split, and store the transformed data as Parquet files to Cloud Storage.\n",
    "\n",
    "Each component is annotated with Inputs and Outputs to keep track of lineage metadata.\n",
    "The `base_image` used to execute the components is defined in [Dockerfile.nvtabular](Dockerfile.nvtabular). \n",
    "\n",
    "Each step in the pipeline is configured with the required CPU, memory and GPU configurations, as follows:\n",
    "\n",
    "```\n",
    "component_being_executed.set_cpu_limit(\"8\") # Number of CPUs\n",
    "component_being_executed.set_memory_limit(\"32G\") # Memory quantity\n",
    "component_being_executed.set_gpu_limit(\"1\") # Number of GPUs\n",
    "component_being_executed.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-t4') # GPU type\n",
    "```\n",
    "\n",
    "See [Specify machine type for a pipeline step](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#specify-machine-type) for more information.\n",
    "\n",
    "\n",
    "You can configure the pipeline by setting the variables in the [config.py](config.py) module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp' # Change to your project Id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "DATASET_GCS_LOCATION = 'gs://workshop-datasets/criteo'\n",
    "BUCKET =  'merlin-on-gcp' # Change to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'IMAGE_URI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15808/2337415041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maiplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing_gcs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing_gcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mTIMESTAMP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/merlin-on-vertex/src/pipelines/preprocessing_gcs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Preprocessing pipeline\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/merlin-on-vertex/src/pipelines/components.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransform_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mIMAGE_URI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IMAGE_URI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'IMAGE_URI'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import compiler\n",
    "from src.pipelines.preprocessing_gcs import preprocessing_gcs\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container image and push to gcr.io (Google Container Registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker image name and Dockerfile location\n",
    "IMAGE_NAME = 'nvt_preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERFILE = 'src/preprocessing'\n",
    "\n",
    "import os\n",
    "os.environ['IMAGE_URI'] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build can take up to 1 hour\n",
    "! gcloud builds submit --timeout \"2h\" --tag {IMAGE_URI} {DOCKERFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters values for pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_PATH = f'{BUCKET}/workflow' # Where to write the calculated workflow\n",
    "OUTPUT_CONVERTED = f'{BUCKET}/converted' # Location to write the transformed data\n",
    "OUTPUT_TRANSFORMED = f'{BUCKET}/transformed_data' # Location to write the transformed data\n",
    "\n",
    "train_paths = ['gs://workshop-datasets/criteo/day_0'] # Sample training CSV file to be converted to parquet\n",
    "valid_paths = ['gs://workshop-datasets/criteo/day_1'] # Sample validation CSV file to be converted to parquet\n",
    "\n",
    "sep = '\\t' # Separator for the CSV file\n",
    "recursive = False # If the train/valid paths should be navigated recursivelly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NVTabular, NVIDIA provides an option to shuffle the dataset before storing to disk.  \n",
    "The uniformly shuffled dataset enables the data loader to read in contiguous chunks of data that are already randomized across the entire dataset.\n",
    "NVTabular provides the option to control the number of chunks that are combined into a batch, allowing the end user flexibility when trading off between performance and true randomization.  \n",
    "This mechanism is critical when dealing with datasets that exceed CPU memory and per-epoch shuffling is desired during training.  \n",
    "Full shuffling of such a dataset can exceed training time for the epoch by several orders of magnitude.\n",
    "\n",
    "In the next cell, choose between: PER_PARTITION, PER_WORKER, FULL. `None` will not shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = None # How to shuffle the dataset both in the conversion from CSV to PARQUET and during transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary with parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with all the parameters defined until now\n",
    "parameter_values = {\n",
    "    'train_paths': json.dumps(train_paths),\n",
    "    'valid_paths': json.dumps(valid_paths),\n",
    "    'output_converted': OUTPUT_CONVERTED,\n",
    "    'output_transformed': OUTPUT_TRANSFORMED,\n",
    "    'workflow_path': WORKFLOW_PATH,\n",
    "    'sep': sep,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFP pipeline compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline.\n",
    "# This command will validate the pipeline and generate a JSON file with its specifications\n",
    "PACKAGE_PATH = 'nvt_gcs_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_gcs,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize aiplatform SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize aiplatform SDK client\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'gs://{BUCKET}/temp' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f'{TIMESTAMP}_nvt_gcs_pipeline',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m81"
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
