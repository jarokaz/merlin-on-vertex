{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using NVTabular for large scale feature engineering on CSV files in Google Cloud Storage\n",
    "\n",
    "This notebook demonstrates how to do data preprocessing with NVIDIA NVTabular on Vertex AI Pipeline steps using Google Cloud Storage as the data source.  \n",
    "You will create a pipeline with the following steps:\n",
    " - Read CSV files from Google Cloud Storage (GCS)\n",
    " - Convert these files to parquet format and write to GCS\n",
    " - Define the DAG with transformation steps and create a Workflow\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output transformed parquet files to GCS\n",
    "\n",
    "The goal is to present how to use NVTabular to transform the data on multiple GPUs.  \n",
    "The dataset used for this tutorial is the [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).  \n",
    "\n",
    "Architecture overview:\n",
    "\n",
    "<img src=\"./images/pipeline_1.png\" alt=\"Pipeline\" style=\"height: 60%; width:60%;\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Environment variables\n",
    "import config\n",
    "\n",
    "# Standard\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    Clip,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "# Import components and pipeline definition\n",
    "from src.preprocessing.pipeline_gcs import preprocessing_pipeline_gcs\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Create Workflow DAG definition for data transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Columns and dtype definitions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Columns and dtypes definition\n",
    "cont_names = [\"I\" + str(x) for x in range(1, 14)]\n",
    "cat_names = [\"C\" + str(x) for x in range(1, 27)]\n",
    "columns = [\"label\"] + cont_names + cat_names\n",
    "\n",
    "# Specify column dtypes. Note that \"hex\" means that\n",
    "# the values will be hexadecimal strings that should\n",
    "# be converted to int32\n",
    "cols_dtype = {}\n",
    "cols_dtype[\"label\"] = 'int32'\n",
    "for x in cont_names:\n",
    "    cols_dtype[x] = 'int32'\n",
    "for x in cat_names:\n",
    "    cols_dtype[x] = 'hex'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create transformation pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Transformation pipeline\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(max_size=num_buckets)\n",
    "cat_features = cat_names >> categorify_op\n",
    "cont_features = cont_names >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "features = cat_features + cont_features + ['label']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Create and save workflow\n",
    "workflow = nvt.Workflow(features)\n",
    "workflow.save('./saved_workflow')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Upload saved workflow to GCS\n",
    "! gsutil cp -r ./saved_workflow/ 'gs://{config.SAVED_WORKFLOW_PATH[5:]}'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Copying file://./saved_workflow/workflow.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://./saved_workflow/metadata.json [Content-Type=application/json]...\n",
      "\n",
      "Operation completed over 2 objects/1.6 KiB.                                      \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Pipeline definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameters values for pipeline execution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_paths = ['gs://workshop-datasets/criteo/day_0'] # Sample training CSV file to be converted\n",
    "valid_paths = ['gs://workshop-datasets/criteo/day_1'] # Sample validation CSV file to be converted\n",
    "\n",
    "sep = '\\t' # Separator for the CSV file\n",
    "gpus = '0' # Identifier of the GPU. As you will execute with only 1 GPU, only the first identier is passed.\n",
    "           # If you were to execute the pipeline with 4 GPUs, you should use '0,1,2,3'.\n",
    "\n",
    "recursive = False # If the train/valid paths should be navigated recursivelly\n",
    "shuffle = None # How to shuffle the dataset both in the conversion from CSV to PARQUET and during transformation."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dictionary with parameter values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Create a dictionarry will all the parameters defined until now\n",
    "parameter_values = {\n",
    "    'train_paths': json.dumps(train_paths),\n",
    "    'valid_paths': json.dumps(valid_paths),\n",
    "    'output_path': config.OUTPUT_PATH,\n",
    "    'columns': json.dumps(columns),\n",
    "    'cols_dtype': json.dumps(cols_dtype),\n",
    "    'output_transformed': config.OUTPUT_TRANSFORMED,\n",
    "    'workflow_path': config.SAVED_WORKFLOW_PATH,\n",
    "    'sep': sep,\n",
    "    'gpus': gpus,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline execution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KFP pipeline compilation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Compile the pipeline.\n",
    "# This command will validate the pipeline and generate a JSON file with its specifications\n",
    "PACKAGE_PATH = 'nvt_gcs_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_gcs,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize aiplatform SDK client"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Initialize aiplatform SDK client\n",
    "aiplatform.init(\n",
    "    project=config.PROJECT_ID,\n",
    "    location=config.REGION,\n",
    "    staging_bucket=config.STAGING_BUCKET\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f'{TIMESTAMP}_nvt_gcs_pipeline',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}