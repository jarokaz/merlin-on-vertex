{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Using Vertex AI for online serving with NVIDIA Triton\n",
    "\n",
    "- This notebooks demonstrates serving of ensemble models - NVTabular preprocessing + HugeCTR recommender on Triton server \n",
    "\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "- Building a custom container derived from NVIDIA NGC Merlin inference image and the model artifacts\n",
    "- Creating Vertex model using the custome container\n",
    "- Creating a Vertex endpoint and deploying the model to that endpoint\n",
    "- Getting the inference on a sample dataset using hte endpoint\n",
    "\n",
    "## Model serving\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\n",
    "Triton can load models from local storage or cloud platforms. As models are retrained with new data, developers can easily make updates without restarting the inference server or disrupting the application.\n",
    "\n",
    "Triton runs multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding.\n",
    "\n",
    "It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensemble for use cases that require multiple models to perform end-to-end inference, such as conversational AI.\n",
    "\n",
    "%Users can also use shared memory. The Inputs and outputs that pass to and from Triton are stored in shared memory, reducing HTTP/gRPC overhead and increasing performance.\n",
    "\n",
    "The following figure shows the Triton Inference Server high-level architecture. The model repository is a file-system based repository of the models that Triton will make available for inferencing. Inference requests arrive at the server via either HTTP/REST or GRPC or by the C API and are then routed to the appropriate per-model scheduler. Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis. Each model's scheduler optionally performs batching of inference requests and then passes the requests to the backend corresponding to the model type. The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs. The outputs are then returned.\n",
    "\n",
    "<img src=\"./images/triton-architecture.png\" alt=\"Triton Architecture\" />\n",
    "\n",
    "Triton supports a backend C API that allows Triton to be extended with new functionality such as custom pre- and post-processing operations or even a new deep-learning framework.\n",
    "\n",
    "The models being served by Triton can be queried and controlled by a dedicated model management API that is available by HTTP/REST or GRPC protocol, or by the C API.\n",
    "\n",
    "Readiness and liveness health endpoints and utilization, throughput and latency metrics ease the integration of Triton into deployment framework such as Kubernetes.\n",
    "\n",
    "Here we use Triton to serve an ensemble model that contains data processing operations using NVTabular and HugeCTR model trained on Criteo data. The model is deployed into Google's Vertex AI and served via a Vertex Endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## Notebook flow\n",
    "\n",
    "This notebook assumes that the emsemble model containg the Hugectr trained model asn the NVTabular preprocessed wrokflow is created using ... notebook.\n",
    "\n",
    "As you walk through the notebook you will execute the following steps:\n",
    "- Configure notebook environment settings like GCP project and compute region.\n",
    "- Build a custom Vertex container based on NVIDIA NGC Merlin Inference container\n",
    "- Configure and submit the model based on the custom container \n",
    "- Create the endoint\n",
    "- Configure the deployment of the model and submit the deployment job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6a68a-ed54-4810-9e2b-85972ae24e06",
   "metadata": {},
   "source": [
    "## Configure notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp'\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"gs://cloud-ai-platform-61647b5e-05eb-4c08-b632-92067b616f37\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f5255-c846-4eee-b0f3-bea76e3b8030",
   "metadata": {},
   "source": [
    "## Submit a Vertex custom training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71a3ac-10dc-42fc-9789-daf4fb37727e",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3f0679-1102-45a5-a581-90b391396566",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb83ba-e6a2-49aa-8c14-8c1d4f4c6a73",
   "metadata": {},
   "source": [
    "### Build a custom prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa33dc62-9210-41fe-9733-66d4d2c77566",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'triton_deploy-hugectr'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton-hugectr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bda537e0-fe3d-48cb-b031-84f70465e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://diman-criteo/models/deepfm/1/.ipynb_checkpoints/deepfm-checkpoint.json...\n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm.json...                        \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_opt_sparse_0.model...         \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_sparse_0.model/emb_vector...  \n",
      "==> NOTE: You are downloading one or more large file(s), which would            \n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "| [4 files][569.7 MiB/569.7 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_sparse_0.model/key...\n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_sparse_0.model/slot_id...     \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm_dense_0.model...               \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm_opt_dense_0.model...           \n",
      "Copying gs://diman-criteo/models/deepfm/config.pbtxt...                         \n",
      "Copying gs://diman-criteo/models/deepfm_ens/config.pbtxt...                     \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/.ipynb_checkpoints/model-checkpoint.py...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/__pycache__/model.cpython-38.pyc...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/model.py...                       \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/.ipynb_checkpoints/column_types-checkpoint.json...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/.ipynb_checkpoints/metadata-checkpoint.json...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C1.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C10.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C11.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C12.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C13.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C14.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C15.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C16.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C17.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C18.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C19.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C2.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C20.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C21.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C22.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C23.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C24.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C25.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C26.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C3.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C4.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C5.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C6.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C7.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C8.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C9.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/column_types.json...     \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/metadata.json...         \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/workflow.pkl...          \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/config.pbtxt...                     \n",
      "Copying gs://diman-criteo/models/ps.json...B/s                                  \n",
      "\\ [46 files][  1.1 GiB/  1.1 GiB]   43.5 MiB/s                                  \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 46 objects/1.1 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://diman-criteo/models ./src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0db0d830-5328-4f05-9e6e-a080b1e0b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./src/models/deepfm_ens/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9672b9b-9e73-471b-8600-a30660432be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  1.133GB\n",
      "Step 1/8 : FROM gcr.io/merlin-on-gcp/dongm-merlin-inference-hugectr:v0.6.1\n",
      " ---> fb6f7db2d7fd\n",
      "Step 2/8 : EXPOSE 8000\n",
      " ---> Using cache\n",
      " ---> 6483e4a811d5\n",
      "Step 3/8 : EXPOSE 8001\n",
      " ---> Using cache\n",
      " ---> 36f81f5b7f47\n",
      "Step 4/8 : EXPOSE 8002\n",
      " ---> Using cache\n",
      " ---> 541852b52454\n",
      "Step 5/8 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> b625e263b707\n",
      "Step 6/8 : RUN mkdir /model\n",
      " ---> Using cache\n",
      " ---> 959204525114\n",
      "Step 7/8 : COPY /models/ /model/models/\n",
      " ---> Using cache\n",
      " ---> b7fa7ce1fce7\n",
      "Step 8/8 : CMD [\"tritonserver\", \"--model-repository=/model/models/\", \"--backend-config=hugectr,ps=/model/models/ps.json\"]\n",
      " ---> Using cache\n",
      " ---> 8af0879748fc\n",
      "Successfully built 8af0879748fc\n",
      "Successfully tagged gcr.io/merlin-on-gcp/triton_deploy-hugectr:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/merlin-on-gcp/triton_deploy-hugectr]\n",
      "\n",
      "\u001b[1B62eee2b8: Preparing \n",
      "\u001b[1B6660d413: Preparing \n",
      "\u001b[1Be58e8598: Preparing \n",
      "\u001b[1Bc9824aed: Preparing \n",
      "\u001b[1B23fe2ec9: Preparing \n",
      "\u001b[1Beb0eea4e: Preparing \n",
      "\u001b[1B08de1536: Preparing \n",
      "\u001b[1Bd698577a: Preparing \n",
      "\u001b[1B21cc7b5f: Preparing \n",
      "\u001b[1B646f53eb: Preparing \n",
      "\u001b[1Bf0fe3ed8: Preparing \n",
      "\u001b[1B7d455483: Preparing \n",
      "\u001b[1B026e0fab: Preparing \n",
      "\u001b[1B9b492bd7: Preparing \n",
      "\u001b[1B2784440c: Preparing \n",
      "\u001b[1B2f6421c3: Preparing \n",
      "\u001b[1B8a55152d: Preparing \n",
      "\u001b[1B589ca15a: Preparing \n",
      "\u001b[1B0a50ea64: Preparing \n",
      "\u001b[1B64c9831d: Preparing \n",
      "\u001b[1B42df9148: Preparing \n",
      "\u001b[1B4e7260d1: Preparing \n",
      "\u001b[1B01b2e862: Preparing \n",
      "\u001b[1Bda137864: Preparing \n",
      "\u001b[1B5d96cc97: Preparing \n",
      "\u001b[1B41d8a723: Preparing \n",
      "\u001b[1B8118241f: Preparing \n",
      "\u001b[1B75c0649e: Preparing \n",
      "\u001b[1B357bd24c: Preparing \n",
      "\u001b[1Bb05908c8: Preparing \n",
      "\u001b[18Bb492bd7: Waiting g \n",
      "\u001b[23B46f53eb: Waiting g \n",
      "\u001b[1B7c006461: Preparing \n",
      "\u001b[20B784440c: Waiting g \n",
      "\u001b[2Bd42ad1cc: Waiting g \n",
      "\u001b[1B08382770: Preparing \n",
      "\u001b[1B08abd40a: Preparing \n",
      "\u001b[1B325c8271: Preparing \n",
      "\u001b[1B22e43a26: Preparing \n",
      "\u001b[6B8046cdd7: Waiting g \n",
      "\u001b[1B9ecbfbfe: Preparing \n",
      "\u001b[1Bd199c000: Preparing \n",
      "\u001b[1B35c73479: Preparing \n",
      "\u001b[9B08382770: Waiting g \n",
      "\u001b[1B6a18e033: Preparing \n",
      "\u001b[1Bb0b86b83: Preparing \n",
      "\u001b[1B428c2607: Preparing \n",
      "\u001b[5Bf46ba1c7: Waiting g \n",
      "\u001b[3B428c2607: Waiting g \n",
      "\u001b[1Bd4750603: Preparing \n",
      "\u001b[1B4d6d50a7: Preparing \n",
      "\u001b[1B2b1df38f: Preparing \n",
      "\u001b[1B9a293c2f: Preparing \n",
      "\u001b[1B60a9947e: Preparing \n",
      "\u001b[1Bb9ef49ca: Preparing \n",
      "\u001b[1B2cb0a2aa: Preparing \n",
      "\u001b[7B4d6d50a7: Waiting g \n",
      "\u001b[7B2b1df38f: Waiting g \n",
      "\u001b[5Bb9ef49ca: Waiting g \n",
      "\u001b[7B60a9947e: Waiting g \n",
      "\u001b[1B7c57c2d4: Preparing \n",
      "\u001b[6B13f0a43e: Waiting g \n",
      "\u001b[1B075e907e: Preparing \n",
      "\u001b[1Ba057adac: Preparing \n",
      "\u001b[8B98d0b77d: Waiting g \n",
      "\u001b[1Bfa616420: Preparing \n",
      "\u001b[1B762bfd28: Preparing \n",
      "\u001b[5Ba057adac: Waiting g \n",
      "\u001b[5Ba85aee62: Waiting g \n",
      "\u001b[1B65fd7ee4: Preparing \n",
      "\u001b[6Bfa616420: Waiting g \n",
      "\u001b[6B762bfd28: Waiting g \n",
      "\u001b[6B316f3ae7: Waiting g \n",
      "\u001b[4B96ef8c35: Waiting g \n",
      "\u001b[7B2fa93862: Waiting g \n",
      "\u001b[7B65fd7ee4: Waiting g \n",
      "\u001b[1Ba1abcbfa: Layer already exists \u001b[74A\u001b[2K\u001b[70A\u001b[2K\u001b[68A\u001b[2K\u001b[63A\u001b[2K\u001b[60A\u001b[2K\u001b[57A\u001b[2K\u001b[53A\u001b[2K\u001b[49A\u001b[2K\u001b[46A\u001b[2K\u001b[40A\u001b[2K\u001b[37A\u001b[2K\u001b[33A\u001b[2K\u001b[30A\u001b[2K\u001b[24A\u001b[2K\u001b[21A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:345ed25ebeea7fbc192d4adfe036357ccb8bd04dc23b968a2f53047157f15f5b size: 16502\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {IMAGE_URI} -f {DOCKERFILE} src\n",
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dead7-e920-4417-b61a-d8a9ccc9c7d7",
   "metadata": {},
   "source": [
    "### Configure a custom prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "247c919b-1855-4040-bac2-af1d41fa1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{IMAGE_NAME}-deepfm-v{VERSION}\"\n",
    "model_description = \"Serving with Triton inference server using a custom container\"\n",
    "\n",
    "health_route = \"/v2/health/ready\"\n",
    "predict_route = f\"/v2/models/deepfm_ens/infer\"\n",
    "serving_container_ports = [8000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c1220-bf18-4256-90e8-47b125734c4c",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa84abf4-3b8a-4d79-9490-58cc569350bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/659831510405/locations/us-central1/models/8979605910930325504/operations/8507813305471991808\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/659831510405/locations/us-central1/models/8979605910930325504\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/659831510405/locations/us-central1/models/8979605910930325504')\n",
      "triton_deploy-hugectr-deepfm-v1\n",
      "projects/659831510405/locations/us-central1/models/8979605910930325504\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/659831510405/locations/us-central1/endpoints/5806274615680434176/operations/2118700770047033344\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/659831510405/locations/us-central1/endpoints/5806274615680434176\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/659831510405/locations/us-central1/endpoints/5806274615680434176')\n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{IMAGE_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Set deployment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16274eb5-feb6-48d4-9fa7-fea87b1da376",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fd27dda-7c1b-4ce5-9477-e62a9f1199bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/659831510405/locations/us-central1/endpoints/5806274615680434176\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/659831510405/locations/us-central1/endpoints/5806274615680434176/operations/3871920439047487488\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/659831510405/locations/us-central1/endpoints/5806274615680434176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f98656d7550> \n",
       "resource name: projects/659831510405/locations/us-central1/endpoints/5806274615680434176"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "### Getting inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aafdee-283d-4377-ac5a-ef92a7de6d38",
   "metadata": {},
   "source": [
    "### Reading CSV data file, getting the input data, and generating the inference request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50394a33-e4b3-4fe8-950f-1cbd76f9b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import infer_input\n",
    "\n",
    "data = pd.read_csv('gs://diman-criteo/data/criteo.csv', encoding='utf-8')\n",
    "#data = pd.read_csv('/data/criteo.csv', encoding='utf-8', index_col=[0])\n",
    "\n",
    "binary_data = True\n",
    "\n",
    "data = data[[x for x in data.columns if x != \"label\"]].fillna(0)\n",
    "\n",
    "inputs = infer_input.get_inference_input(data, binary_data)\n",
    "\n",
    "if (binary_data):\n",
    "    request_body, json_size = infer_input.get_inference_request(inputs, '1')\n",
    "else:\n",
    "    infer_request, request_body, json_size = infer_input.get_inference_request(inputs, '1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855163ea-e9fb-490f-a185-20289de6e4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"1\",\"model_name\":\"deepfm_ens\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[3],\"data\":[0.06609038263559342,0.07316402345895767,0.08091689646244049]}]}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\"  \\\n",
    "  https://us-central1-aiplatform.googleapis.com/v1/projects/merlin-on-gcp/locations/us-central1/endpoints/5806274615680434176:rawPredict \\\n",
    "  -d @criteo.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
