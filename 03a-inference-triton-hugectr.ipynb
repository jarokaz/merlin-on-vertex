{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Using Vertex AI for online serving with NVIDIA Triton\n",
    "\n",
    "- This notebooks demonstrates serving of ensemble models - NVTabular preprocessing + HugeCTR recommender on Triton server \n",
    "\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "- Building a custom container derived from NVIDIA NGC Merlin inference image and the model artifacts\n",
    "- Creating Vertex model using the custome container\n",
    "- Creating a Vertex endpoint and deploying the model to that endpoint\n",
    "- Getting the inference on a sample dataset using hte endpoint\n",
    "\n",
    "## Model serving\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\n",
    "Triton can load models from local storage or cloud platforms. As models are retrained with new data, developers can easily make updates without restarting the inference server or disrupting the application.\n",
    "\n",
    "Triton runs multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding.\n",
    "\n",
    "It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensemble for use cases that require multiple models to perform end-to-end inference, such as conversational AI.\n",
    "\n",
    "Users can also use shared memory. The Inputs and outputs that pass to and from Triton are stored in shared memory, reducing HTTP/gRPC overhead and increasing performance.\n",
    "\n",
    "<img src=\"./images/triton-architecture.png\" alt=\"Triton Architecture\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## Notebook flow\n",
    "\n",
    "This notebook assumes that the emsemble model containg the Hugectr trained model asn the NVTabular preprocessed wrokflow is created using ... notebook.\n",
    "\n",
    "As you walk through the notebook you will execute the following steps:\n",
    "- Configure notebook environment settings like GCP project and compute region.\n",
    "- Build a custom Vertex container based on NVIDIA NGC Merlin Inference container\n",
    "- Configure and submit the model based on the custom container \n",
    "- Create the endoint\n",
    "- Configure the deployment of the model and submit the deployment job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6a68a-ed54-4810-9e2b-85972ae24e06",
   "metadata": {},
   "source": [
    "## Configure notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp'\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "BUCKET_NAME = \"gs://cloud-ai-platform-61647b5e-05eb-4c08-b632-92067b616f37\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f5255-c846-4eee-b0f3-bea76e3b8030",
   "metadata": {},
   "source": [
    "## Submit a Vertex custom training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71a3ac-10dc-42fc-9789-daf4fb37727e",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3f0679-1102-45a5-a581-90b391396566",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb83ba-e6a2-49aa-8c14-8c1d4f4c6a73",
   "metadata": {},
   "source": [
    "### Build a custom prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa33dc62-9210-41fe-9733-66d4d2c77566",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'triton_deploy-hugectr'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton-hugectr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9672b9b-9e73-471b-8600-a30660432be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  420.9kB\n",
      "Step 1/4 : FROM nvcr.io/nvidia/merlin/merlin-training:21.09\n",
      "21.09: Pulling from nvidia/merlin/merlin-training\n",
      "\n",
      "\u001b[1Bccf8d472: Already exists \n",
      "\u001b[1Ba3ad5c35: Already exists \n",
      "\u001b[1Bbc8dc1bd: Already exists \n",
      "\u001b[1B082db0a6: Already exists \n",
      "\u001b[1Bdaaa33a4: Already exists \n",
      "\u001b[1Baf991e8d: Already exists \n",
      "\u001b[1B1fb0fdb6: Already exists \n",
      "\u001b[1B571dbd7c: Already exists \n",
      "\u001b[1Bd18379eb: Already exists \n",
      "\u001b[1B8153258e: Already exists \n",
      "\u001b[1B19f57f60: Already exists \n",
      "\u001b[1B76a15bd8: Already exists \n",
      "\u001b[1Ba8b4dfd1: Already exists \n",
      "\u001b[1Be2b38283: Already exists \n",
      "\u001b[1B03da1dad: Already exists \n",
      "\u001b[1B161feb9f: Already exists \n",
      "\u001b[1B56dbc5fb: Already exists \n",
      "\u001b[1Bf2c05242: Already exists \n",
      "\u001b[1B06e859d2: Already exists \n",
      "\u001b[1B32c1dafe: Already exists \n",
      "\u001b[1B31c8c71b: Already exists \n",
      "\u001b[1B2263d14b: Already exists \n",
      "\u001b[1B874d11d2: Already exists \n",
      "\u001b[1Bfd25df9e: Already exists \n",
      "\u001b[1B01889029: Already exists \n",
      "\u001b[1Ba23c7e38: Already exists \n",
      "\u001b[1B2be66c3a: Already exists \n",
      "\u001b[1Ba81b8838: Already exists \n",
      "\u001b[1B806fd406: Already exists \n",
      "\u001b[1B6bd498b3: Already exists \n",
      "\u001b[1Bef747788: Already exists \n",
      "\u001b[1B6961f0f0: Already exists \n",
      "\u001b[1B366176eb: Already exists \n",
      "\u001b[1B584ede4e: Already exists \n",
      "\u001b[1B435e941b: Already exists \n",
      "\u001b[1Be3c5d152: Already exists \n",
      "\u001b[1B5c395d48: Already exists \n",
      "\u001b[1B9efc3ba5: Already exists \n",
      "\u001b[1B0e5d7c6b: Already exists \n",
      "\u001b[1B2b3c3a4c: Already exists \n",
      "\u001b[1Bf54df6a5: Already exists \n",
      "\u001b[1B77515535: Already exists \n",
      "\u001b[1Bfe64da0c: Already exists \n",
      "\u001b[1B0be661cd: Already exists \n",
      "\u001b[1B551b1f81: Already exists \n",
      "\u001b[1B3486b246: Already exists \n",
      "\u001b[1B73cbe267: Already exists \n",
      "\u001b[1B41d6bb3c: Already exists \n",
      "\u001b[1B4e4a9f46: Already exists \n",
      "\u001b[1Bfe24165f: Already exists \n",
      "\u001b[1B079a46e3: Already exists \n",
      "\u001b[1B963adc59: Already exists \n",
      "\u001b[1B969091b5: Already exists \n",
      "\u001b[1B74d30104: Already exists \n",
      "\u001b[1Bae3fea90: Already exists \n",
      "\u001b[1B20f88c04: Already exists \n",
      "\u001b[1B28cfc2d2: Already exists \n",
      "\u001b[1Bef74fc10: Already exists \n",
      "\u001b[1B01febd9e: Already exists \n",
      "\u001b[1Ba7aa4230: Already exists \n",
      "\u001b[1B17741684: Already exists \n",
      "\u001b[1Baa2690c4: Already exists \n",
      "\u001b[1B7f5477b2: Already exists \n",
      "\u001b[1B6e39d73a: Already exists \n",
      "\u001b[1Ba177384e: Already exists \n",
      "\u001b[1Bd1d3935d: Already exists \n",
      "\u001b[1Bddc33d1a: Already exists \n",
      "\u001b[1Bb88e1090: Already exists \n",
      "\u001b[1B3dc59d33: Already exists \n",
      "\u001b[1Bde2dd91d: Already exists \n",
      "\u001b[1B60dbad01: Already exists \n",
      "\u001b[1Bae843fc1: Already exists \n",
      "\u001b[1B74145eea: Already exists \n",
      "\u001b[1B39135aca: Already exists \n",
      "\u001b[1Bb527fed8: Already exists \n",
      "\u001b[1Bf32b975d: Already exists \n",
      "\u001b[1B4b48f4eb: Already exists \n",
      "\u001b[1Bb392c26d: Already exists \n",
      "\u001b[1B1a16f226: Already exists \n",
      "\u001b[1B747e37db: Already exists \n",
      "\u001b[1Bcecc9834: Already exists \n",
      "\u001b[1Bfa92f8f8: Already exists \n",
      "\u001b[1B2912e521: Already exists \n",
      "\u001b[1BDigest: sha256:0e915a1dfdc77304837db58bd2f3b6ef92fa5ed1804fe91ad17449469464e299\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-training:21.09\n",
      " ---> 8f6ef763d770\n",
      "Step 2/4 : RUN pip3 install cloudml-hypertune\n",
      " ---> Running in ec53b7dc9c91\n",
      "\u001b[91mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /usr/local/lib/python3.8/dist-packages\n",
      "sysconfig: /usr/lib/python3.8/site-packages\n",
      "WARNING: Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /usr/local/lib/python3.8/dist-packages\n",
      "sysconfig: /usr/lib/python3.8/site-packages\n",
      "\u001b[0m\u001b[91mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /usr/local/include/python3.8/UNKNOWN\n",
      "sysconfig: /usr/include/python3.8/UNKNOWN\n",
      "\u001b[0m\u001b[91mWARNING: Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /usr/local/bin\n",
      "sysconfig: /usr/bin\n",
      "\u001b[0m\u001b[91mWARNING: Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /usr/local\n",
      "sysconfig: /usr\n",
      "\u001b[0m\u001b[91mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=28059c1b6c70b6ad830db486adb5d4d0715c2d341054bfd636d213632a89fd34\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-aozwfkbg/wheels/87/fb/3b/365271726c73d8bc0b5bf39ef0f5db5a9c75b2babe4fd67794\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "\u001b[91m  WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /usr/local/include/python3.8/cloudml-hypertune\n",
      "  sysconfig: /usr/include/python3.8/cloudml-hypertune\n",
      "\u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: You are using pip version 21.2.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container ec53b7dc9c91\n",
      " ---> b9da15d28037\n",
      "Step 3/4 : WORKDIR /src\n",
      " ---> Running in b690f97715bb\n",
      "Removing intermediate container b690f97715bb\n",
      " ---> 3d10c5251be4\n",
      "Step 4/4 : COPY training/hugectr/ ./\n",
      " ---> 73cccb1a437f\n",
      "Successfully built 73cccb1a437f\n",
      "Successfully tagged gcr.io/merlin-on-gcp/triton_deploy-hugectr:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/merlin-on-gcp/triton_deploy-hugectr]\n",
      "\n",
      "\u001b[1Bc2d6d552: Preparing \n",
      "\u001b[1Bafb26e20: Preparing \n",
      "\u001b[1Be88b23a3: Preparing \n",
      "\u001b[1B1f36f93d: Preparing \n",
      "\u001b[1Be0fa48ac: Preparing \n",
      "\u001b[1Bd4cd8fbb: Preparing \n",
      "\u001b[1B2cf6a026: Preparing \n",
      "\u001b[1B26a95f43: Preparing \n",
      "\u001b[1Bba5d6668: Preparing \n",
      "\u001b[1Ba9641b18: Preparing \n",
      "\u001b[1B42012c23: Preparing \n",
      "\u001b[1Bbd3e801f: Preparing \n",
      "\u001b[1B7039f00c: Preparing \n",
      "\u001b[1B023e019c: Preparing \n",
      "\u001b[1Bf9e4afbc: Preparing \n",
      "\u001b[1Be9115178: Preparing \n",
      "\u001b[1B220871e3: Preparing \n",
      "\u001b[1B76810591: Preparing \n",
      "\u001b[1B7b7db32e: Preparing \n",
      "\u001b[1B176606fb: Preparing \n",
      "\u001b[1B44765ddc: Preparing \n",
      "\u001b[1B295911d7: Preparing \n",
      "\u001b[1B1a643992: Preparing \n",
      "\u001b[1B6f8bf009: Preparing \n",
      "\u001b[1Bd3ad3992: Preparing \n",
      "\u001b[1Bd0bdb6b2: Preparing \n",
      "\u001b[1B59647bc1: Preparing \n",
      "\u001b[1B9b0b029e: Preparing \n",
      "\u001b[1B3c6245a7: Preparing \n",
      "\u001b[1B9f6effa5: Preparing \n",
      "\u001b[1B43806c89: Preparing \n",
      "\u001b[1B31c5510f: Preparing \n",
      "\u001b[1B4c5d4460: Preparing \n",
      "\u001b[1Be3d1aa10: Preparing \n",
      "\u001b[1Bc1212f82: Preparing \n",
      "\u001b[1B6fc769b7: Preparing \n",
      "\u001b[1B58a72969: Preparing \n",
      "\u001b[1Bcb179a54: Preparing \n",
      "\u001b[1B109a3ea2: Preparing \n",
      "\u001b[1Bf3c3f94e: Preparing \n",
      "\u001b[1Bdeda2561: Preparing \n",
      "\u001b[1B7e454744: Preparing \n",
      "\u001b[1B3117f7ee: Preparing \n",
      "\u001b[1Bcec9ff49: Preparing \n",
      "\u001b[1B15c2b3a3: Preparing \n",
      "\u001b[1Bd1994cdb: Preparing \n",
      "\u001b[1Bc8e05279: Preparing \n",
      "\u001b[1B1b9afbae: Preparing \n",
      "\u001b[1B54a6be62: Preparing \n",
      "\u001b[1B60182953: Preparing \n",
      "\u001b[1B823004fd: Preparing \n",
      "\u001b[1B4b4d2292: Preparing \n",
      "\u001b[1B9ce6b542: Preparing \n",
      "\u001b[49B4cd8fbb: Waiting g \n",
      "\u001b[49Bcf6a026: Waiting g \n",
      "\u001b[1B664f5ac5: Preparing \n",
      "\u001b[1B6513dba7: Preparing \n",
      "\u001b[46B039f00c: Waiting g \n",
      "\u001b[51Ba5d6668: Waiting g \n",
      "\u001b[1B316378c2: Preparing \n",
      "\u001b[52B9641b18: Waiting g \n",
      "\u001b[1Bf3851c58: Preparing \n",
      "\u001b[49B9e4afbc: Waiting g \n",
      "\u001b[54B2012c23: Waiting g \n",
      "\u001b[1B7e819190: Preparing \n",
      "\u001b[55Bd3e801f: Waiting g \n",
      "\u001b[1Bd201b9e7: Preparing \n",
      "\u001b[52B20871e3: Waiting g \n",
      "\u001b[50B76606fb: Waiting g \n",
      "\u001b[53B6810591: Waiting g \n",
      "\u001b[1Be5379d4b: Preparing \n",
      "\u001b[52B4765ddc: Waiting g \n",
      "\u001b[55Bb7db32e: Waiting g \n",
      "\u001b[1Bf42fb8d5: Preparing \n",
      "\u001b[54B95911d7: Waiting g \n",
      "\u001b[1B970605a9: Preparing \n",
      "\u001b[55Ba643992: Waiting g \n",
      "\u001b[45B3d1aa10: Waiting g \n",
      "\u001b[1B518e20b0: Preparing \n",
      "\u001b[57Bf8bf009: Waiting g \n",
      "\u001b[47B1212f82: Waiting g \n",
      "\u001b[1Bf6dd0bf3: Preparing \n",
      "\u001b[59B3ad3992: Waiting g \n",
      "\u001b[49Bfc769b7: Waiting g \n",
      "\u001b[60B0bdb6b2: Waiting g \n",
      "\u001b[1B6a01493d: Preparing \n",
      "\u001b[7B5d7876c0: Layer already exists kBB\u001b[82A\u001b[2K\u001b[81A\u001b[2K\u001b[80A\u001b[2K\u001b[79A\u001b[2K\u001b[78A\u001b[2K\u001b[76A\u001b[2K\u001b[75A\u001b[2K\u001b[74A\u001b[2K\u001b[73A\u001b[2K\u001b[71A\u001b[2K\u001b[69A\u001b[2K\u001b[67A\u001b[2K\u001b[65A\u001b[2K\u001b[64A\u001b[2K\u001b[62A\u001b[2K\u001b[61A\u001b[2K\u001b[59A\u001b[2K\u001b[57A\u001b[2K\u001b[56A\u001b[2K\u001b[55A\u001b[2K\u001b[87A\u001b[2K\u001b[51A\u001b[2K\u001b[45A\u001b[2K\u001b[40A\u001b[2K\u001b[37A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2Klatest: digest: sha256:33fb2955f1e7ad7be5b056a8fbafdee73988051daed07e883d6a67bc88449656 size: 18609\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {IMAGE_URI} -f {DOCKERFILE} src\n",
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dead7-e920-4417-b61a-d8a9ccc9c7d7",
   "metadata": {},
   "source": [
    "### Configure a custom prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c919b-1855-4040-bac2-af1d41fa1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{APP_NAME}-movielens-v{VERSION}\"\n",
    "model_description = \"Serving with Triton inference server using a custom container\"\n",
    "\n",
    "health_route = \"/v2/health/ready\"\n",
    "predict_route = f\"/v2/models/deepfm/infer\"\n",
    "serving_container_ports = [8000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c1220-bf18-4256-90e8-47b125734c4c",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84abf4-3b8a-4d79-9490-58cc569350bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Set deployment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16274eb5-feb6-48d4-9fa7-fea87b1da376",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd27dda-7c1b-4ce5-9477-e62a9f1199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "### Getting inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a8045-35c3-4395-a93f-c4f7ab21c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \\\n",
    "-X POST  https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/endpoints/5851653659582005248:rawPredict \\\n",
    "-k -H \"Content-Type: application/octet-stream\" \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Infer-Header-Content-Length: 3710\" \\\n",
    "--data-binary \"@criteo.dat\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m80"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
