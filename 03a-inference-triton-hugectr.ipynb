{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Using Vertex AI for online serving with NVIDIA Triton\n",
    "\n",
    "- This notebooks demonstrates serving of ensemble models - NVTabular preprocessing + HugeCTR recommender on Triton server \n",
    "\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "- Building a custom container derived from NVIDIA NGC Merlin inference image and the model artifacts\n",
    "- Creating Vertex model using the custome container\n",
    "- Creating a Vertex endpoint and deploying the model to that endpoint\n",
    "- Getting the inference on a sample dataset using hte endpoint\n",
    "\n",
    "## Model serving\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\n",
    "Triton can load models from local storage or cloud platforms. As models are retrained with new data, developers can easily make updates without restarting the inference server or disrupting the application.\n",
    "\n",
    "Triton runs multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding.\n",
    "\n",
    "It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensemble for use cases that require multiple models to perform end-to-end inference, such as conversational AI.\n",
    "\n",
    "Users can also use shared memory. The Inputs and outputs that pass to and from Triton are stored in shared memory, reducing HTTP/gRPC overhead and increasing performance.\n",
    "\n",
    "<img src=\"./images/triton-architecture.png\" alt=\"Triton Architecture\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## Notebook flow\n",
    "\n",
    "This notebook assumes that the emsemble model containg the Hugectr trained model asn the NVTabular preprocessed wrokflow is created using ... notebook.\n",
    "\n",
    "As you walk through the notebook you will execute the following steps:\n",
    "- Configure notebook environment settings like GCP project and compute region.\n",
    "- Build a custom Vertex container based on NVIDIA NGC Merlin Inference container\n",
    "- Configure and submit the model based on the custom container \n",
    "- Create the endoint\n",
    "- Configure the deployment of the model and submit the deployment job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6a68a-ed54-4810-9e2b-85972ae24e06",
   "metadata": {},
   "source": [
    "## Configure notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp'\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"gs://cloud-ai-platform-61647b5e-05eb-4c08-b632-92067b616f37\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d372cc8-42bb-4859-aa32-8a9a8acf9f52",
   "metadata": {},
   "source": [
    "## Export the ensemble model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa911f62-c1fd-4288-89da-ca896f79b2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6f5255-c846-4eee-b0f3-bea76e3b8030",
   "metadata": {},
   "source": [
    "## Submit a Vertex custom training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71a3ac-10dc-42fc-9789-daf4fb37727e",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3f0679-1102-45a5-a581-90b391396566",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb83ba-e6a2-49aa-8c14-8c1d4f4c6a73",
   "metadata": {},
   "source": [
    "### Build a custom prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa33dc62-9210-41fe-9733-66d4d2c77566",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'triton_deploy-hugectr'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton-hugectr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bda537e0-fe3d-48cb-b031-84f70465e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://diman-criteo/models/deepfm/.ipynb_checkpoints/config-checkpoint.pbtxt...\n",
      "Copying gs://diman-criteo/models/deepfm/1/.ipynb_checkpoints/deepfm-checkpoint.json...\n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm.json...                        \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_opt_sparse_0.model...         \n",
      "/ [4 files][ 11.3 KiB/ 11.3 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_sparse_0.model/emb_vector...\n",
      "==> NOTE: You are downloading one or more large file(s), which would            \n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_sparse_0.model/key...         \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm0_sparse_0.model/slot_id...     \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm_dense_0.model...               \n",
      "Copying gs://diman-criteo/models/deepfm/1/deepfm_opt_dense_0.model...           \n",
      "Copying gs://diman-criteo/models/deepfm/config.pbtxt...                         \n",
      "Copying gs://diman-criteo/models/deepfm_ens/.ipynb_checkpoints/config-checkpoint.pbtxt...\n",
      "Copying gs://diman-criteo/models/deepfm_ens/config.pbtxt...                     \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/.ipynb_checkpoints/config-checkpoint.pbtxt...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/.ipynb_checkpoints/model-checkpoint.py...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/__pycache__/model.cpython-38.pyc...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/model.py...                       \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/.ipynb_checkpoints/column_types-checkpoint.json...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/.ipynb_checkpoints/metadata-checkpoint.json...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C1.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C10.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C11.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C12.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C13.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C14.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C15.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C16.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C17.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C18.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C19.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C2.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C20.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C21.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C22.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C23.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C24.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C25.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C26.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C3.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C4.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C5.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C6.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C7.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C8.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/categories/unique.C9.parquet...\n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/column_types.json...     \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/metadata.json...         \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/1/workflow/workflow.pkl...          \n",
      "Copying gs://diman-criteo/models/deepfm_nvt/config.pbtxt...                     \n",
      "Copying gs://diman-criteo/models/ps.json...B/s                                  \n",
      "/ [49 files][  1.1 GiB/  1.1 GiB]   46.1 MiB/s                                  \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 49 objects/1.1 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://diman-criteo/models ./src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9672b9b-9e73-471b-8600-a30660432be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  1.133GB\n",
      "Step 1/8 : FROM gcr.io/merlin-on-gcp/dongm-merlin-inference-hugectr:v0.6.1\n",
      " ---> fb6f7db2d7fd\n",
      "Step 2/8 : EXPOSE 8000\n",
      " ---> Using cache\n",
      " ---> 6483e4a811d5\n",
      "Step 3/8 : EXPOSE 8001\n",
      " ---> Using cache\n",
      " ---> 36f81f5b7f47\n",
      "Step 4/8 : EXPOSE 8002\n",
      " ---> Using cache\n",
      " ---> 541852b52454\n",
      "Step 5/8 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> b625e263b707\n",
      "Step 6/8 : RUN mkdir /model\n",
      " ---> Using cache\n",
      " ---> 959204525114\n",
      "Step 7/8 : COPY /models/ /model/models/\n",
      " ---> 5cc88f76e499\n",
      "Step 8/8 : CMD [\"tritonserver\", \"--model-repository=/model/models/\", \"--backend-config=hugectr,ps=/model/models/ps.json\"]\n",
      " ---> Running in c840754e3766\n",
      "Removing intermediate container c840754e3766\n",
      " ---> 0c248610ce10\n",
      "Successfully built 0c248610ce10\n",
      "Successfully tagged gcr.io/merlin-on-gcp/triton_deploy-hugectr:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/merlin-on-gcp/triton_deploy-hugectr]\n",
      "\n",
      "\u001b[1B7e30de7f: Preparing \n",
      "\u001b[1B6660d413: Preparing \n",
      "\u001b[1Be58e8598: Preparing \n",
      "\u001b[1Bc9824aed: Preparing \n",
      "\u001b[1B23fe2ec9: Preparing \n",
      "\u001b[1Beb0eea4e: Preparing \n",
      "\u001b[1B08de1536: Preparing \n",
      "\u001b[1Bd698577a: Preparing \n",
      "\u001b[1B21cc7b5f: Preparing \n",
      "\u001b[1B646f53eb: Preparing \n",
      "\u001b[1Bf0fe3ed8: Preparing \n",
      "\u001b[1B7d455483: Preparing \n",
      "\u001b[1B026e0fab: Preparing \n",
      "\u001b[1B9b492bd7: Preparing \n",
      "\u001b[1B2784440c: Preparing \n",
      "\u001b[1B2f6421c3: Preparing \n",
      "\u001b[1B8a55152d: Preparing \n",
      "\u001b[1B589ca15a: Preparing \n",
      "\u001b[1B0a50ea64: Preparing \n",
      "\u001b[1B64c9831d: Preparing \n",
      "\u001b[1B42df9148: Preparing \n",
      "\u001b[1B4e7260d1: Preparing \n",
      "\u001b[1B01b2e862: Preparing \n",
      "\u001b[1Bda137864: Preparing \n",
      "\u001b[1B5d96cc97: Preparing \n",
      "\u001b[1B41d8a723: Preparing \n",
      "\u001b[1B8118241f: Preparing \n",
      "\u001b[1B75c0649e: Preparing \n",
      "\u001b[1B357bd24c: Preparing \n",
      "\u001b[1Bb05908c8: Preparing \n",
      "\u001b[1B4217793b: Preparing \n",
      "\u001b[25B698577a: Waiting g \n",
      "\u001b[1B7c006461: Preparing \n",
      "\u001b[21Bb492bd7: Waiting g \n",
      "\u001b[27B1cc7b5f: Waiting g \n",
      "\u001b[22B784440c: Waiting g \n",
      "\u001b[28B46f53eb: Waiting g \n",
      "\u001b[28B0fe3ed8: Waiting g \n",
      "\u001b[23Ba55152d: Waiting g \n",
      "\u001b[29Bd455483: Waiting g \n",
      "\u001b[29B26e0fab: Waiting g \n",
      "\u001b[1Bd199c000: Preparing \n",
      "\u001b[1B35c73479: Preparing \n",
      "\u001b[1Bf46ba1c7: Preparing \n",
      "\u001b[1B6a18e033: Preparing \n",
      "\u001b[1Bb0b86b83: Preparing \n",
      "\u001b[30B89ca15a: Waiting g \n",
      "\u001b[30Ba50ea64: Waiting g \n",
      "\u001b[1Bc3489c5c: Preparing \n",
      "\u001b[31B4c9831d: Waiting g \n",
      "\u001b[1B4d6d50a7: Preparing \n",
      "\u001b[1B2b1df38f: Preparing \n",
      "\u001b[19B046cdd7: Waiting g \n",
      "\u001b[19B8382770: Waiting g \n",
      "\u001b[19B8abd40a: Waiting g \n",
      "\u001b[19B25c8271: Waiting g \n",
      "\u001b[1B13f0a43e: Preparing \n",
      "\u001b[17B199c000: Waiting g \n",
      "\u001b[1B6d5c90ce: Preparing \n",
      "\u001b[18B5c73479: Waiting g \n",
      "\u001b[1B7c57c2d4: Preparing \n",
      "\u001b[1Bbcfb7e1f: Preparing \n",
      "\u001b[20B46ba1c7: Waiting g \n",
      "\u001b[15B4750603: Waiting g \n",
      "\u001b[60Bb0eea4e: Waiting g \n",
      "\u001b[1Bfa616420: Preparing \n",
      "\u001b[1B762bfd28: Preparing \n",
      "\u001b[1B316f3ae7: Preparing \n",
      "\u001b[1B2fa93862: Preparing \n",
      "\u001b[14B3f0a43e: Waiting g \n",
      "\u001b[1B96ef8c35: Preparing \n",
      "\u001b[66B8de1536: Waiting g \n",
      "\u001b[13Bc57c2d4: Waiting g \n",
      "\u001b[1B1da8e28f: Preparing \n",
      "\u001b[14Bcfb7e1f: Waiting g \n",
      "\u001b[1B8f6f0e57: Preparing \n",
      "\u001b[77Be30de7f: Pushed   1.133GB/1.133GB\u001b[72A\u001b[2K\u001b[68A\u001b[2K\u001b[67A\u001b[2K\u001b[64A\u001b[2K\u001b[60A\u001b[2K\u001b[57A\u001b[2K\u001b[54A\u001b[2K\u001b[50A\u001b[2K\u001b[77A\u001b[2K\u001b[48A\u001b[2K\u001b[77A\u001b[2K\u001b[40A\u001b[2K\u001b[77A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[29A\u001b[2K\u001b[27A\u001b[2K\u001b[77A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2K\u001b[77A\u001b[2Klatest: digest: sha256:52556671041d66b0d0b0b7f51771cb4a1ec1366e6acecc09b376a244841be057 size: 16502\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {IMAGE_URI} -f {DOCKERFILE} src\n",
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dead7-e920-4417-b61a-d8a9ccc9c7d7",
   "metadata": {},
   "source": [
    "### Configure a custom prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247c919b-1855-4040-bac2-af1d41fa1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{IMAGE_NAME}-deepfm-v{VERSION}\"\n",
    "model_description = \"Serving with Triton inference server using a custom container\"\n",
    "\n",
    "health_route = \"/v2/health/ready\"\n",
    "predict_route = f\"/v2/models/deepfm_ens/infer\"\n",
    "serving_container_ports = [8000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c1220-bf18-4256-90e8-47b125734c4c",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa84abf4-3b8a-4d79-9490-58cc569350bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/659831510405/locations/us-central1/models/1764839307882790912/operations/5824600313419530240\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/659831510405/locations/us-central1/models/1764839307882790912\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/659831510405/locations/us-central1/models/1764839307882790912')\n",
      "triton_deploy-hugectr-deepfm-v1\n",
      "projects/659831510405/locations/us-central1/models/1764839307882790912\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/659831510405/locations/us-central1/endpoints/5806274615680434176/operations/2118700770047033344\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/659831510405/locations/us-central1/endpoints/5806274615680434176\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/659831510405/locations/us-central1/endpoints/5806274615680434176')\n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{IMAGE_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Set deployment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16274eb5-feb6-48d4-9fa7-fea87b1da376",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd27dda-7c1b-4ce5-9477-e62a9f1199bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/659831510405/locations/us-central1/endpoints/5806274615680434176\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/659831510405/locations/us-central1/endpoints/5806274615680434176/operations/5089598780483829760\n"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "### Getting inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c0a8045-35c3-4395-a93f-c4f7ab21c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"1\",\"model_name\":\"deepfm_ens\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[3],\"data\":[0.06609038263559342,0.07316402345895767,0.08091689646244049]}]}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\"  \\\n",
    "  https://europe-west2-aiplatform.googleapis.com/v1/projects/merlin-on-gcp/locations/europe-west2/endpoints/5851653659582005248:rawPredict \\\n",
    "  -d @criteo.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m80"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
