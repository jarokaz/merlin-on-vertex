{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da9d32d",
   "metadata": {},
   "source": [
    "# NVtabular preprocessing pipeline prototype\n",
    "\n",
    "This is a prototype of a Vertex Pipeline pipeline that uses NVIDIA NVTabular for feature engineering and data preprocessing. During prototyping Jupyter notebook is used for developing and submitting test runs. The final sample will be refactored to Python modules.\n",
    "\n",
    "![NVT pipeline](images/nvt-pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56703e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import json\n",
    "\n",
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath)\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64defe89",
   "metadata": {},
   "source": [
    "## Configure Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c920bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "STAGING_BUCKET = 'gs://jk-vertex-us-central1'\n",
    "VERTEX_SA = f'vertex-sa@{PROJECT_ID}.iam.gserviceaccount.com'\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac1983",
   "metadata": {},
   "source": [
    "## Define KFP components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06343aa",
   "metadata": {},
   "source": [
    "### Prepare a base image for NVTabular\n",
    "\n",
    "Eventually, we will use the NGC Merlin image. In the interim we will use a custom image based on a GCP DL container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1c402",
   "metadata": {},
   "source": [
    "#### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c348184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cu110\n",
    "\n",
    "WORKDIR /nvtabular\n",
    "\n",
    "RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3  cudatoolkit=11.0\n",
    "\n",
    "ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b8b4",
   "metadata": {},
   "source": [
    "#### Build and push the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72a1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE_NAME = f'gcr.io/{PROJECT_ID}/nvt_base_image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t {BASE_IMAGE_NAME} . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push {BASE_IMAGE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2cda3",
   "metadata": {},
   "source": [
    "### Data ingestion component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74416bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=BASE_IMAGE_NAME)\n",
    "def ingest_csv_op(\n",
    "    train_files: list,\n",
    "    valid_files: list,\n",
    "    sep: str,\n",
    "    schema: list,\n",
    "    gpus: list,\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    from dask.distributed import Client\n",
    "    \n",
    "    TRAIN_SPLIT_FOLDER = 'train'\n",
    "    VALID_SPLIT_FOLDER = 'valid'\n",
    "    \n",
    "    client = None\n",
    "    if len(gpus) > 1:\n",
    "        logging.info('Creating a Dask CUDA cluster')\n",
    "        cluster = LocalCUDACluster(\n",
    "            CUDA_VISIBLE_DEVICES=','.join(gpus),\n",
    "            n_workers=len(gpus)\n",
    "        )\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    names = [feature[0] for feature in schema]\n",
    "    dtypes = {feature[0]: feature[1] for feature in schema}\n",
    "    \n",
    "    for folder_name, files in zip([TRAIN_SPLIT_FOLDER, VALID_SPLIT_FOLDER], [train_files, valid_files]):\n",
    "        dataset = nvt.Dataset(\n",
    "            path_or_source = files,\n",
    "            engine='csv',\n",
    "            names=names,\n",
    "            sep=sep,\n",
    "            dtypes=dtypes,\n",
    "            client=client\n",
    "        )\n",
    "        \n",
    "        output_path = os.path.join(output_dataset.uri, folder_name)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        logging.info('Writing a parquet file to {}'.format(output_path))\n",
    "        dataset.to_parquet(\n",
    "            output_path=output_path,\n",
    "            preserve_files=True\n",
    "        )\n",
    "    \n",
    "    output_dataset.metadata['split_names'] = [TRAIN_SPLIT_FOLDER, VALID_SPLIT_FOLDER]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c13e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=BASE_IMAGE_NAME)\n",
    "def fit_workflow_op(\n",
    "    dataset: Input[Dataset],\n",
    "    fitted_workflow: Output[Artifact],\n",
    "    gpus: list,\n",
    "    part_mem_frac: Optional[float]=0.1,\n",
    "    device_limit_frac: Optional[float]=0.7,\n",
    "    device_pool_frac: Optional[float]=0.8,\n",
    "    split_name: Optional[str]='train'\n",
    "):\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "    import numpy as np\n",
    "    \n",
    "    from pathlib import Path\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    from dask.distributed import Client\n",
    "    from nvtabular.utils import _pynvml_mem_size, device_mem_size\n",
    "    \n",
    "    from nvtabular.ops import (\n",
    "        Categorify,\n",
    "        Clip,\n",
    "        FillMissing,\n",
    "        Normalize,\n",
    "    )\n",
    "    \n",
    "    STATS_FOLDER = 'stats'\n",
    "    WORKFLOW_FOLDER = 'workflow'\n",
    "    \n",
    "    if not split_name in dataset.metadata['split_names']:\n",
    "        raise RuntimeError('Dataset does not have {} split'.format(split_name))\n",
    "        \n",
    " \n",
    "    CONTINUOUS_COLUMNS = [\"I\" + str(x) for x in range(1, 14)]\n",
    "    CATEGORICAL_COLUMNS = [\"C\" + str(x) for x in range(1, 27)]\n",
    "    LABEL_COLUMNS = [\"label\"]\n",
    "    COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS\n",
    "\n",
    "    \n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    part_size = int(part_mem_frac * device_size)\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    \n",
    "    client = None\n",
    "    if len(gpus) > 1:\n",
    "        logging.info('Creating a Dask CUDA cluster')\n",
    "        cluster = LocalCUDACluster(\n",
    "            CUDA_VISIBLE_DEVICES=','.join(gpus),\n",
    "            n_workers=len(gpus),\n",
    "            device_memory_limit=device_limit,\n",
    "            rmm_pool_size=(device_pool_size // 256) * 256\n",
    "        )\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    num_buckets = 10000000\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    features = cat_features + cont_features + LABEL_COLUMNS\n",
    "\n",
    "    workflow = nvt.Workflow(features, client=client)  \n",
    "    \n",
    "    train_paths = [str(path) for path in Path(dataset.uri, split_name).glob('*.parquet')]\n",
    "    train_dataset = nvt.Dataset(train_paths, engine=\"parquet\", part_size=part_size)\n",
    "    \n",
    "    workflow.fit(train_dataset)\n",
    "    workflow.save(fitted_workflow.uri)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad60084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numba.cuda.cudadrv.driver:init\n",
      "INFO:root:Creating a Dask CUDA cluster\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nvtabular.workflow.Workflow at 0x7fd59bd6e9d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DatasetMock:\n",
    "    def __init__(self, uri):\n",
    "        self.uri = uri\n",
    "        self.metadata = {}\n",
    "        \n",
    "input_dataset = DatasetMock(uri='/home/jupyter/output')\n",
    "input_dataset.metadata['split_names'] = ['train', 'valid']\n",
    "\n",
    "fitted_workflow = DatasetMock(uri='/home/jupyter/workflow')\n",
    "gpus = ['0','1']\n",
    "\n",
    "\n",
    "fit_workflow_op(\n",
    "    dataset=input_dataset, \n",
    "    fitted_workflow=fitted_workflow,\n",
    "    gpus=gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904048c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd7de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b0a58d21",
   "metadata": {},
   "source": [
    "cont_features = [[name, \"int32\"] for name in [\"I\" + str(x) for x in range(1, 14)]]\n",
    "cat_features = [[name, \"hex\"] for name in [\"C\" + str(x) for x in range(1, 27)]]\n",
    "schema = [['label', 'int32']] + cont_features + cat_features\n",
    "sep = \"\\t\"\n",
    "gpus = ['0','1']\n",
    "\n",
    "train_files = ['/home/jupyter/criteo/criteo_orig/day_0']\n",
    "valid_files = ['/home/jupyter/criteo/criteo_orig/day_1']\n",
    "\n",
    "class DatasetMock:\n",
    "    def __init__(self, uri):\n",
    "        self.uri = uri\n",
    "        self.metadata = {}\n",
    "\n",
    "output_dataset = DatasetMock(uri='/home/jupyter/output')\n",
    "\n",
    "ingest_csv_op(train_files, valid_files, sep, schema, gpus, output_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dba0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c583e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = 'nvt-test-pipeline'\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME\n",
    ")\n",
    "def nvt_pipeline(\n",
    "    train_files: list,\n",
    "    valid_files: list,\n",
    "    sep: str,\n",
    "    gpus: list,\n",
    "    schema: list,\n",
    "):\n",
    "    ingest_csv_files = ingest_csv_op(\n",
    "        train_files=train_files,\n",
    "        valid_files=valid_files,\n",
    "        sep=sep,\n",
    "        gpus=gpus,\n",
    "        schema=schema,\n",
    "    )\n",
    "    ingest_csv_files.set_cpu_limit(\"48\")\n",
    "    ingest_csv_files.set_memory_limit(\"312G\")\n",
    "    ingest_csv_files.set_gpu_limit(\"4\")\n",
    "    ingest_csv_files.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-t4')\n",
    "    \n",
    "    fit_workflow = fit_workflow_op(\n",
    "        dataset=ingest_csv_files.outputs['output_dataset'],\n",
    "        gpus=gpus\n",
    "    )\n",
    "    fit_workflow.set_cpu_limit(\"48\")\n",
    "    fit_workflow.set_memory_limit(\"312G\")\n",
    "    fit_workflow.set_gpu_limit(\"4\")\n",
    "    fit_workflow.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-t4')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54318c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = 'nvt_pipeline.json'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=nvt_pipeline,\n",
    "    package_path=package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5145b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/895222332033/locations/us-central1/pipelineJobs/nvt-test-pipeline-20210916051700\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/895222332033/locations/us-central1/pipelineJobs/nvt-test-pipeline-20210916051700')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/nvt-test-pipeline-20210916051700?project=895222332033\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/895222332033/locations/us-central1/pipelineJobs/nvt-test-pipeline-20210916051700 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/895222332033/locations/us-central1/pipelineJobs/nvt-test-pipeline-20210916051700 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/895222332033/locations/us-central1/pipelineJobs/nvt-test-pipeline-20210916051700 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/895222332033/locations/us-central1/pipelineJobs/nvt-test-pipeline-20210916051700 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/895222332033/locations/us-central1/pipelineJobs/nvt-test-pipeline-20210916051700 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job_name = 'test_pipeline_run'\n",
    "\n",
    "cont_features = [[name, \"int32\"] for name in [\"I\" + str(x) for x in range(1, 14)]]\n",
    "cat_features = [[name, \"hex\"] for name in [\"C\" + str(x) for x in range(1, 27)]]\n",
    "schema = [['label', 'int32']] + cont_features + cat_features\n",
    "sep = \"\\t\"\n",
    "gpus = ['0','1']\n",
    "\n",
    "train_files = ['/gcs/jk-criteo-bucket/criteo_orig/day_0']\n",
    "valid_files = ['/gcs/jk-criteo-bucket/criteo_orig/day_1']\n",
    "\n",
    "params = {\n",
    "    'train_files': json.dumps(train_files),\n",
    "    'valid_files': json.dumps(valid_files),\n",
    "    'sep': \"\\t\",\n",
    "    'schema': json.dumps(schema),\n",
    "    'gpus': json.dumps(gpus)\n",
    "}\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=package_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=params,\n",
    ")\n",
    "\n",
    "pipeline_job.run(\n",
    "    service_account=VERTEX_SA,\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195c085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
