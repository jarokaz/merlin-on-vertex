{
  "pipelineSpec": {
    "components": {
      "comp-fit-workflow-op": {
        "executorLabel": "exec-fit-workflow-op",
        "inputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "device_limit_frac": {
              "type": "DOUBLE"
            },
            "device_pool_frac": {
              "type": "DOUBLE"
            },
            "gpus": {
              "type": "STRING"
            },
            "part_mem_frac": {
              "type": "DOUBLE"
            },
            "split_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "fitted_workflow": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-ingest-csv-op": {
        "executorLabel": "exec-ingest-csv-op",
        "inputDefinitions": {
          "parameters": {
            "gpus": {
              "type": "STRING"
            },
            "schema": {
              "type": "STRING"
            },
            "sep": {
              "type": "STRING"
            },
            "train_files": {
              "type": "STRING"
            },
            "valid_files": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-fit-workflow-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "fit_workflow_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef fit_workflow_op(\n    dataset: Input[Dataset],\n    fitted_workflow: Output[Artifact],\n    gpus: list,\n    part_mem_frac: Optional[float]=0.1,\n    device_limit_frac: Optional[float]=0.7,\n    device_pool_frac: Optional[float]=0.8,\n    split_name: Optional[str]='train'\n):\n    import logging\n    import nvtabular as nvt\n    import numpy as np\n\n    from pathlib import Path\n    from dask_cuda import LocalCUDACluster\n    from dask.distributed import Client\n    from nvtabular.utils import _pynvml_mem_size, device_mem_size\n\n    from nvtabular.ops import (\n        Categorify,\n        Clip,\n        FillMissing,\n        Normalize,\n    )\n\n    STATS_FOLDER = 'stats'\n    WORKFLOW_FOLDER = 'workflow'\n\n    if not split_name in dataset.metadata['split_names']:\n        raise RuntimeError('Dataset does not have {} split'.format(split_name))\n\n\n    CONTINUOUS_COLUMNS = [\"I\" + str(x) for x in range(1, 14)]\n    CATEGORICAL_COLUMNS = [\"C\" + str(x) for x in range(1, 27)]\n    LABEL_COLUMNS = [\"label\"]\n    COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS\n\n\n    device_size = device_mem_size(kind=\"total\")\n    part_size = int(part_mem_frac * device_size)\n    device_limit = int(device_limit_frac * device_size)\n    device_pool_size = int(device_pool_frac * device_size)\n\n    client = None\n    if len(gpus) > 1:\n        logging.info('Creating a Dask CUDA cluster')\n        cluster = LocalCUDACluster(\n            CUDA_VISIBLE_DEVICES=','.join(gpus),\n            n_workers=len(gpus),\n            device_memory_limit=device_limit,\n            rmm_pool_size=(device_pool_size // 256) * 256\n        )\n        client = Client(cluster)\n\n    num_buckets = 10000000\n    categorify_op = Categorify(max_size=num_buckets)\n    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n    features = cat_features + cont_features + LABEL_COLUMNS\n\n    workflow = nvt.Workflow(features, client=client)  \n\n    train_paths = [str(path).replace('gs:/', '/gcs') \n                   for path in Path(dataset.uri, split_name).glob('*.parquet')]\n    print('*****************')\n    print(dataset.uri)\n    print(train_paths)\n    train_dataset = nvt.Dataset(train_paths, engine=\"parquet\", part_size=part_size)\n\n    workflow.fit(train_dataset)\n    workflow.save(fitted_workflow.uri)\n\n"
            ],
            "image": "gcr.io/jk-mlops-dev/nvt_base_image",
            "resources": {
              "accelerator": {
                "count": "4",
                "type": "NVIDIA_TESLA_T4"
              },
              "cpuLimit": 48.0,
              "memoryLimit": 312.0
            }
          }
        },
        "exec-ingest-csv-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "ingest_csv_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef ingest_csv_op(\n    train_files: list,\n    valid_files: list,\n    sep: str,\n    schema: list,\n    gpus: list,\n    output_dataset: Output[Dataset]\n):\n    import logging\n    import nvtabular as nvt\n    import numpy as np\n    import os\n\n    from dask_cuda import LocalCUDACluster\n    from dask.distributed import Client\n\n    TRAIN_SPLIT_FOLDER = 'train'\n    VALID_SPLIT_FOLDER = 'valid'\n\n    client = None\n    if len(gpus) > 1:\n        logging.info('Creating a Dask CUDA cluster')\n        cluster = LocalCUDACluster(\n            CUDA_VISIBLE_DEVICES=','.join(gpus),\n            n_workers=len(gpus)\n        )\n        client = Client(cluster)\n\n    names = [feature[0] for feature in schema]\n    dtypes = {feature[0]: feature[1] for feature in schema}\n\n    for folder_name, files in zip([TRAIN_SPLIT_FOLDER, VALID_SPLIT_FOLDER], [train_files, valid_files]):\n        dataset = nvt.Dataset(\n            path_or_source = files,\n            engine='csv',\n            names=names,\n            sep=sep,\n            dtypes=dtypes,\n            client=client\n        )\n\n        output_path = os.path.join(output_dataset.uri, folder_name)\n        os.makedirs(output_path, exist_ok=True)\n\n        logging.info('Writing a parquet file to {}'.format(output_path))\n        dataset.to_parquet(\n            output_path=output_path,\n            preserve_files=True\n        )\n\n    output_dataset.metadata['split_names'] = [TRAIN_SPLIT_FOLDER, VALID_SPLIT_FOLDER]\n\n"
            ],
            "image": "gcr.io/jk-mlops-dev/nvt_base_image",
            "resources": {
              "accelerator": {
                "count": "4",
                "type": "NVIDIA_TESLA_T4"
              },
              "cpuLimit": 48.0,
              "memoryLimit": 312.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "nvt-test-pipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "fit-workflow-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-fit-workflow-op"
            },
            "dependentTasks": [
              "ingest-csv-op"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_dataset",
                    "producerTask": "ingest-csv-op"
                  }
                }
              },
              "parameters": {
                "device_limit_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.7
                    }
                  }
                },
                "device_pool_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.8
                    }
                  }
                },
                "gpus": {
                  "componentInputParameter": "gpus"
                },
                "part_mem_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.1
                    }
                  }
                },
                "split_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "train"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "fit-workflow-op"
            }
          },
          "ingest-csv-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-ingest-csv-op"
            },
            "inputs": {
              "parameters": {
                "gpus": {
                  "componentInputParameter": "gpus"
                },
                "schema": {
                  "componentInputParameter": "schema"
                },
                "sep": {
                  "componentInputParameter": "sep"
                },
                "train_files": {
                  "componentInputParameter": "train_files"
                },
                "valid_files": {
                  "componentInputParameter": "valid_files"
                }
              }
            },
            "taskInfo": {
              "name": "ingest-csv-op"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "gpus": {
            "type": "STRING"
          },
          "schema": {
            "type": "STRING"
          },
          "sep": {
            "type": "STRING"
          },
          "train_files": {
            "type": "STRING"
          },
          "valid_files": {
            "type": "STRING"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.1"
  },
  "runtimeConfig": {}
}