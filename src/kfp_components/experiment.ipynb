{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering for large scale recommenders with NVIDIA NVTabular and Vertex AI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "The focus of this guide is to compile prescriptive guidelines for developing and operationalizing data preprocessing and feature engineering workflows using Vertex AI and NVIDIA NVTabular.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).  \n",
    "\n",
    "### From the Criteo website:\n",
    "\n",
    "#### Overview\n",
    " - This dataset contains feature values and click feedback for millions of display ads. Its purpose is to benchmark algorithms for clickthrough rate (CTR) prediction.\n",
    " - This dataset contains 24 files, each one corresponding to one day of data.\n",
    " - Each row corresponds to a display ad served by Criteo and the first column is indicates whether this ad has been clicked or not. The positive (clicked) and negatives  (non-clicked) examples have both been subsampled (but at different rates) in order to reduce the dataset size.\n",
    " - There are 13 features taking integer values (mostly count features) and 26 categorical features. The values of the categorical features have been hashed\n",
    "onto 32 bits for anonymization purposes.\n",
    " - The semantic of these features is undisclosed. Some features may have missing values.\n",
    " - The rows are chronologically ordered.\n",
    "\n",
    "#### Data fields\n",
    " - Label - Target variable that indicates if an ad was clicked (1) or not (0).\n",
    " - I1-I13 - A total of 13 columns of integer features (mostly count features).\n",
    " - C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes. \n",
    " - The semantic of the features is undisclosed.  \n",
    " - When a value is missing, the field is empty.\n",
    "\n",
    "#### Format\n",
    "The columns are tab separated with the following schema:  \n",
    "<label> <integer feature 1> … <integer feature 13> <categorical feature 1> … <categorical feature 26>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objective\n",
    "This notebook demonstrates how to do data preprocessing with NVIDIA NVTabular on Vertex AI Pipeline steps.  \n",
    "You will create three different pipelines to understand possible alternatives on how to manipulate data.\n",
    "\n",
    "*Pipeline 1*: Input CSV files from Google Cloud Storage (GCS for short)  \n",
    " - Read CSV files from GCS\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    "\n",
    "*Pipeline 2*: Input Parquet files exported from BigQuery\n",
    " - Export Parquer files from a table in Bigquery\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    "\n",
    "*Pipeline 3*: Input CSV files from GCS and output to Vertex AI Feature Store \n",
    " - Read CSV files from Google Cloud Storage\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    " - Load the transformed data to BigQuery\n",
    " - Create a Vertex AI Feature Store and load the data from BigQuery\n",
    "\n",
    "The goal is to present how to use NVTabular to transform the data on GPU and different ways of inputing (GCS and Bigquery) and outputing (GCS, BigQuery, FeatureStore) data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Costs\n",
    "This tutorial uses billable components of Google Cloud (GCP):\n",
    " - Vertex AI (Pipelines, FeatureStore, GPUs, Training)\n",
    " - Cloud Storage\n",
    " - BigQuery\n",
    " - Cloud Artifact Registry\n",
    "\n",
    "Use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instalation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up your local development environment\n",
    "\n",
    "If you are using Google Cloud Notebooks, your environment already meets all the requirements to run this notebook. You can skip this step.\n",
    "\n",
    "Otherwise, make sure your environment meets this notebook's requirements. You need the following:\n",
    " - The Google Cloud SDK\n",
    " - Git\n",
    " - Python 3\n",
    " - virtualenv\n",
    " - Jupyter notebook running in a virtual environment with Python 3\n",
    " - Docker\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development environment](https://cloud.google.com/python/setup) and the [Jupyter installation guide](https://jupyter.org/install) provide detailed instructions for meeting these requirements. The following steps provide a condensed set of instructions:\n",
    "\n",
    " - [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    " - [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    " - [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    " - To install Jupyter, run pip install jupyter on the command-line in a terminal shell.\n",
    " - To launch Jupyter, run jupyter notebook on the command-line in a terminal shell.\n",
    " - Open this notebook in the Jupyter Notebook Dashboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install additional packages\n",
    "\n",
    "To run build and run the pipelines, you need to install Kubeflow Pipelines SDK, Vertex AI SDK and NVTabular SDK locally."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup user flag specific for your environment\n",
    "\n",
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Install kfp, vertex and nvtabular SDKs\n",
    "! pip install {USER_FLAG} --upgrade kfp google-cloud-aiplatform nvtabular"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restart the kernel\n",
    "After installing the additional packages, you need to restart the notebook kernel so it can find the packages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the versions of the packages you installed. The KFP SDK version should be >=1.6."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Before you begin\n",
    "This notebook does not require a GPU runtime, but the pipeline steps running on GCP will.\n",
    "\n",
    "## Set up your Google Cloud project\n",
    "### The following steps are required, regardless of your notebook environment.\n",
    " - [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    " - [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    " - [Enable the Vertex AI, Cloud Storage, BigQuery, Compute Engine, Cloud Build and Artifact Registry APIs.](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com,bigquery.googleapis.com,cloudbuild.googleapis.com,artifactregistry.googleapis.com)\n",
    " - Follow the \"Configuring your project\" instructions from the Vertex Pipelines documentation.\n",
    " - If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    " - Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with ! as shell commands, and it interpolates Python variables prefixed with $ into these commands."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set your project ID\n",
    "### If you don't know your project ID, you may be able to get your project ID using gcloud."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output= ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otherwise, set your project ID here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"<YOUR_GCP_PROJECT_HERE>\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Timestamp\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Authenticate your Google Cloud account\n",
    "### **If you are using Google Cloud Notebooks or a GCE VM, your environment is already authenticated with the default service account. Skip this step.**\n",
    "\n",
    "If you are using Colab, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
    "\n",
    "Otherwise, follow these steps:\n",
    " - In the Cloud Console, go to the [Create service account key page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    " - Click **Create service account**.\n",
    " - In the **Service account** name field, enter a name, and click **Create**.\n",
    " - In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**. Type \"Bigquery Object Admin\" into the filter box, and select **BigQuery Admin**.\n",
    " - Click Create. A JSON file that contains your key downloads to your local environment.\n",
    " - Enter the path to your service account key as the **GOOGLE_APPLICATION_CREDENTIALS** variable in the cell below and run the cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Cloud Storage bucket as necessary\n",
    "You will need a Cloud Storage bucket for this example. If you don't have one that you want to use, you can make one now.  \n",
    "The bucket will be used to store the following artifacts:\n",
    " - Temporary files\n",
    " - NVTabular workflow files\n",
    " - Transformed and preprocessed files\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
    "\n",
    "You may also change the REGION variable, which is used for operations throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may not use a Multi-Regional Storage bucket for training with Vertex AI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BUCKET_NAME = \"gs://<YOUR-BUCKET-NAME>\" # <------- Change HERE\n",
    "REGION = \"us-central1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://<YOUR-BUCKET-NAME>\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Only if your bucket doesn't already exist**: Uncomment and run the following cell to create your Cloud Storage bucket."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_NAME"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the docker image\n",
    "In order to execute each step in the pipeline, you need to build a docker image with the required packages and push it to Artifact Registry.  \n",
    "Follow these steps to create an Artifact Registry for your docker images:\n",
    "\n",
    "1) [Choose a shell](https://cloud.google.com/artifact-registry/docs/docker/quickstart) to execute the gcloud commands\n",
    "2) Define the name of your registry in the next cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "REG_NAME = 'quickstart-docker-repo' # <----- Change HERE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Run the following command to create a new Docker repository in the location us-central1 with the description \"docker repository\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gcloud artifacts repositories create f'{REG_NAME}' --repository-format=docker \\\n",
    "--location=us-central1 --description=\"Docker repository\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Run the following command to verify that your repository was created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gcloud artifacts repositories list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5) **IMPORTANT**: [Configure Authentication](https://cloud.google.com/artifact-registry/docs/docker/quickstart#auth)\n",
    "6) Create a Dockerfile with the next cell and build the image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%file Dockerfile.test\n",
    "FROM gcr.io/deeplearning-platform-release/base-cu110\n",
    "\n",
    "WORKDIR /nvtabular\n",
    "\n",
    "RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3 cudatoolkit=11.0\n",
    "RUN pip install google-cloud-aiplatform kfp\n",
    "\n",
    "ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "ENV PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "7) Define a tag for your docker image, build and push it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "IMG_NAME = 'nvt-preprocessing' # <------ Change HERE\n",
    "\n",
    "IMG_REGION = 'us-central1-docker.pkg.dev'\n",
    "IMG_TAG = f'{IMG_REGION}/{PROJECT_ID}/{REG_NAME}/{IMG_NAME}'\n",
    "! docker build -t IMG_TAG .\n",
    "! docker push IMG_TAG"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "8) Make sure your image was pushed to the registry."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gcloud artifacts docker images list IMG_TAG"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quotas\n",
    "\n",
    "Make sure you have enough quota (GPU, CPU and Memory) for your environment before proceeding.  \n",
    "On GCP console, visit [IAM & Admin > Quotas](https://console.cloud.google.com/iam-admin/quotas)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries and define constants\n",
    "Define some constants."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "USER = '<YOUR-USERNAME-HERE>'  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do some imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Standard\n",
    "import json\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    Clip,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing with NVTabular: Vertex AI Pipelines definitions\n",
    "\n",
    "### Pipeline Components\n",
    "The components were defined as python functions in the file `kfp_components.py`.  \n",
    "Each component is annotated with Inputs or Outputs (or both) to keep track of lineage metadata.\n",
    "\n",
    "The BASE_IMAGE used to execute the components is the same docker image you built a few steps back.\n",
    "\n",
    "### Pipeline step configuration\n",
    "Most of the NVTabular preprocessing are executed as steps in the Pipeline.  \n",
    "Some steps require a more robust runtime configuration with more CPU, memory and GPU.  \n",
    "\n",
    "Each pipeline definition has its own runtime configurations that can be set directly to the component execution.  \n",
    "In Vertex AI Pipelines, you can set the amount of CPU, memory and GPU in the pipeline specification, like this:\n",
    "\n",
    "```\n",
    "    component_being_executed.set_cpu_limit(\"8\") # Number of CPUs\n",
    "    component_being_executed.set_memory_limit(\"32G\") # Memory quantity\n",
    "    component_being_executed.set_gpu_limit(\"1\") # Number of GPUs\n",
    "    component_being_executed.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-t4') # GPU type\n",
    "```\n",
    "\n",
    "### Execution order\n",
    "Each pipeline specification presented in the following cells are self-contained, that is, you can choose which one is more adequate for your needs.  \n",
    "If you want to experiment the Pipeline #2, there is no need to execute the first or the third."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Pipeline: Input CSV files from GCS\n",
    "\n",
    "This pipeline will execute the following steps:\n",
    "\n",
    "*Pipeline 1*: \n",
    " - Read CSV files from GCS\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    "\n",
    " Architecture overview:\n",
    "\n",
    "<img src=\"../../images/pipeline_1.png\" alt=\"Pipeline\" style=\"height: 60%; width:60%;\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The original Criteo dataset is in CSV format, but the recommended format to run the NVTabular preprocessing step is in PARQUET.  \n",
    "The first step of this pipeline read the data from GCS, converts the data from CSV to PARQUET using the `nvtabular.Dataset.to_parquet` method, and write the converted data back to GCS.\n",
    "\n",
    "The second step in the pipeline calculates statistics on the dataset based on the transformations defined in the `nvtabular.Workflow`.  \n",
    "Finally, the last step is executed to transform the dataset and write back to GCS."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import components and pipeline definition\n",
    "from pipeline_gcs import preprocessing_pipeline_gcs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To convert the CSV to PARQUET you need to pass a list with the column names and a dictionary mapping the column name to the type of the column.  \n",
    "This data will be used to instantiate the classe `nvtabular.Dataset` and consequently call the `.to_parquet` method.\n",
    "\n",
    "This dataset was two different types: `int32` and `hex` (hexadecimal strings that should be converted to int32)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Columns and dtypes definition\n",
    "cont_names = [\"I\" + str(x) for x in range(1, 14)]\n",
    "cat_names = [\"C\" + str(x) for x in range(1, 27)]\n",
    "columns = [\"label\"] + cont_names + cat_names\n",
    "\n",
    "# Specify column dtypes. Note that \"hex\" means that\n",
    "# the values will be hexadecimal strings that should\n",
    "# be converted to int32\n",
    "cols_dtype = {}\n",
    "cols_dtype[\"label\"] = 'int32'\n",
    "for x in cont_names:\n",
    "    cols_dtype[x] = 'int32'\n",
    "for x in cat_names:\n",
    "    cols_dtype[x] = 'hex'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To transform the data using NVTabular you must create a DAG with the transformation steps.  \n",
    "NVTabular implements several operators to processes your dataset. They can be found [HERE](https://nvidia.github.io/NVTabular/v0.4.0/resources/api/ops/index.html).\n",
    "\n",
    "In this example the DAG perform the following transformations:\n",
    " - Categorical features (columns that start with C): Apply [Categorify](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/categorify.html)\n",
    " - Continuous features (columns that start with I): Apply [FillMissing](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/fillmissing.html), [Clip](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/clip.html) and [Normalize](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/normalize.html) (in this order).\n",
    "\n",
    " The definition of the Workflow will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " It will be uploaded to GCS for future use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transformation pipeline\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(max_size=num_buckets)\n",
    "cat_features = cat_names >> categorify_op\n",
    "cont_features = cont_names >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "features = cat_features + cont_features + ['label']\n",
    "\n",
    "# Create and save workflow\n",
    "workflow = nvt.Workflow(features)\n",
    "workflow.save('./saved_workflow')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Upload to GCS\n",
    "! gsutil cp -r ./saved_workflow BUCKET_NAME"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's define some variables necessary to execute the pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BUCKET = BUCKET_NAME[5:] # retrive the bucket name only, without prefix\n",
    "\n",
    "train_paths = ['workshop-datasets/criteo_small/day_0'] # Sample training CSV file to be converted\n",
    "valid_paths = ['workshop-datasets/criteo_small/day_1'] # Sample validation CSV file to be converted\n",
    "output_path = f'{BUCKET}/converted' # Where to write the converted PARQUET files\n",
    "workflow_path = f'{BUCKET}/saved_workflow' # Location of the workflow json/pkl files\n",
    "output_transformed = f'{BUCKET}/transformed_data' # Location to write the transformed data\n",
    "\n",
    "sep = '\\t' # Separator for the CSV file\n",
    "gpus = '0' # Identifier of the GPU. As you will execute with only 1 GPU, only the first identier is passed.\n",
    "           # If you were to execute the pipeline with 4 GPUs, you should use '0,1,2,3'.\n",
    "\n",
    "recursive = False # If the train/valid paths should be navigated recursivelly\n",
    "shuffle = None # How to shuffle the dataset both in the conversion from CSV to PARQUET and during transformation."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "NVTabular makes it possible to shuffle during dataset creation. This creates a uniformly shuffled dataset that allows the dataloader to load large contiguous chunks of data, which are already randomized across the entire dataset. NVTabular also makes it possible to control the number of chunks that are combined into a batch, providing flexibility when trading off between performance and true randomization. This mechanism is critical when dealing with datasets that exceed CPU memory and individual epoch shuffling is desired during training. Full shuffle of such a dataset can exceed training time for the epoch by several orders of magnitude."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a dictionarry will all the parameters defined until now\n",
    "parameter_values = {\n",
    "    'train_paths': json.dumps(train_paths),\n",
    "    'valid_paths': json.dumps(valid_paths),\n",
    "    'output_path': output_path,\n",
    "    'columns': json.dumps(columns),\n",
    "    'cols_dtype': json.dumps(cols_dtype),\n",
    "    'output_transformed': output_transformed,\n",
    "    'workflow_path': workflow_path,\n",
    "    'sep': sep,\n",
    "    'gpus': gpus,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compile the pipeline.\n",
    "# This command will validate the pipeline and generate a JSON file with its specifications\n",
    "PACKAGE_PATH = 'pipeline_gcs.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_gcs,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize aiplatform SDK client\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET_NAME\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Submit the job to Vertex AI Pipelines\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name='nvt_convert_pipeline_gcs',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of the execution, the pipeline will look like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../images/gcs_vertex_pipeline.png\" alt=\"Pipeline\" style=\"height: 15%; width:15%;\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Pipeline: Source data in BQ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This pipeline will execute the following steps:\n",
    "\n",
    "*Pipeline 2*: Input Parquet files exported from BigQuery\n",
    " - Export Parquer files from a table in Bigquery\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    "\n",
    "Architecture overview:\n",
    "\n",
    " <img src=\"../../images/pipeline_2.png\" alt=\"Pipeline\" style=\"height: 60%; width:60%;\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this scenario, the dataset is located in a table in BigQuery.  \n",
    "The recommended format to run the NVTabular preprocessing step is in Parquet.  \n",
    "The first step of this pipeline exports the data from BigQuery to PARQUET files in GCS.\n",
    "\n",
    "The second step in the pipeline calculates statistics on the dataset based on the transformations defined in the `nvtabular.Workflow`.  \n",
    "Finally, the last step is executed to transform the dataset and write back to GCS."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import components and pipeline definition\n",
    "from pipeline_bq import preprocessing_pipeline_bq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To transform the data using NVTabular you must create a DAG with the transformation steps.  \n",
    "NVTabular implements several operators to processes your dataset. They can be found [HERE](https://nvidia.github.io/NVTabular/v0.4.0/resources/api/ops/index.html).\n",
    "\n",
    "In this example the DAG perform the following transformations:\n",
    " - Categorical features (columns that start with C): Apply [Categorify](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/categorify.html)\n",
    " - Continuous features (columns that start with I): Apply [FillMissing](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/fillmissing.html), [Clip](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/clip.html) and [Normalize](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/normalize.html) (in this order).\n",
    "\n",
    " The definition of the Workflow will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " It will be uploaded to GCS for future use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Columns and dtypes definition\n",
    "cont_names = [\"I\" + str(x) for x in range(1, 14)]\n",
    "cat_names = [\"C\" + str(x) for x in range(1, 27)]\n",
    "columns = [\"label\"] + cont_names + cat_names\n",
    "\n",
    "# Transformation pipeline\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(max_size=num_buckets)\n",
    "cat_features = cat_names >> categorify_op\n",
    "cont_features = cont_names >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "features = cat_features + cont_features + ['label']\n",
    "\n",
    "# Create and save workflow\n",
    "workflow = nvt.Workflow(features)\n",
    "workflow.save('./saved_workflow')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Upload to GCS\n",
    "! gsutil cp -r ./saved_workflow BUCKET_NAME"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's define some variables necessary to execute the pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BUCKET = BUCKET_NAME[5:] # retrive the bucket name only, without prefix\n",
    "\n",
    "output_path = f'{BUCKET}/bq_converted' # Where to write the converted PARQUET files\n",
    "bq_project = PROJECT_ID # Project ID where the dataset is stored\n",
    "bq_dataset_id = 'criteo_pipeline' # Dataset ID\n",
    "bq_table_train = 'train' # Table ID for the training data\n",
    "bq_table_valid = 'valid' # Table ID for the validation data\n",
    "location = 'US' # Location to process the data in BigQuery\n",
    "\n",
    "workflow_path = f'{BUCKET}/saved_workflow' # Location of the workflow json/pkl files\n",
    "output_transformed = f'{BUCKET}/bq_transformed_data' # Location to write the transformed data\n",
    "gpus = '0' # Identifier of the GPU. As you will execute with only 1 GPU, only the first identier is passed.\n",
    "           # If you were to execute the pipeline with 4 GPUs, you should use '0,1,2,3'.\n",
    "\n",
    "recursive = False # If the train/valid paths should be navigated recursivelly\n",
    "shuffle = None # How to shuffle the dataset both in the conversion from CSV to PARQUET and during transformation."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a dictionarry will all the parameters defined until now\n",
    "parameter_values = {\n",
    "    'bq_table_train': bq_table_train,\n",
    "    'bq_table_valid': bq_table_valid,\n",
    "    'output_path': output_path,\n",
    "    'bq_project': bq_project,\n",
    "    'bq_dataset_id': bq_dataset_id,\n",
    "    'location': location,\n",
    "    'gpus': gpus,\n",
    "    'workflow_path': workflow_path,\n",
    "    'output_transformed': output_transformed,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compile the pipeline.\n",
    "# This command will validate the pipeline and generate a JSON file with its specifications\n",
    "PACKAGE_PATH = 'pipeline_bq.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_bq,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize aiplatform SDK client\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET_NAME\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Submit the job to Vertex AI Pipelines\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name='nvt_convert_pipeline_bq',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of the execution, the pipeline will look like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../images/bq_vertex_pipeline.png\" alt=\"Pipeline\" style=\"height: 15%; width:15%;\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Pipeline: Source GCS and output to Feature Store"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This pipeline will execute the following steps:\n",
    "\n",
    "**Pipeline 3**: Input CSV files from GCS and output to Vertex AI Feature Store \n",
    " - Read CSV files from Google Cloud Storage\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    " - Load the transformed data to BigQuery\n",
    " - Create a Vertex AI Feature Store and load the data from BigQuery\n",
    "\n",
    " Architecture overview:\n",
    "\n",
    " <img src=\"../../images/pipeline_3.png\" alt=\"Pipeline\" style=\"height: 70%; width:70%;\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import components and pipeline definition\n",
    "from pipeline_gcs_feat import preprocessing_pipeline_gcs_feat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The original Criteo dataset is in CSV format, but the recommended format to run the NVTabular preprocessing step is in PARQUET.  \n",
    "The first step of this pipeline read the data from GCS, converts the data from CSV to PARQUET using the `nvtabular.Dataset.to_parquet` method, and write the converted data back to GCS.\n",
    "\n",
    "The second step in the pipeline calculates statistics on the dataset based on the transformations defined in the `nvtabular.Workflow`.  \n",
    "Finally, the last step is executed to transform the dataset and write back to GCS."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Columns and dtypes definition\n",
    "cont_names = [\"I\" + str(x) for x in range(1, 14)]\n",
    "cat_names = [\"C\" + str(x) for x in range(1, 27)]\n",
    "columns = [\"label\"] + cont_names + cat_names\n",
    "\n",
    "# Specify column dtypes. Note that \"hex\" means that\n",
    "# the values will be hexadecimal strings that should\n",
    "# be converted to int32\n",
    "cols_dtype = {}\n",
    "cols_dtype[\"label\"] = 'int32'\n",
    "for x in cont_names:\n",
    "    cols_dtype[x] = 'int32'\n",
    "for x in cat_names:\n",
    "    cols_dtype[x] = 'hex'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To transform the data using NVTabular you must create a DAG with the transformation steps.  \n",
    "NVTabular implements several operators to processes your dataset. They can be found [HERE](https://nvidia.github.io/NVTabular/v0.4.0/resources/api/ops/index.html).\n",
    "\n",
    "In this example the DAG perform the following transformations:\n",
    " - Categorical features (columns that start with C): Apply [Categorify](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/categorify.html)\n",
    " - Continuous features (columns that start with I): Apply [FillMissing](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/fillmissing.html), [Clip](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/clip.html) and [Normalize](https://nvidia.github.io/NVTabular/v0.7.0/resources/api/ops/normalize.html) (in this order).\n",
    "\n",
    " The definition of the Workflow will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " It will be uploaded to GCS for future use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transformation pipeline\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(max_size=num_buckets)\n",
    "cat_features = cat_names >> categorify_op\n",
    "cont_features = cont_names >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "features = cat_features + cont_features + ['label']\n",
    "\n",
    "# Create and save workflow\n",
    "workflow = nvt.Workflow(features)\n",
    "workflow.save('./saved_workflow')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Upload to GCS\n",
    "! gsutil cp -r ./saved_workflow BUCKET_NAME"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's define some variables necessary to execute the pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BUCKET = BUCKET_NAME[5:] # retrive the bucket name only, without prefix\n",
    "\n",
    "train_paths = ['workshop-datasets/criteo_small/day_0'] # Sample training CSV file to be converted\n",
    "valid_paths = ['workshop-datasets/criteo_small/day_1'] # Sample validation CSV file to be converted\n",
    "output_path = f'{BUCKET}/converted' # Where to write the converted PARQUET files\n",
    "workflow_path = f'{BUCKET}/saved_workflow' # Location of the workflow json/pkl files\n",
    "output_transformed = f'{BUCKET}/transformed_data' # Location to write the transformed data\n",
    "\n",
    "sep = '\\t' # Separator for the CSV file\n",
    "gpus = '0' # Identifier of the GPU. As you will execute with only 1 GPU, only the first identier is passed.\n",
    "           # If you were to execute the pipeline with 4 GPUs, you should use '0,1,2,3'.\n",
    "\n",
    "recursive = False # If the train/valid paths should be navigated recursivelly\n",
    "shuffle = None # How to shuffle the dataset both in the conversion from CSV to PARQUET and during transformation.\n",
    "\n",
    "bq_project = 'renatoleite-mldemos' # Project ID where BQ is located\n",
    "bq_dataset_id = 'criteo_pipeline' # Dataset ID\n",
    "bq_dest_table_id = 'transformed_train' # Table ID where the data will be loaded after transformation."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a dictionarry will all the parameters defined until now\n",
    "parameter_values = {\n",
    "    'train_paths': json.dumps(train_paths),\n",
    "    'valid_paths': json.dumps(valid_paths),\n",
    "    'output_path': output_path,\n",
    "    'columns': json.dumps(columns),\n",
    "    'cols_dtype': json.dumps(cols_dtype),\n",
    "    'output_transformed': output_transformed,\n",
    "    'workflow_path': workflow_path,\n",
    "    'sep': sep,\n",
    "    'gpus': gpus,\n",
    "    'bq_project': bq_project,\n",
    "    'bq_dataset_id':bq_dataset_id,\n",
    "    'bq_dest_table_id': bq_dest_table_id,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compile the pipeline.\n",
    "# This command will validate the pipeline and generate a JSON file with its specifications\n",
    "PACKAGE_PATH = 'pipeline_nvt_gcs_feat.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_gcs_feat,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize aiplatform SDK client\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET_NAME\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Submit the job to Vertex AI Pipelines\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name='pipeline_nvt_gcs_feat',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of the execution, the pipeline will look like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../images/gcs_feat_vertex_pipeline.png\" alt=\"Pipeline\" style=\"height: 15%; width:15%;\"/>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}