{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering for large scale recommenders with NVIDIA NVTabular and Vertex AI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "The focus of this guide is to compile prescriptive guidelines for developing and operationalizing data preprocessing and feature engineering workflows using Vertex AI and NVIDIA NVTabular.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).  \n",
    "\n",
    "### From the Criteo website:\n",
    "\n",
    "#### Overview\n",
    " - This dataset contains feature values and click feedback for millions of display ads. Its purpose is to benchmark algorithms for clickthrough rate (CTR) prediction.\n",
    " - This dataset contains 24 files, each one corresponding to one day of data.\n",
    " - Each row corresponds to a display ad served by Criteo and the first column is indicates whether this ad has been clicked or not. The positive (clicked) and negatives  (non-clicked) examples have both been subsampled (but at different rates) in order to reduce the dataset size.\n",
    " - There are 13 features taking integer values (mostly count features) and 26 categorical features. The values of the categorical features have been hashed\n",
    "onto 32 bits for anonymization purposes.\n",
    " - The semantic of these features is undisclosed. Some features may have missing values.\n",
    " - The rows are chronologically ordered.\n",
    "\n",
    "#### Data fields\n",
    " - Label - Target variable that indicates if an ad was clicked (1) or not (0).\n",
    " - I1-I13 - A total of 13 columns of integer features (mostly count features).\n",
    " - C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes. \n",
    " - The semantic of the features is undisclosed.  \n",
    " - When a value is missing, the field is empty.\n",
    "\n",
    "#### Format\n",
    "The columns are tab separated with the following schema:  \n",
    "<label> <integer feature 1> … <integer feature 13> <categorical feature 1> … <categorical feature 26>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objective\n",
    "This notebook demonstrates how to do data preprocessing with NVIDIA NVTabular on Vertex AI Pipeline steps.  \n",
    "You will create three different pipelines to understand possible alternatives on how to manipulate data.\n",
    "\n",
    "*Pipeline 1*: Input CSV files from Google Cloud Storage (GCS for short)  \n",
    " - Read CSV files from GCS\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    "\n",
    "*Pipeline 2*: Input Parquet files exported from BigQuery\n",
    " - Export Parquer files from a table in Bigquery\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    "\n",
    "*Pipeline 3*: Input CSV files from GCS and output to Vertex AI Feature Store \n",
    " - Read CSV files from Google Cloud Storage\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    " - Load the transformed data to BigQuery\n",
    " - Create a Vertex AI Feature Store and load the data from BigQuery\n",
    "\n",
    "The goal is to present how to use NVTabular to transform the data on GPU and different ways of inputing (GCS and Bigquery) and outputing (GCS, BigQuery, FeatureStore) data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Costs\n",
    "This tutorial uses billable components of Google Cloud (GCP):\n",
    " - Vertex AI (Pipelines, FeatureStore, GPUs, Training)\n",
    " - Cloud Storage\n",
    " - BigQuery\n",
    " - Cloud Artifact Registry\n",
    "\n",
    "Use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instalation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up your local development environment\n",
    "\n",
    "If you are using Google Cloud Notebooks, your environment already meets all the requirements to run this notebook. You can skip this step.\n",
    "\n",
    "Otherwise, make sure your environment meets this notebook's requirements. You need the following:\n",
    " - The Google Cloud SDK\n",
    " - Git\n",
    " - Python 3\n",
    " - virtualenv\n",
    " - Jupyter notebook running in a virtual environment with Python 3\n",
    " - docker\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development environment](https://cloud.google.com/python/setup) and the [Jupyter installation guide](https://jupyter.org/install) provide detailed instructions for meeting these requirements. The following steps provide a condensed set of instructions:\n",
    "\n",
    " - [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    " - [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    " - [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    " - To install Jupyter, run pip install jupyter on the command-line in a terminal shell.\n",
    " - To launch Jupyter, run jupyter notebook on the command-line in a terminal shell.\n",
    " - Open this notebook in the Jupyter Notebook Dashboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install additional packages\n",
    "\n",
    "To run build and run the pipelines, you need to install Kubeflow Pipelines SDK, Vertex AI SDK and NVTabular SDK locally."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup user flag specific for your environment\n",
    "\n",
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Install kfp, vertex and nvtabular SDKs\n",
    "! pip install {USER_FLAG} --upgrade kfp google-cloud-aiplatform nvtabular"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restart the kernel\n",
    "After installing the additional packages, you need to restart the notebook kernel so it can find the packages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the versions of the packages you installed. The KFP SDK version should be >=1.6."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFP SDK version: 1.8.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Before you begin\n",
    "This notebook does not require a GPU runtime, but the pipeline steps running on GCP will.\n",
    "\n",
    "## Set up your Google Cloud project\n",
    "### The following steps are required, regardless of your notebook environment.\n",
    " - [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    " - [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    " - [Enable the Vertex AI, Cloud Storage, BigQuery, Compute Engine, Cloud Build and Artifact Registry APIs.](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com,bigquery.googleapis.com,cloudbuild.googleapis.com,artifactregistry.googleapis.com)\n",
    " - Follow the \"Configuring your project\" instructions from the Vertex Pipelines documentation.\n",
    " - If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    " - Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with ! as shell commands, and it interpolates Python variables prefixed with $ into these commands."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set your project ID\n",
    "### If you don't know your project ID, you may be able to get your project ID using gcloud."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output= ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otherwise, set your project ID here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"<YOUR_GCP_PROJECT_HERE>\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Timestamp\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Authenticate your Google Cloud account\n",
    "### **If you are using Google Cloud Notebooks or a GCE VM, your environment is already authenticated with the default service account. Skip this step.**\n",
    "\n",
    "If you are using Colab, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
    "\n",
    "Otherwise, follow these steps:\n",
    " - In the Cloud Console, go to the [Create service account key page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    " - Click **Create service account**.\n",
    " - In the **Service account** name field, enter a name, and click **Create**.\n",
    " - In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**. Type \"Bigquery Object Admin\" into the filter box, and select **BigQuery Admin**.\n",
    " - Click Create. A JSON file that contains your key downloads to your local environment.\n",
    " - Enter the path to your service account key as the **GOOGLE_APPLICATION_CREDENTIALS** variable in the cell below and run the cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Cloud Storage bucket as necessary\n",
    "You will need a Cloud Storage bucket for this example. If you don't have one that you want to use, you can make one now.  \n",
    "The bucket will be used to store the following artifacts:\n",
    " - Temporary files\n",
    " - NVTabular workflow files\n",
    " - Transformed and preprocessed files\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
    "\n",
    "You may also change the REGION variable, which is used for operations throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may not use a Multi-Regional Storage bucket for training with Vertex AI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "BUCKET_NAME = \"gs://<YOUR-BUCKET-NAME>\" # <------- Change HERE\n",
    "REGION = \"us-central1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://<YOUR-BUCKET-NAME>\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Only if your bucket doesn't already exist**: Uncomment and run the following cell to create your Cloud Storage bucket."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_NAME"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the docker image\n",
    "In order to execute each step in the pipeline, you need to build a docker image with the required packages and push it to Artifact Registry.  \n",
    "Follow these steps to create an Artifact Registry for your docker images:\n",
    "\n",
    "1) [Choose a shell](https://cloud.google.com/artifact-registry/docs/docker/quickstart) to execute the gcloud commands\n",
    "2) Define the name of your registry in the next cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "REG_NAME = 'quickstart-docker-repo' # <----- Change HERE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Run the following command to create a new Docker repository in the location us-central1 with the description \"docker repository\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gcloud artifacts repositories create f'{REG_NAME}' --repository-format=docker \\\n",
    "--location=us-central1 --description=\"Docker repository\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Run the following command to verify that your repository was created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gcloud artifacts repositories list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5) **IMPORTANT**: [Configure Authentication](https://cloud.google.com/artifact-registry/docs/docker/quickstart#auth)\n",
    "6) Create a Dockerfile with the next cell and build the image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%%file Dockerfile.test\n",
    "FROM gcr.io/deeplearning-platform-release/base-cu110\n",
    "\n",
    "WORKDIR /nvtabular\n",
    "\n",
    "RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3 cudatoolkit=11.0\n",
    "RUN pip install google-cloud-aiplatform kfp\n",
    "\n",
    "ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "ENV PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION python"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing Dockerfile.test\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "7) Define a tag for your docker image, build and push it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "IMG_REGION = 'us-central1-docker.pkg.dev'\n",
    "IMG_NAME = 'nvt-preprocessing'\n",
    "IMG_TAG = f'{IMG_REGION}/{PROJECT_ID}/{REG_NAME}/{IMG_NAME}'\n",
    "! docker build -t IMG_TAG .\n",
    "! docker push IMG_TAG"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "8) Make sure your image was pushed to the registry."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! gcloud artifacts docker images list IMG_TAG"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quotas\n",
    "\n",
    "Make sure you have enough quota (GPU, CPU and Memory) for your environment before proceeding.  \n",
    "On GCP console, visit [IAM & Admin > Quotas](https://console.cloud.google.com/iam-admin/quotas)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries and define constants\n",
    "Define some constants."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "USER = '<YOUR-USERNAME-HERE>'  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do some imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Standard\n",
    "import json\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    Clip,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/utils/gpu_utils.py:156: UserWarning: No NVIDIA GPU detected\n",
      "  warnings.warn(\"No NVIDIA GPU detected\")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Pipeline: Input CSV files from GCS\n",
    "\n",
    "This pipeline will execute the following steps:\n",
    "\n",
    "*Pipeline 1*: \n",
    " - Read CSV files from GCS\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output to GCS\n",
    "\n",
    "<img src=\"../../images/pipeline_1.png\" alt=\"Pipeline\" style=\"height: 60%; width:60%;\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The original Criteo dataset is in CSV format, but the recommended format to run the NVTabular preprocessing step is in Parquet.  \n",
    "The first step of this pipeline converts the data from CSV to PARQUET using the `nvtabular.Dataset.to_parquet` method.\n",
    "\n",
    "The second step in the pipeline calculates statistics on the dataset based on the transformations defined in the `nvtabular.Workflow`.  \n",
    "Finally, the last step is executed to transform the dataset and write back to GCS."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import components and pipeline definition\n",
    "from pipeline_gcs import preprocessing_pipeline_gcs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Columns and dtypes definition\n",
    "cont_names = [\"I\" + str(x) for x in range(1, 14)]\n",
    "cat_names = [\"C\" + str(x) for x in range(1, 27)]\n",
    "columns = [\"label\"] + cont_names + cat_names\n",
    "\n",
    "# Specify column dtypes. Note that \"hex\" means that\n",
    "# the values will be hexadecimal strings that should\n",
    "# be converted to int32\n",
    "cols_dtype = {}\n",
    "cols_dtype[\"label\"] = 'int32'\n",
    "for x in cont_names:\n",
    "    cols_dtype[x] = 'int32'\n",
    "for x in cat_names:\n",
    "    cols_dtype[x] = 'hex'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Transformation pipeline\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(max_size=num_buckets)\n",
    "cat_features = cat_names >> categorify_op\n",
    "cont_features = cont_names >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "features = cat_features + cont_features + ['label']\n",
    "\n",
    "# Create and save workflow\n",
    "workflow = nvt.Workflow(features)\n",
    "workflow.save('./saved_workflow')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_paths = ['renatoleite-criteo-partial/flat_data/day_0']\n",
    "valid_paths = ['renatoleite-criteo-partial/flat_data/day_1']\n",
    "output_path = 'renatoleite-criteo-partial/converted'\n",
    "workflow_path = 'renatoleite-criteo-partial/saved_workflow'\n",
    "output_transformed = 'renatoleite-criteo-partial/transformed_data'\n",
    "\n",
    "sep = '\\t'\n",
    "gpus = '0'\n",
    "\n",
    "recursive = False\n",
    "shuffle = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "parameter_values = {\n",
    "    'train_paths': json.dumps(train_paths),\n",
    "    'valid_paths': json.dumps(valid_paths),\n",
    "    'output_path': output_path,\n",
    "    'columns': json.dumps(columns),\n",
    "    'cols_dtype': json.dumps(cols_dtype),\n",
    "    'output_transformed': output_transformed,\n",
    "    'workflow_path': workflow_path,\n",
    "    'sep': sep,\n",
    "    'gpus': gpus,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Compile Pipeline\n",
    "PACKAGE_PATH = 'pipeline_gcs.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_gcs,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name='nvt_convert_pipeline_gcs',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Pipeline: Source data in BQ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Import components and pipeline definition\n",
    "from pipeline_bq import preprocessing_pipeline_bq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "output_path = 'renatoleite-criteo-partial/bq_converted'\n",
    "bq_project = 'renatoleite-mldemos'\n",
    "bq_dataset_id = 'criteo_pipeline'\n",
    "bq_table_train = 'train'\n",
    "bq_table_valid = 'valid'\n",
    "location = 'US'\n",
    "\n",
    "workflow_path = 'renatoleite-criteo-partial/saved_workflow'\n",
    "output_transformed = 'renatoleite-criteo-partial/bq_transformed_data'\n",
    "gpus = '0'\n",
    "\n",
    "recursive = False\n",
    "shuffle = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "parameter_values = {\n",
    "    'bq_table_train': bq_table_train,\n",
    "    'bq_table_valid': bq_table_valid,\n",
    "    'output_path': output_path,\n",
    "    'bq_project': bq_project,\n",
    "    'bq_dataset_id': bq_dataset_id,\n",
    "    'location': location,\n",
    "    'gpus': gpus,\n",
    "    'workflow_path': workflow_path,\n",
    "    'output_transformed': output_transformed,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Compile Pipeline\n",
    "PACKAGE_PATH = 'pipeline_bq.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_bq,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "project_id = 'renatoleite-mldemos'\n",
    "region = 'us-central1'\n",
    "staging_bucket = 'gs://renatoleite-staging'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=region,\n",
    "    staging_bucket=staging_bucket\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name='nvt_convert_pipeline_bq',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Pipeline: Source GCS and output to Feature Store"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 1) Create op to export from gcs parquet back to BQ\n",
    "# 2) Export dataset to Feature Store\n",
    "\n",
    "# Import components and pipeline definition\n",
    "from pipeline_gcs_feat import preprocessing_pipeline_gcs_feat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Columns and dtypes definition\n",
    "cont_names = [\"I\" + str(x) for x in range(1, 14)]\n",
    "cat_names = [\"C\" + str(x) for x in range(1, 27)]\n",
    "columns = [\"label\"] + cont_names + cat_names\n",
    "\n",
    "# Specify column dtypes. Note that \"hex\" means that\n",
    "# the values will be hexadecimal strings that should\n",
    "# be converted to int32\n",
    "cols_dtype = {}\n",
    "cols_dtype[\"label\"] = 'int32'\n",
    "for x in cont_names:\n",
    "    cols_dtype[x] = 'int32'\n",
    "for x in cat_names:\n",
    "    cols_dtype[x] = 'hex'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_paths = ['renatoleite-criteo-partial/flat_data/day_0']\n",
    "valid_paths = ['renatoleite-criteo-partial/flat_data/day_1']\n",
    "output_path = 'renatoleite-criteo-partial/converted'\n",
    "workflow_path = 'renatoleite-criteo-partial/saved_workflow'\n",
    "output_transformed = 'renatoleite-criteo-partial/transformed_data'\n",
    "\n",
    "sep = '\\t'\n",
    "gpus = '0'\n",
    "\n",
    "recursive = False\n",
    "shuffle = None\n",
    "\n",
    "bq_project = 'renatoleite-mldemos'\n",
    "bq_dataset_id = 'criteo_pipeline'\n",
    "bq_dest_table_id = 'transformed_train'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "parameter_values = {\n",
    "    'train_paths': json.dumps(train_paths),\n",
    "    'valid_paths': json.dumps(valid_paths),\n",
    "    'output_path': output_path,\n",
    "    'columns': json.dumps(columns),\n",
    "    'cols_dtype': json.dumps(cols_dtype),\n",
    "    'output_transformed': output_transformed,\n",
    "    'workflow_path': workflow_path,\n",
    "    'sep': sep,\n",
    "    'gpus': gpus,\n",
    "    'bq_project': bq_project,\n",
    "    'bq_dataset_id':bq_dataset_id,\n",
    "    'bq_dest_table_id': bq_dest_table_id,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Compile Pipeline\n",
    "PACKAGE_PATH = 'pipeline_nvt_gcs_feat.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_gcs_feat,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "project_id = 'renatoleite-mldemos'\n",
    "region = 'us-central1'\n",
    "staging_bucket = 'gs://renatoleite-staging'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=region,\n",
    "    staging_bucket=staging_bucket\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name='pipeline_nvt_gcs_feat',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}