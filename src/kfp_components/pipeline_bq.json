{
  "pipelineSpec": {
    "components": {
      "comp-export-parquet-from-bq-op": {
        "executorLabel": "exec-export-parquet-from-bq-op",
        "inputDefinitions": {
          "parameters": {
            "bq_dataset_id": {
              "type": "STRING"
            },
            "bq_project": {
              "type": "STRING"
            },
            "bq_table_train": {
              "type": "STRING"
            },
            "bq_table_valid": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "output_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_datasets": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-fit-dataset-op": {
        "executorLabel": "exec-fit-dataset-op",
        "inputDefinitions": {
          "artifacts": {
            "datasets": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "device_limit_frac": {
              "type": "DOUBLE"
            },
            "device_pool_frac": {
              "type": "DOUBLE"
            },
            "gpus": {
              "type": "STRING"
            },
            "part_mem_frac": {
              "type": "DOUBLE"
            },
            "protocol": {
              "type": "STRING"
            },
            "split_name": {
              "type": "STRING"
            },
            "workflow_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "fitted_workflow": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-transform-dataset-op": {
        "executorLabel": "exec-transform-dataset-op",
        "inputDefinitions": {
          "artifacts": {
            "fitted_workflow": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "device_limit_frac": {
              "type": "DOUBLE"
            },
            "device_pool_frac": {
              "type": "DOUBLE"
            },
            "gpus": {
              "type": "STRING"
            },
            "output_transformed": {
              "type": "STRING"
            },
            "part_mem_frac": {
              "type": "DOUBLE"
            },
            "protocol": {
              "type": "STRING"
            },
            "split_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "transformed_dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-export-parquet-from-bq-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "export_parquet_from_bq_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef export_parquet_from_bq_op(\n    output_datasets: Output[Dataset],\n    output_path: str,\n    bq_project: str,\n    bq_dataset_id: str,\n    bq_table_train: str,\n    bq_table_valid: str,\n    location: str\n):\n    '''\n    output_path: str\n        Path to write the exported parquet files\n        Format:\n            'gs://<bucket_name>/<subfolder1>/<subfolder>/'\n    bq_project: str\n        GCP project id\n    bq_dataset_id: str\n        Bigquery dataset id\n    bq_table_train: str\n        Bigquery table name for training dataset\n    bq_table_valid: str\n        BigQuery table name for validation dataset\n    output_datasets: dict\n        Output metadata for the next step. Stores the path in GCS\n        for the datasets.\n        Usage:\n            train_path = output_datasets.metadata['train']\n            # returns: bucket_name/subfolder/subfolder/\n    '''\n\n    import logging\n    import os\n    from google.cloud import bigquery\n\n    logging.basicConfig(level=logging.INFO)\n\n    TRAIN_SPLIT_FOLDER = 'train'\n    VALID_SPLIT_FOLDER = 'valid'\n\n    client = bigquery.Client()\n    dataset_ref = bigquery.DatasetReference(bq_project, bq_dataset_id)\n\n    for folder_name, table_id in zip(\n        [TRAIN_SPLIT_FOLDER, VALID_SPLIT_FOLDER], \n        [bq_table_train, bq_table_valid]\n    ):\n        bq_glob_path = os.path.join(\n            'gs://',\n            output_path,\n            folder_name,\n            f'{folder_name}-*.parquet'\n        )\n        table_ref = dataset_ref.table(table_id)\n\n        logging.info(f'Extracting {table_ref} to {bq_glob_path}')\n        client.extract_table(table_ref, bq_glob_path, location=location)\n\n        full_output_path = os.path.join('/gcs', output_path, folder_name)\n        logging.info(\n            f'Saving metadata for {folder_name} path: {full_output_path}'\n        )\n        output_datasets.metadata[folder_name] = full_output_path\n\n    logging.info('Finished exporting to GCS.')\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-fit-dataset-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "fit_dataset_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef fit_dataset_op(\n    datasets: Input[Dataset],\n    fitted_workflow: Output[Artifact],\n    workflow_path: str,\n    gpus: str,\n    split_name: Optional[str] = 'train',\n    protocol: Optional[str] = 'tcp',\n    device_limit_frac: Optional[float] = 0.8,\n    device_pool_frac: Optional[float] = 0.9,\n    part_mem_frac: Optional[float] = 0.125\n):\n    '''\n    datasets: dict\n        Input metadata from previus step. Stores the full path of the \n        converted datasets.\n        How to access:\n            full_path = datasets.metadata.get('train')\n    fitted_workflow: dict\n        Output metadata for next step. Stores the full path of the \n        converted dataset, and saved workflow with statistics.\n    workflow_path: str\n        Path to the current workflow, not fitted.\n        Format:\n            '<bucket_name>/<subfolder1>/<subfolder>/'\n    split_name: str\n        Which dataset to calculate the statistics. 'train' or 'valid'\n    '''\n\n    import logging\n    import nvtabular as nvt\n    import os\n\n    from dask_cuda import LocalCUDACluster\n    from dask.distributed import Client\n    from nvtabular.utils import device_mem_size\n\n    logging.basicConfig(level=logging.INFO)\n\n    FIT_FOLDER = os.path.join('/gcs', workflow_path, 'fitted_workflow')\n\n    # Check if the `split_name` dataset is present\n    logging.info(f'Checking if split {split_name} is present.')\n    data_path = datasets.metadata.get(split_name, '')\n    if not data_path:\n        raise RuntimeError(f'Dataset does not have {split_name} split.')\n\n    # Dask Cluster defintions\n    device_size = device_mem_size()\n    device_limit = int(device_limit_frac * device_size)\n    device_pool_size = int(device_pool_frac * device_size)\n    part_size = int(part_mem_frac * device_size)\n    rmm_pool_size = (device_pool_size // 256) * 256\n\n    if gpus:\n        logging.info('Creating a Dask CUDA cluster')\n        cluster = LocalCUDACluster(\n            protocol=protocol,\n            n_workers=len(gpus.split(sep=',')),\n            CUDA_VISIBLE_DEVICES=gpus,\n            device_memory_limit=device_limit,\n            rmm_pool_size=rmm_pool_size\n        )\n        client = Client(cluster)\n    else:\n        raise Exception('Cannot create Cluster. \\\n                            Provide a list of available GPUs')\n\n    # Load Transformation steps\n    full_workflow_path = os.path.join('/gcs', workflow_path)\n\n    logging.info('Loading saved workflow')\n    workflow = nvt.Workflow.load(full_workflow_path, client)\n    fitted_dataset = nvt.Dataset(\n        data_path, engine=\"parquet\", part_size=part_size\n    )\n    logging.info('Starting workflow fitting')\n    workflow.fit(fitted_dataset)\n    logging.info('Finished generating statistics for dataset.')\n\n    logging.info(f'Saving workflow to {FIT_FOLDER}')\n    workflow.save(FIT_FOLDER)\n\n    fitted_workflow.metadata['fitted_workflow'] = FIT_FOLDER\n    fitted_workflow.metadata['datasets'] = datasets.metadata\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "accelerator": {
                "count": "1",
                "type": "NVIDIA_TESLA_T4"
              },
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-transform-dataset-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "transform_dataset_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef transform_dataset_op(\n    fitted_workflow: Input[Artifact],\n    transformed_dataset: Output[Dataset],\n    output_transformed: str,\n    gpus: str,\n    split_name: str = 'train',\n    shuffle: str = None,\n    protocol: str = 'tcp',\n    device_limit_frac: float = 0.8,\n    device_pool_frac: float = 0.9,\n    part_mem_frac: float = 0.125,\n):\n    '''\n    fitted_workflow: dict\n        Input metadata from previous step. Stores the path of the fitted_workflow\n        and the location of the datasets (train and validation).\n        Usage:\n            train_path = fitted_workflow.metadata['datasets']['train]\n            output: '<bucket_name>/<subfolder1>/<subfolder>/'\n    transformed_dataset: dict\n        Output metadata for next step. Stores the path of the transformed dataset \n        and the validation dataset.\n    output_transformed: str,\n        Path to write the transformed parquet files\n        Format:\n            '<bucket_name>/<subfolder1>/<subfolder>/'\n    gpus: str\n        GPUs available. Example:\n            If there are 4 gpus available, must be '0,1,2,3'\n    shuffle: str\n        How to shuffle the converted data, default to None.\n        Options:\n            PER_PARTITION\n            PER_WORKER\n            FULL\n    '''\n    import logging\n    import nvtabular as nvt\n    import os\n\n    from dask_cuda import LocalCUDACluster\n    from dask.distributed import Client\n    from nvtabular.utils import device_mem_size\n    from nvtabular.io.shuffle import Shuffle\n\n    logging.basicConfig(level=logging.INFO)\n\n    # Define output path for transformed files\n    TRANSFORM_FOLDER = os.path.join('/gcs', output_transformed, split_name)\n\n    # Get path to dataset to be transformed\n    data_path = fitted_workflow.metadata.get('datasets').get(split_name, '')\n    if not data_path:\n        raise RuntimeError(f'Dataset does not have {split_name} split.')\n\n    # Dask Cluster defintions\n    device_size = device_mem_size()\n    device_limit = int(device_limit_frac * device_size)\n    device_pool_size = int(device_pool_frac * device_size)\n    part_size = int(part_mem_frac * device_size)\n    rmm_pool_size = (device_pool_size // 256) * 256\n\n    if gpus:\n        logging.info('Creating a Dask CUDA cluster')\n        cluster = LocalCUDACluster(\n            protocol=protocol,\n            n_workers=len(gpus.split(sep=',')),\n            CUDA_VISIBLE_DEVICES=gpus,\n            device_memory_limit=device_limit,\n            rmm_pool_size=rmm_pool_size\n        )\n        client = Client(cluster)\n    else:\n        raise Exception('Cannot create Cluster. \\\n                            Provide a list of available GPUs')\n\n    # Load Transformation steps\n    logging.info('Loading workflow and statistics')\n    workflow = nvt.Workflow.load(\n        fitted_workflow.metadata.get('fitted_workflow'), client\n    )\n\n    logging.info('Creating dataset definition')\n    dataset = nvt.Dataset(\n        data_path, engine=\"parquet\", part_size=part_size\n    )\n\n    if shuffle:\n        shuffle = getattr(Shuffle, shuffle)\n\n    logging.info('Starting workflow transformation')\n    workflow.transform(dataset).to_parquet(\n        output_files=len(gpus.split(sep='/')),\n        output_path=TRANSFORM_FOLDER,\n        shuffle=shuffle\n    )\n    logging.info('Finished transformation')\n\n    transformed_dataset.metadata['transformed_dataset'] = TRANSFORM_FOLDER\n    transformed_dataset.metadata['original_datasets'] = \\\n        fitted_workflow.metadata.get('datasets')\n\n    # Maybe this can speedup steps transition\n    # TODO: Check if this makes any difference.\n    client.close()\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "accelerator": {
                "count": "1",
                "type": "NVIDIA_TESLA_T4"
              },
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "nvt-pipeline-bq"
    },
    "root": {
      "dag": {
        "tasks": {
          "export-parquet-from-bq-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-export-parquet-from-bq-op"
            },
            "inputs": {
              "parameters": {
                "bq_dataset_id": {
                  "componentInputParameter": "bq_dataset_id"
                },
                "bq_project": {
                  "componentInputParameter": "bq_project"
                },
                "bq_table_train": {
                  "componentInputParameter": "bq_table_train"
                },
                "bq_table_valid": {
                  "componentInputParameter": "bq_table_valid"
                },
                "location": {
                  "componentInputParameter": "location"
                },
                "output_path": {
                  "componentInputParameter": "output_path"
                }
              }
            },
            "taskInfo": {
              "name": "export-parquet-from-bq-op"
            }
          },
          "fit-dataset-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-fit-dataset-op"
            },
            "dependentTasks": [
              "export-parquet-from-bq-op"
            ],
            "inputs": {
              "artifacts": {
                "datasets": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_datasets",
                    "producerTask": "export-parquet-from-bq-op"
                  }
                }
              },
              "parameters": {
                "device_limit_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.8
                    }
                  }
                },
                "device_pool_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.9
                    }
                  }
                },
                "gpus": {
                  "componentInputParameter": "gpus"
                },
                "part_mem_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.125
                    }
                  }
                },
                "protocol": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "tcp"
                    }
                  }
                },
                "split_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "train"
                    }
                  }
                },
                "workflow_path": {
                  "componentInputParameter": "workflow_path"
                }
              }
            },
            "taskInfo": {
              "name": "fit-dataset-op"
            }
          },
          "transform-dataset-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-transform-dataset-op"
            },
            "dependentTasks": [
              "fit-dataset-op"
            ],
            "inputs": {
              "artifacts": {
                "fitted_workflow": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "fitted_workflow",
                    "producerTask": "fit-dataset-op"
                  }
                }
              },
              "parameters": {
                "device_limit_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.8
                    }
                  }
                },
                "device_pool_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.9
                    }
                  }
                },
                "gpus": {
                  "componentInputParameter": "gpus"
                },
                "output_transformed": {
                  "componentInputParameter": "output_transformed"
                },
                "part_mem_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.125
                    }
                  }
                },
                "protocol": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "tcp"
                    }
                  }
                },
                "split_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "train"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "transform-dataset-op"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "bq_dataset_id": {
            "type": "STRING"
          },
          "bq_project": {
            "type": "STRING"
          },
          "bq_table_train": {
            "type": "STRING"
          },
          "bq_table_valid": {
            "type": "STRING"
          },
          "gpus": {
            "type": "STRING"
          },
          "location": {
            "type": "STRING"
          },
          "output_path": {
            "type": "STRING"
          },
          "output_transformed": {
            "type": "STRING"
          },
          "recursive": {
            "type": "STRING"
          },
          "shuffle": {
            "type": "STRING"
          },
          "workflow_path": {
            "type": "STRING"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.1"
  },
  "runtimeConfig": {}
}