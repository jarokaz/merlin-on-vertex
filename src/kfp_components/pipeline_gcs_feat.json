{
  "pipelineSpec": {
    "components": {
      "comp-convert-csv-to-parquet-op": {
        "executorLabel": "exec-convert-csv-to-parquet-op",
        "inputDefinitions": {
          "parameters": {
            "cols_dtype": {
              "type": "STRING"
            },
            "columns": {
              "type": "STRING"
            },
            "gpus": {
              "type": "STRING"
            },
            "output_path": {
              "type": "STRING"
            },
            "recursive": {
              "type": "STRING"
            },
            "sep": {
              "type": "STRING"
            },
            "train_paths": {
              "type": "STRING"
            },
            "valid_paths": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_datasets": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-fit-dataset-op": {
        "executorLabel": "exec-fit-dataset-op",
        "inputDefinitions": {
          "artifacts": {
            "datasets": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "device_limit_frac": {
              "type": "DOUBLE"
            },
            "device_pool_frac": {
              "type": "DOUBLE"
            },
            "gpus": {
              "type": "STRING"
            },
            "part_mem_frac": {
              "type": "DOUBLE"
            },
            "protocol": {
              "type": "STRING"
            },
            "split_name": {
              "type": "STRING"
            },
            "workflow_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "fitted_workflow": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-import-parquet-to-bq-op": {
        "executorLabel": "exec-import-parquet-to-bq-op",
        "inputDefinitions": {
          "artifacts": {
            "transformed_dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "bq_dataset_id": {
              "type": "STRING"
            },
            "bq_dest_table_id": {
              "type": "STRING"
            },
            "bq_project": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_bq_table": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-load-bq-to-feature-store-op": {
        "executorLabel": "exec-load-bq-to-feature-store-op",
        "inputDefinitions": {
          "artifacts": {
            "output_bq_table": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "cols_dtype": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "feature_store_path": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-transform-dataset-op": {
        "executorLabel": "exec-transform-dataset-op",
        "inputDefinitions": {
          "artifacts": {
            "fitted_workflow": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "device_limit_frac": {
              "type": "DOUBLE"
            },
            "device_pool_frac": {
              "type": "DOUBLE"
            },
            "gpus": {
              "type": "STRING"
            },
            "output_transformed": {
              "type": "STRING"
            },
            "part_mem_frac": {
              "type": "DOUBLE"
            },
            "protocol": {
              "type": "STRING"
            },
            "split_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "transformed_dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-convert-csv-to-parquet-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "convert_csv_to_parquet_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef convert_csv_to_parquet_op(\n    output_datasets: Output[Dataset],\n    train_paths: list,\n    valid_paths: list,\n    output_path: str,\n    columns: list,\n    cols_dtype: dict,\n    sep: str,\n    gpus: str,\n    shuffle: Optional[str] = None,\n    recursive: Optional[bool] = False\n):\n    '''\n    Component to convert CSV file(s) to Parquet format using NVTabular.\n\n    output_datasets: Output[Dataset]\n        Output metadata with references to the converted CSVs in GCS.\n        Usage:\n            output_datasets.metadata['train']\n                .example: '/gcs/my_bucket/folders/train'\n            output_datasets.metadata['valid']\n                .example: '/gcs/my_bucket/folders/valid'\n    train_paths: list\n        List of paths to folders or files in GCS for training.\n        For recursive folder search, set the recursive variable to True\n        Format:\n            '<bucket_name>/<subfolder1>/<subfolder>/' or\n            '<bucket_name>/<subfolder1>/<subfolder>/flat_file.csv' or\n            a combination of both.\n    valid_paths: list\n        List of paths to folders or files in GCS for validation.\n        For recursive folder search, set the recursive variable to True\n        Format:\n            '<bucket_name>/<subfolder1>/<subfolder>/' or\n            '<bucket_name>/<subfolder1>/<subfolder>/flat_file.csv' or\n            a combination of both.\n    output_path: str\n        Path in GCS to write the converted parquet files.\n        Format:\n            '<bucket_name>/<subfolder1>/<subfolder>'\n    columns: list\n        List with the columns name from CSV file.\n        Format:\n            ['I1', 'I2', ..., 'C1', ...]\n    cols_dtype: dict\n        Dict with the dtype of the columns from CSV.\n        Format:\n            {'I1':'int32', ..., 'C20':'hex'}\n    gpus: str\n        GPUs available. \n        Format:\n            If there are 4 gpus available, must be '0,1,2,3'\n    shuffle: str\n        How to shuffle the converted CSV, default to None.\n        Options:\n            PER_PARTITION\n            PER_WORKER\n            FULL\n    '''\n\n    # Standard Libraries\n    import logging\n    from pathlib import Path\n    import fsspec\n    import os\n\n    # External Dependencies\n    from dask_cuda import LocalCUDACluster\n    from dask.distributed import Client\n    import numpy as np\n\n    # NVTabular\n    from nvtabular.utils import device_mem_size, get_rmm_size\n    import nvtabular as nvt\n    from nvtabular.io.shuffle import Shuffle\n\n    logging.basicConfig(level=logging.INFO)\n\n    # Specify column dtypes (from numpy). Note that 'hex' means that\n    # the values will be hexadecimal strings that should be converted to int32\n    logging.info('Converting columns dtypes to numpy objects')\n    converted_col_dtype = {}\n    for col, dt in cols_dtype.items():\n        if dt == 'hex':\n            converted_col_dtype[col] = 'hex'\n        else:\n            converted_col_dtype[col] = getattr(np, dt)\n\n    fs_spec = fsspec.filesystem('gs')\n    rec_symbol = '**' if recursive else '*'\n\n    TRAIN_SPLIT_FOLDER = 'train'\n    VALID_SPLIT_FOLDER = 'valid'\n\n    if gpus:\n        logging.info('Creating a Dask CUDA cluster')\n        cluster = LocalCUDACluster(\n                    n_workers=len(gpus.split(sep=',')),\n                    CUDA_VISIBLE_DEVICES=gpus,\n                    rmm_pool_size=get_rmm_size(0.8 * device_mem_size())\n        )\n        client = Client(cluster)\n    else:\n        raise Exception(\n            'Cannot create Cluster. Provide a list of available GPUs'\n        )\n\n    for folder_name, data_paths in zip(\n        [TRAIN_SPLIT_FOLDER, VALID_SPLIT_FOLDER], \n        [train_paths, valid_paths]\n    ):\n        valid_paths = []\n        for path in data_paths:\n            try:\n                if fs_spec.isfile(path):\n                    valid_paths.append(os.path.join('/gcs', path))\n                else:\n                    path = os.path.join(path, rec_symbol)\n                    for i in fs_spec.glob(path):\n                        if fs_spec.isfile(i):\n                            valid_paths.append(os.path.join('/gcs', i))\n            except FileNotFoundError as fnf_expt:\n                print(fnf_expt)\n                print('One of the paths provided are incorrect.')\n            except OSError as os_err:\n                print(os_err)\n                print(f'Verify access to the bucket.')\n\n        dataset = nvt.Dataset(\n            path_or_source = valid_paths,\n            engine='csv',\n            names=columns,\n            sep=sep,\n            dtypes=converted_col_dtype,\n            client=client\n        )\n\n        full_output_path = os.path.join('/gcs', output_path, folder_name)\n\n        logging.info(f'Writing parquet file(s) to {full_output_path}')\n        if shuffle:\n            shuffle = getattr(Shuffle, shuffle)\n\n        dataset.to_parquet(\n            full_output_path,\n            preserve_files=True,\n            shuffle=shuffle\n        )\n\n        # Write output path to metadata\n        output_datasets.metadata[folder_name] = full_output_path\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "accelerator": {
                "count": "1",
                "type": "NVIDIA_TESLA_T4"
              },
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-fit-dataset-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "fit_dataset_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef fit_dataset_op(\n    datasets: Input[Dataset],\n    fitted_workflow: Output[Artifact],\n    workflow_path: str,\n    gpus: str,\n    split_name: Optional[str] = 'train',\n    protocol: Optional[str] = 'tcp',\n    device_limit_frac: Optional[float] = 0.8,\n    device_pool_frac: Optional[float] = 0.9,\n    part_mem_frac: Optional[float] = 0.125\n):\n    '''\n    Component to generate statistics from the dataset.\n\n    datasets: Input[Dataset]\n        Input metadata with references to the train and valid converted\n        datasets in GCS.\n        Usage:\n            full_path_train = datasets.metadata.get('train')\n                .example: '/gcs/my_bucket/folders/converted/train'\n            full_path_valid = datasets.metadata.get('train')\n                .example: '/gcs/my_bucket/folders/converted/valid'\n    fitted_workflow: Output[Artifact]\n        Output metadata with the path to the fitted workflow artifacts\n        (statistics) and converted datasets in GCS.\n        Usage:\n            fitted_workflow.metadata['fitted_workflow']\n                .example: '/gcs/my_bucket/fitted_workflow'\n            fitted_workflow.metadata['datasets']\n                .example: '/gcs/my_bucket/folders/converted/train'\n    workflow_path: str\n        Path to the current workflow, not fitted. This folder must have \n        2 files: metadata.json and workflow.pkl.\n        Format:\n            '<bucket_name>/<subfolder1>/<subfolder>'\n    split_name: str\n        Which dataset split to calculate the statistics. 'train' or 'valid'\n    '''\n\n    import logging\n    import nvtabular as nvt\n    import os\n\n    from dask_cuda import LocalCUDACluster\n    from dask.distributed import Client\n    from nvtabular.utils import device_mem_size\n\n    logging.basicConfig(level=logging.INFO)\n\n    FIT_FOLDER = os.path.join('/gcs', workflow_path, 'fitted_workflow')\n\n    # Check if the `split_name` dataset is present\n    logging.info(f'Checking if split {split_name} is present.')\n    data_path = datasets.metadata.get(split_name, '')\n    if not data_path:\n        raise RuntimeError(f'Dataset does not have {split_name} split.')\n\n    # Dask Cluster defintions\n    device_size = device_mem_size()\n    device_limit = int(device_limit_frac * device_size)\n    device_pool_size = int(device_pool_frac * device_size)\n    part_size = int(part_mem_frac * device_size)\n    rmm_pool_size = (device_pool_size // 256) * 256\n\n    if gpus:\n        logging.info('Creating a Dask CUDA cluster')\n        cluster = LocalCUDACluster(\n            protocol=protocol,\n            n_workers=len(gpus.split(sep=',')),\n            CUDA_VISIBLE_DEVICES=gpus,\n            device_memory_limit=device_limit,\n            rmm_pool_size=rmm_pool_size\n        )\n        client = Client(cluster)\n    else:\n        raise Exception(\n            'Cannot create Cluster. Provide a list of available GPUs'\n        )\n\n    # Load Transformation steps\n    full_workflow_path = os.path.join('/gcs', workflow_path)\n\n    logging.info('Loading saved workflow')\n    workflow = nvt.Workflow.load(full_workflow_path, client)\n    fitted_dataset = nvt.Dataset(\n        data_path, engine=\"parquet\", part_size=part_size\n    )\n    logging.info('Starting workflow fitting')\n    workflow.fit(fitted_dataset)\n    logging.info('Finished generating statistics for dataset.')\n\n    logging.info(f'Saving workflow to {FIT_FOLDER}')\n    workflow.save(FIT_FOLDER)\n\n    fitted_workflow.metadata['fitted_workflow'] = FIT_FOLDER\n    fitted_workflow.metadata['datasets'] = datasets.metadata\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "accelerator": {
                "count": "1",
                "type": "NVIDIA_TESLA_T4"
              },
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-import-parquet-to-bq-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "import_parquet_to_bq_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef import_parquet_to_bq_op(\n    transformed_dataset: Input[Dataset],\n    output_bq_table: Output[Dataset],\n    bq_project: str,\n    bq_dataset_id: str,\n    bq_dest_table_id: str\n):\n    '''\n    Component to load PARQUET files to a Bigquery table.\n\n    transformed_dataset: dict\n        Input metadata. Stores the path in GCS\n        for the datasets.\n        Usage:\n            train_path = output_dataset['train']\n            # returns: bucket_name/subfolder/subfolder/\n    bq_project: str\n        GCP project id\n        Format:\n            'my_project'\n    bq_dataset_id: str\n        Bigquery dataset id\n        Format:\n            'my_dataset_id'\n    bq_dest_table_id: str\n        Bigquery destination table name\n        Format:\n            'my_destination_table_id'\n    '''\n\n    # Standard Libraries\n    import logging\n    import os\n    from google.cloud import bigquery\n\n    logging.basicConfig(level=logging.INFO)\n\n    data_path = transformed_dataset.metadata['transformed_dataset'][5:]\n    full_data_path = os.path.join('gs://', data_path, '*.parquet')\n\n    # Construct a BigQuery client object.\n    client = bigquery.Client(project=bq_project)\n    table_id = '.'.join([bq_project, bq_dataset_id, bq_dest_table_id])\n\n    job_config = bigquery.LoadJobConfig(source_format=bigquery.SourceFormat.PARQUET)\n\n    load_job = client.load_table_from_uri(\n        full_data_path, table_id, job_config=job_config\n    )  # Make an API request.\n\n    logging.info('Loading data from GCS to BQ')\n    load_job.result()  # Waits for the job to complete.\n\n    output_bq_table.metadata['bq_project'] = bq_project\n    output_bq_table.metadata['bq_dataset_id'] = bq_dataset_id\n    output_bq_table.metadata['bq_dest_table_id'] = bq_dest_table_id\n    output_bq_table.metadata['dataset_path'] = data_path\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-load-bq-to-feature-store-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "load_bq_to_feature_store_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef load_bq_to_feature_store_op(\n    output_bq_table: Input[Dataset],\n    feature_store_path: Output[Artifact],\n    cols_dtype: dict\n):\n    '''\n    Component to create a feature store and load the data from Bigquery.\n\n    output_bq_table: Input[Artifact]\n        Input metadata with references to the project ID, dataset ID and table\n        ID where BQ table was imported.\n        Usage:\n            output_bq_table.metadata['bq_project']\n                .example: 'my_project'\n            output_bq_table.metadata['bq_dataset_id']\n                .example: 'my_dataset'\n            output_bq_table.metadata['bq_dest_table_id']\n                .example: 'my_table_id'\n            output_bq_table.metadata['dataset_path']\n                .example: 'my_bucket/subfolder/train'\n    feature_store_path: Output[Dataset]\n        Output metadata with informations about the feature store,\n        its entity types and features.\n        Usage:\n            feature_store_path.metadata['featurestore_id']\n                .example: 'feat_store_1234'\n    cols_dtype: dict\n        Dict with the dtype of the columns from CSV.\n        Format:\n            {'I1':'int32', ..., 'C1':'hex'}\n    '''\n\n    from datetime import datetime\n    import re\n    import time\n    import logging\n\n    from google.api_core.exceptions import AlreadyExists\n\n    from google.cloud.aiplatform_v1beta1 import (FeaturestoreServiceClient)\n    from google.cloud.aiplatform_v1beta1.types import \\\n        entity_type as entity_type_pb2\n    from google.cloud.aiplatform_v1beta1.types import feature as feature_pb2\n    from google.cloud.aiplatform_v1beta1.types import \\\n        featurestore as featurestore_pb2\n    from google.cloud.aiplatform_v1beta1.types import \\\n        featurestore_service as featurestore_service_pb2\n    from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n    from google.protobuf.timestamp_pb2 import Timestamp\n\n    logging.basicConfig(level=logging.INFO)\n\n    PROJECT_ID = output_bq_table.metadata['bq_project']\n    DATASET_ID = output_bq_table.metadata['bq_dataset_id']\n    TABLE_ID = output_bq_table.metadata['bq_dest_table_id']\n\n    # To import the BQ data to feature store we need to \n    # define an EntityType which groups the features. \n    # As this dataset does not have an ID, a temporary one was \n    # created based on the row number.\n\n    # This is a temporary solution for this dataset. \n    # Reconsider for your dataset.\n\n    from google.cloud import bigquery\n\n    client = bigquery.Client(project=PROJECT_ID)\n    job_config = bigquery.QueryJobConfig(\n        destination=f'{PROJECT_ID}.{DATASET_ID}.train_users'\n    )\n\n    query_job = client.query(\n        f'''\n        SELECT CAST(DIV(ROW_NUMBER() OVER(), 100000) as STRING) user_id, *\n        FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n        ''',\n        job_config=job_config\n    )\n    query_job.result()\n\n    # Temporary TABLE_ID\n    TABLE_ID = 'train_users'\n\n    REGION = 'us-central1'\n\n    BIGQUERY_TABLE = f'{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}'\n    ID_COLUMN = \"user_id\"\n    IGNORE_COLUMNS_INGESTION = [\"user_id\", \"label\"]\n\n    FEATURE_STORE_NAME_PREFIX = \"criteo_nvt_e2e\"\n    FEATURE_STORE_NODE_COUNT = 1\n\n    ENTITY_TYPE_ID = \"users\"\n    ENTITY_TYPE_DESCRIPTION = \"Users website navigation\"\n\n    IMPORT_WORKER_COUNT = 1\n\n    # Constants based on the params\n    BIGQUERY_SOURCE = f\"bq://{BIGQUERY_TABLE}\"\n    API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n\n    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    feature_store_path.metadata['bq_source'] = BIGQUERY_SOURCE\n\n    # Create admin_client for CRUD and data_client for reading feature values.\n    admin_client = FeaturestoreServiceClient(\n        client_options={\"api_endpoint\": API_ENDPOINT}\n    )\n\n    # Represents featurestore resource path.\n    BASE_RESOURCE_PATH = admin_client.common_location_path(PROJECT_ID, REGION)\n\n    FEATURESTORE_ID = f\"{FEATURE_STORE_NAME_PREFIX}_{TIMESTAMP}\"\n    feature_store_path.metadata['featurestore_id'] = FEATURESTORE_ID\n\n    create_lro = admin_client.create_featurestore(\n        featurestore_service_pb2.CreateFeaturestoreRequest(\n            parent=BASE_RESOURCE_PATH,\n            featurestore_id=FEATURESTORE_ID,\n            featurestore=featurestore_pb2.Featurestore(\n                online_serving_config= \\\n                    featurestore_pb2.Featurestore.OnlineServingConfig(\n                        fixed_node_count=FEATURE_STORE_NODE_COUNT\n                ),\n            ),\n        )\n    )\n    logging.info(f'Creating feature store {FEATURESTORE_ID}.')\n    create_lro.result()\n\n    # Create users entity type. Monitoring disabled.\n    users_entity_type_lro = admin_client.create_entity_type(\n        featurestore_service_pb2.CreateEntityTypeRequest(\n            parent=admin_client.featurestore_path(\n                PROJECT_ID, REGION, FEATURESTORE_ID\n            ),\n            entity_type_id=ENTITY_TYPE_ID,\n            entity_type=entity_type_pb2.EntityType(\n                description=ENTITY_TYPE_DESCRIPTION\n            ),\n        )\n    )\n\n    # Wait for EntityType creation operation.\n    logging.info(f'Creating Entity Type {ENTITY_TYPE_ID}.')\n    feature_store_path.metadata['entity_type_id'] = ENTITY_TYPE_ID\n    users_entity_type_lro.result()\n\n    create_feature_requests = []\n    feature_specs = []\n\n    mapping = {\n        'int32': feature_pb2.Feature.ValueType.INT64,\n        'hex': feature_pb2.Feature.ValueType.INT64\n    }\n\n    for col, dtype in cols_dtype.items():\n        if col in IGNORE_COLUMNS_INGESTION:\n            continue\n        create_feature_requests.append(\n            featurestore_service_pb2.CreateFeatureRequest(\n                feature=feature_pb2.Feature(\n                    name=col,\n                    value_type=mapping[dtype],\n                    description=col\n                ),\n                parent=admin_client.entity_type_path(\n                    PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_TYPE_ID\n                ),\n                feature_id=re.sub(r'[\\W]+', '', col).lower(),\n            )\n        )\n        feature_specs.append(\n            featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(\n                id=re.sub(r'[\\W]+', '', col).lower(), source_field=col\n            )\n        )\n\n    for request in create_feature_requests:\n        try:\n            logging.info(admin_client.create_feature(request).result())\n        except AlreadyExists as e:\n            logging.info(e)\n\n    now = time.time()\n    seconds = int(now)\n    timestamp = Timestamp(seconds=seconds)\n\n    import_request = featurestore_service_pb2.ImportFeatureValuesRequest(\n        entity_type=admin_client.entity_type_path(\n            PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_TYPE_ID),\n        bigquery_source=io_pb2.BigQuerySource(input_uri=BIGQUERY_SOURCE),\n        entity_id_field=ID_COLUMN,\n        feature_specs=feature_specs,\n        feature_time=timestamp,\n        worker_count=IMPORT_WORKER_COUNT,\n    )\n    ingestion_lro = admin_client.import_feature_values(import_request)\n    logging.info('Start to import, will take a couple of minutes.')\n    # Polls for the LRO status and prints when the LRO has completed\n    ingestion_lro.result()\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-transform-dataset-op": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "transform_dataset_op"
            ],
            "command": [
              "sh",
              "-c",
              "(python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location 'kfp==1.8.1' --user) && \"$0\" \"$@\"",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef transform_dataset_op(\n    fitted_workflow: Input[Artifact],\n    transformed_dataset: Output[Dataset],\n    output_transformed: str,\n    gpus: str,\n    split_name: str = 'train',\n    shuffle: str = None,\n    protocol: str = 'tcp',\n    device_limit_frac: float = 0.8,\n    device_pool_frac: float = 0.9,\n    part_mem_frac: float = 0.125,\n):\n    '''\n    Component to transform a dataset according to the workflow specifications.\n\n    fitted_workflow: Input[Artifact]\n        Input metadata with the path to the fitted_workflow and the \n        location of the converted datasets in GCS (train and validation).\n        Usage:\n            fitted_workflow.metadata['datasets']['train']\n                example: '/gcs/my_bucket/converted/train'\n            fitted_workflow.metadata['fitted_workflow']\n                example: '/gcs/my_bucket/fitted_workflow'\n    transformed_dataset: Output[Dataset]\n        Output metadata with the path to the transformed dataset \n        and the validation dataset.\n        Usage:\n            transformed_dataset.metadata['transformed_dataset']\n                .example: '/gcs/my_bucket/transformed_data/train'\n            transformed_dataset.metadata['original_datasets']\n                .example: '/gcs/my_bucket/converted/train'\n    output_transformed: str,\n        Path in GCS to write the transformed parquet files.\n        Format:\n            '<bucket_name>/<subfolder1>/<subfolder>/'\n    '''\n\n    import logging\n    import nvtabular as nvt\n    import os\n\n    from dask_cuda import LocalCUDACluster\n    from dask.distributed import Client\n    from nvtabular.utils import device_mem_size\n    from nvtabular.io.shuffle import Shuffle\n\n    logging.basicConfig(level=logging.INFO)\n\n    # Define output path for transformed files\n    TRANSFORM_FOLDER = os.path.join('/gcs', output_transformed, split_name)\n\n    # Get path to dataset to be transformed\n    data_path = fitted_workflow.metadata.get('datasets').get(split_name, '')\n    if not data_path:\n        raise RuntimeError(f'Dataset does not have {split_name} split.')\n\n    # Dask Cluster defintions\n    device_size = device_mem_size()\n    device_limit = int(device_limit_frac * device_size)\n    device_pool_size = int(device_pool_frac * device_size)\n    part_size = int(part_mem_frac * device_size)\n    rmm_pool_size = (device_pool_size // 256) * 256\n\n    if gpus:\n        logging.info('Creating a Dask CUDA cluster')\n        cluster = LocalCUDACluster(\n            protocol=protocol,\n            n_workers=len(gpus.split(sep=',')),\n            CUDA_VISIBLE_DEVICES=gpus,\n            device_memory_limit=device_limit,\n            rmm_pool_size=rmm_pool_size\n        )\n        client = Client(cluster)\n    else:\n        raise Exception(\n            'Cannot create Cluster. Provide a list of available GPUs'\n        )\n\n    # Load Transformation steps\n    logging.info('Loading workflow and statistics')\n    workflow = nvt.Workflow.load(\n        fitted_workflow.metadata.get('fitted_workflow'), client\n    )\n\n    logging.info('Creating dataset definition')\n    dataset = nvt.Dataset(\n        data_path, engine=\"parquet\", part_size=part_size\n    )\n\n    if shuffle:\n        shuffle = getattr(Shuffle, shuffle)\n\n    logging.info('Starting workflow transformation')\n    workflow.transform(dataset).to_parquet(\n        output_files=len(gpus.split(sep='/')),\n        output_path=TRANSFORM_FOLDER,\n        shuffle=shuffle\n    )\n    logging.info('Finished transformation')\n\n    transformed_dataset.metadata['transformed_dataset'] = TRANSFORM_FOLDER\n    transformed_dataset.metadata['original_datasets'] = \\\n        fitted_workflow.metadata.get('datasets')\n\n"
            ],
            "image": "us-east1-docker.pkg.dev/renatoleite-mldemos/docker-images/nvt-conda",
            "resources": {
              "accelerator": {
                "count": "1",
                "type": "NVIDIA_TESLA_T4"
              },
              "cpuLimit": 8.0,
              "memoryLimit": 32.0
            }
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "nvt-pipeline-gcs-feat"
    },
    "root": {
      "dag": {
        "tasks": {
          "convert-csv-to-parquet-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-convert-csv-to-parquet-op"
            },
            "inputs": {
              "parameters": {
                "cols_dtype": {
                  "componentInputParameter": "cols_dtype"
                },
                "columns": {
                  "componentInputParameter": "columns"
                },
                "gpus": {
                  "componentInputParameter": "gpus"
                },
                "output_path": {
                  "componentInputParameter": "output_path"
                },
                "recursive": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "False"
                    }
                  }
                },
                "sep": {
                  "componentInputParameter": "sep"
                },
                "train_paths": {
                  "componentInputParameter": "train_paths"
                },
                "valid_paths": {
                  "componentInputParameter": "valid_paths"
                }
              }
            },
            "taskInfo": {
              "name": "convert-csv-to-parquet-op"
            }
          },
          "fit-dataset-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-fit-dataset-op"
            },
            "dependentTasks": [
              "convert-csv-to-parquet-op"
            ],
            "inputs": {
              "artifacts": {
                "datasets": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_datasets",
                    "producerTask": "convert-csv-to-parquet-op"
                  }
                }
              },
              "parameters": {
                "device_limit_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.8
                    }
                  }
                },
                "device_pool_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.9
                    }
                  }
                },
                "gpus": {
                  "componentInputParameter": "gpus"
                },
                "part_mem_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.125
                    }
                  }
                },
                "protocol": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "tcp"
                    }
                  }
                },
                "split_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "train"
                    }
                  }
                },
                "workflow_path": {
                  "componentInputParameter": "workflow_path"
                }
              }
            },
            "taskInfo": {
              "name": "fit-dataset-op"
            }
          },
          "import-parquet-to-bq-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-import-parquet-to-bq-op"
            },
            "dependentTasks": [
              "transform-dataset-op"
            ],
            "inputs": {
              "artifacts": {
                "transformed_dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "transformed_dataset",
                    "producerTask": "transform-dataset-op"
                  }
                }
              },
              "parameters": {
                "bq_dataset_id": {
                  "componentInputParameter": "bq_dataset_id"
                },
                "bq_dest_table_id": {
                  "componentInputParameter": "bq_dest_table_id"
                },
                "bq_project": {
                  "componentInputParameter": "bq_project"
                }
              }
            },
            "taskInfo": {
              "name": "import-parquet-to-bq-op"
            }
          },
          "load-bq-to-feature-store-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-load-bq-to-feature-store-op"
            },
            "dependentTasks": [
              "import-parquet-to-bq-op"
            ],
            "inputs": {
              "artifacts": {
                "output_bq_table": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_bq_table",
                    "producerTask": "import-parquet-to-bq-op"
                  }
                }
              },
              "parameters": {
                "cols_dtype": {
                  "componentInputParameter": "cols_dtype"
                }
              }
            },
            "taskInfo": {
              "name": "load-bq-to-feature-store-op"
            }
          },
          "transform-dataset-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-transform-dataset-op"
            },
            "dependentTasks": [
              "fit-dataset-op"
            ],
            "inputs": {
              "artifacts": {
                "fitted_workflow": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "fitted_workflow",
                    "producerTask": "fit-dataset-op"
                  }
                }
              },
              "parameters": {
                "device_limit_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.8
                    }
                  }
                },
                "device_pool_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.9
                    }
                  }
                },
                "gpus": {
                  "componentInputParameter": "gpus"
                },
                "output_transformed": {
                  "componentInputParameter": "output_transformed"
                },
                "part_mem_frac": {
                  "runtimeValue": {
                    "constantValue": {
                      "doubleValue": 0.125
                    }
                  }
                },
                "protocol": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "tcp"
                    }
                  }
                },
                "split_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "train"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "transform-dataset-op"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "bq_dataset_id": {
            "type": "STRING"
          },
          "bq_dest_table_id": {
            "type": "STRING"
          },
          "bq_project": {
            "type": "STRING"
          },
          "cols_dtype": {
            "type": "STRING"
          },
          "columns": {
            "type": "STRING"
          },
          "gpus": {
            "type": "STRING"
          },
          "output_path": {
            "type": "STRING"
          },
          "output_transformed": {
            "type": "STRING"
          },
          "recursive": {
            "type": "STRING"
          },
          "sep": {
            "type": "STRING"
          },
          "shuffle": {
            "type": "STRING"
          },
          "train_paths": {
            "type": "STRING"
          },
          "valid_paths": {
            "type": "STRING"
          },
          "workflow_path": {
            "type": "STRING"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.1"
  },
  "runtimeConfig": {}
}