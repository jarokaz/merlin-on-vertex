{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NVTabular for large scale feature engineering on BigQuery datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to do data preprocessing with NVIDIA NVTabular on Vertex AI Pipeline steps using BigQuery as the data source.  \n",
    "You will create a pipeline with the following steps:\n",
    " - Export data from BigQuery tables to Parquet files in GCS\n",
    " - Define the DAG with transformation steps and create a Workflow\n",
    " - Fit the dataset (calculate statistics necessary for data transformation)\n",
    " - Transform the data\n",
    " - Output transformed parquet files to GCS\n",
    "\n",
    "The goal is to present how to use NVTabular to transform the data on multiple GPUs.  \n",
    "The dataset used for this tutorial is the [Criteo 1TB Click Logs dataset](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).  \n",
    "\n",
    "Architecture overview:\n",
    "\n",
    "<img src=\"./images/pipeline_2.png\" alt=\"Pipeline\" style=\"height: 60%; width:60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "import config\n",
    "\n",
    "# Standard\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    Clip,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "# Import components and pipeline definition\n",
    "from src.preprocessing.pipeline_bq import preprocessing_pipeline_bq\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Workflow DAG definition for data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns and dtypes definition\n",
    "cont_names = [\"I\" + str(x) for x in range(1, 14)]\n",
    "cat_names = [\"C\" + str(x) for x in range(1, 27)]\n",
    "columns = [\"label\"] + cont_names + cat_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation pipeline\n",
    "num_buckets = 10000000\n",
    "categorify_op = Categorify(max_size=num_buckets)\n",
    "cat_features = cat_names >> categorify_op\n",
    "cont_features = cont_names >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "features = cat_features + cont_features + ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save workflow\n",
    "workflow = nvt.Workflow(features)\n",
    "workflow.save('./saved_workflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./saved_workflow/workflow.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://./saved_workflow/metadata.json [Content-Type=application/json]...\n",
      "\n",
      "Operation completed over 2 objects/1.6 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Upload saved workflow to GCS\n",
    "! gsutil cp -r ./saved_workflow/ 'gs://{config.SAVED_WORKFLOW_PATH}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters values for pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset_id = 'criteo_pipeline' # Dataset ID\n",
    "bq_table_train = 'train' # Table ID for the training data\n",
    "bq_table_valid = 'valid' # Table ID for the validation data\n",
    "\n",
    "gpus = '0' # Identifier of the GPU. As you will execute with only 1 GPU, only the first identier is passed.\n",
    "           # If you were to execute the pipeline with 4 GPUs, you should use '0,1,2,3'.\n",
    "\n",
    "recursive = False # If the train/valid paths should be navigated recursivelly\n",
    "shuffle = None # How to shuffle the dataset both in the conversion from CSV to PARQUET and during transformation.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary with parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionarry will all the parameters defined until now\n",
    "parameter_values = {\n",
    "    'bq_table_train': bq_table_train,\n",
    "    'bq_table_valid': bq_table_valid,\n",
    "    'output_converted': config.OUTPUT_CONVERTED,\n",
    "    'bq_project': config.PROJECT_ID,\n",
    "    'bq_dataset_id': bq_dataset_id,\n",
    "    'location': config.LOCATION,\n",
    "    'gpus': gpus,\n",
    "    'workflow_path': config.SAVED_WORKFLOW_PATH,\n",
    "    'output_transformed': config.OUTPUT_TRANSFORMED,\n",
    "    'recursive': json.dumps(recursive),\n",
    "    'shuffle': json.dumps(shuffle)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFP pipeline compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline.\n",
    "# This command will validate the pipeline and generate a JSON file with its specifications\n",
    "PACKAGE_PATH = 'nvt_bq_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_pipeline_bq,\n",
    "       package_path=PACKAGE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize aiplatform SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize aiplatform SDK client\n",
    "aiplatform.init(\n",
    "    project=config.PROJECT_ID,\n",
    "    location=config.REGION,\n",
    "    staging_bucket=config.STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f'{TIMESTAMP}_nvt_bq_pipeline',\n",
    "    template_path=PACKAGE_PATH,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
