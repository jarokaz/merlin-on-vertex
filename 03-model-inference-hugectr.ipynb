{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Serving NVIDIA HugeCTR model using NVIDIA Triton and Vertex AI Prediction\n",
    "\n",
    "This notebook demonstrates how to serve NVIDIA HugeCTR deep learning models using NVIDIA Triton Inference Server and Vertex AI Prediction.\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "1. Creating Triton ensemble models that combine NVTabular preprocessing workflows and HugeCTR models\n",
    "2. Building a Vertex Prediction custom serving container image for serving the ensembles with Triton Inference server. \n",
    "2. Registering and deploying the ensemble models with Vertex Prediction Models and Endpoints.\n",
    "5. Getting online predictions from the deployed ensembles.\n",
    "\n",
    "To fully benefit from the content covered in this notebook, you should have a solid understanding of key Vertex AI Prediction concepts like models, endpoints, and model deployments. We strongly recommend reviewing [Vertex AI Prediction documentation](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) before proceeding.\n",
    "\n",
    "### Triton Inference Server Overview\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides an inferencing solution optimized for both CPUs and GPUs. Triton can run multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding. It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensembles for use cases that require multiple models to perform end-to-end inference.\n",
    "\n",
    "The following figure shows the Triton Inference Server high-level architecture.\n",
    "\n",
    "<img src=\"./images/triton-architecture.png\" alt=\"Triton Architecture\" style=\"width:70%\"/>\n",
    "\n",
    "\n",
    "- The model repository is a file-system based repository of the models that Triton will make available for inferencing. \n",
    "- Inference requests arrive at the server via either HTTP/REST or gRPC and are then routed to the appropriate per-model scheduler. \n",
    "- Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis.\n",
    "- The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs.\n",
    "\n",
    "\n",
    "Triton server provides readiness and liveness health endpoints, as well as utilization, throughput, and latency metrics, which enable the integration of Triton into deployment environments, such as Vertex AI Prediction.\n",
    "\n",
    "Refer to [Triton Inference Server Architecture](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md) for more detailed information.\n",
    "\n",
    "### Triton Inference Server on Vertex AI Prediction\n",
    "\n",
    "\n",
    "\n",
    "In this section, we describe the deployment of Triton Inference Server on Vertex AI Prediction. Although, the focus of this notebook is on demonstrating how to serve an ensemble of an NVTabular preprocessing workflow and a HugeCTR model, the outlined design patterns are applicable to a wider set of serving scenarios.  The following figure shows a deployment architecture.\n",
    "\n",
    "<img src=\"./images/triton-in-vertex.png\" alt=\"Triton on Vertex AI Prediction\" style=\"width:70%\"/>\n",
    "\n",
    "\n",
    "Triton Inference Server runs inside a container based on a custom serving image. The custom container image is built on top of [NVIDIA Merlin Inference image](https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-inference) and adds packages and configurations to align with Vertex AI [requirements for custom serving container images](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements). \n",
    "\n",
    "An ensemble to be served by Triton is registered with Vertex AI Prediction as a `Model`. The `Model`'s metadata reference a location of the ensemble artifacts in Google Cloud Storage and the custom serving container and its configurations. \n",
    "\n",
    "After the model is deployed to a Vertex AI Prediction endpoint, the entrypoint script of the custom container copies the ensemble's artifacts from the GCS location to a local file system in the container. It then starts Triton, referencing a local copy of the ensemble as Triton's model repository. \n",
    "\n",
    "Triton loads the models comprising the ensemble and exposes inference, health, and model management REST endpoints using [standard inference protocols](https://github.com/kserve/kserve/tree/master/docs/predict-api/v2). The Triton's inference endpoint - `/v2/models/{ENSEMBLE_NAME}/infer` is mapped to Vertex AI Prediction predict route and exposed to external clients through Vertex Prediction endpoint. The Triton's health endpoint - `/v2/health/ready` - is mapped to Vertex AI Prediction health route and used by Vertex AI Prediction for [health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health).\n",
    "\n",
    "To invoke the ensemble through the Vertex AI Prediction endpoint you need to format your request using a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference) or a [Inference Request JSON Object with a binary extension](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md) and submit a request to Vertex AI Prediction [REST rawPredict endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/rawPredict). You need to use the `rawPredict` rather than `predict` endpoint because inference request formats used by Triton are not compatible with the Vertex AI Prediction [standard input format](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#formatting-prediction-input).\n",
    "\n",
    "\n",
    "### Notebook flow\n",
    "\n",
    "This notebook assumes that you have access to both: a trained HugeCTR model and a fitted NVTabular workflow that converts raw inputs into the intputs required by the model. These artifacts are created by the [01-dataset-preprocessing.ipynb](01-dataset-preprocessing.ipynb) and [02-model-training-hugectr.ipynb](02-model-training-hugectr.ipynb) notebooks.\n",
    "\n",
    "As you walk through the notebook you will execute the following tasks:\n",
    "\n",
    "- Configure the notebook environment settings, including GCP project, compute region, and the GCS locations of a HugeCTR trained model and an NVTabular fitted workflow.\n",
    "- Create an ensemble model consisting of the fitted model for input preprocessing and the HugeCTR model for generating predictions\n",
    "- Build a custom Vertex serving container based on NVIDIA NGC Merlin Inference container\n",
    "- Register the ensemble as a Vertex Prediction model\n",
    "- Create a Vertex Prediction endpoint\n",
    "- Deploy the model endpoint\n",
    "- Invoke the deployed ensemble model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a Vertex AI compute region, and a Vertex AI staging GCS bucket. \n",
    "You also set the locations of a fitted NVTaubular workflow, a trained HugeCTR model, and a set of constants that are used to create names and display names of Vertex AI Prediction resources.\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from src.serving import export\n",
    "from src.configs import EnsembleConfig\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d16e2-f098-4ab0-bed9-41665eec3790",
   "metadata": {},
   "source": [
    "Set the below constants to your project id, a compute region for Vertex AI and a GCS bucket that will be used for Vertex AI staging and storing exported model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jk-mlops-dev' # Change to your project.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "STAGING_BUCKET = 'jk-merlin-dev' # Change to your bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8acbb-305a-412d-904b-b9d274852e53",
   "metadata": {},
   "source": [
    "`LOCAL_WORKSPACE` is used for staging artifacts that need to be processed on a local file system. `MODEL_ARTIFACTS_REPOSITORY` is a root GCS location where the exported ensemble model artifacts will be stored. If you run this notebook on Vertex Workbench you don't need to change these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b732f8-6837-4c82-99a3-9b09a1298898",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_WORKSPACE = '/home/jupyter/staging'\n",
    "MODEL_ARTIFACTS_REPOSITORY = f'gs://{STAGING_BUCKET}/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f3935-c192-4e77-bdf9-687c9bff79bc",
   "metadata": {},
   "source": [
    "The following set of constants will be used to create names and display names of Vertex Prediction resources like models, endpoints, and model deployments. The HugeCTR model trained in the previous notebooks is a *DeepFM* deep learning ranking model so the default model name is set to `deepfm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d994ba-d39c-4b3d-a3ae-776f9eb995cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'deepfm'\n",
    "MODEL_VERSION = 'v01'\n",
    "MODEL_DISPLAY_NAME = f'hugectr-{MODEL_NAME}-{MODEL_VERSION}'\n",
    "MODEL_DESCRIPTION = 'HugeCTR DeepFM model'\n",
    "ENDPOINT_DISPLAY_NAME = f'hugectr-{MODEL_NAME}-{MODEL_VERSION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918199b-637c-4242-90be-2cb159ec4de6",
   "metadata": {},
   "source": [
    "The following constants set the name and the location of the Dockerfile for the custom serving container you will build in the following section of the notebook. You don't need to change these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b282bce1-b820-47dc-8f3a-257414f3fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'triton-deploy-hugectr'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581a920-e41d-48ee-9a03-fc00e2c12c41",
   "metadata": {},
   "source": [
    "And finally, the `WORKFLOW_MODEL_PATH` and the `HUGECTR_MODEL_PATH` should be updated to point to GCS locations of your NVTabular fitted workflow and the trained HugeCTR model generated by the [01-dataset-preprocessing.ipynb](01-dataset-preprocessing.ipynb) and [02-model-training-hugectr.ipynb](02-model-training-hugectr.ipynb) notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991260eb-c5f5-4f6e-84d4-9b127cdd0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_MODEL_PATH = \"gs://criteo-datasets/criteo_processed_parquet/workflow\" # Change to GCS path of the nvt workflow.\n",
    "HUGECTR_MODEL_PATH = \"gs://merlin-models/hugectr_deepfm_21.09\" # Change to GCS path of the hugectr trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e3f45-2bab-48fb-9b81-4e0e65635ece",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e0cca3-11e6-4454-af41-c5c632dbbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9c710-537a-49c9-86d5-fae7e9f2b4be",
   "metadata": {},
   "source": [
    "## 1. Exporting Triton ensemble model\n",
    "\n",
    "A Triton ensemble model represents a pipeline of one or more models and the connection of input and output tensors between these models. Ensemble models are intended to encapsulate inference pipelines that involves multiple steps, each performed by a different model. For example, a common  \"data preprocessing -> inference -> data postprocessing\" pattern. Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors between client and serving endpoints and minimize the number of requests that must be sent to Triton. \n",
    "\n",
    "In our case, an inference pipeline comprises two steps: input preprocessing using a fitted NVTabular workflow and generating predictions using a HugeCTR ranking model.\n",
    "\n",
    "An ensemble model is not an actual serialized model. There are no addtional model artifacts created when an ensemble is defined. It is a configuration that specifies which actual models comprise the ensemble, the execution flow when processing an inference request and the flow of data between inputs and outputs of the component models. This configuration is defined using the same [protocol buffer](https://developers.google.com/protocol-buffers) based configuration format as used for serving other model types in Triton. Refer to [Trition Inference Server Model Configuration guide](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) for detailed information about configuring models and model ensembles.\n",
    "\n",
    "You can create an ensemble model manually by arranging the component models into the prescribed folder structure and editing the required configuration files. For ensemble models that utilize the \"NVTabular workflow -> Inference Model\" processing pattern you can utilize a set of utility functions provided by the `nvtabular.inference.triton` module. Specifically to create a \"NVTabular workflow -> HugeCTR model\" ensemble, as utilized in this notebook, you can use the `nvtabular.inference.triton.export_hugectr_ensemble` function.\n",
    "\n",
    "We have encapsulated the ensemble export logic in the `src.serving.export_ensemble` function. In addition to calling `nvtabular.inference.triton.export_hugectr_ensemble`, the function also creates a JSON configuration file required by Triton when serving HugeCTR models. This file - `ps.json` - specifies the locations of different components comprising a saved HugeCTR model and is used by Triton HugeCTR backend to correctly load the saved model and prepare it for serving. \n",
    "\n",
    "Recall that the entrypoint script in the custom serving container copies the ensemble's models artifacts from a source GCS location as prepared by Vertex AI Prediction into the serving container's local file systems. The `ps.json` file needs to use the paths that correctly point to saved model artifacts in the container's file system. Also some of the paths embedded in the configs generated by `nvtabular.inference.triton.export_hugectr_ensemble` use absolute paths and need to be properly set. The `src.serving.export_ensemble` function handles all of that. You can specify the target root folder in the containers local file system using the `model_repository_path` parameter and all the paths will be adjusted accordingly.\n",
    "\n",
    "\n",
    "\n",
    "### Copy a HugeCTR saved model and a fitted NVTabular workflow to a local staging folder\n",
    "\n",
    "The `nvtabular.inference.triton.export_hugectr_ensemble` does not support GCS. As such you need to copy NVTabular workflow and HugeCTR model artifacts to a local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4fc36f-621e-4471-97ac-0580d2bca48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C1.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C10.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C11.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C12.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C13.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C14.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C15.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C16.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C17.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C19.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C18.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C21.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C2.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C20.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C22.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C24.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C23.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C25.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C26.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C3.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/metadata.json... \n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C4.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C5.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C9.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C6.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/workflow.pkl...  \n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C7.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C8.parquet...\n",
      "\\ [28/28 files][289.7 MiB/289.7 MiB] 100% Done                                  \n",
      "Operation completed over 28 objects/289.7 MiB.                                   \n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm.json...\n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_opt_sparse_0.model...   \n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_sparse_0.model/emb_vector...\n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_sparse_0.model/slot_id...\n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_sparse_0.model/key...   \n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm_dense_0.model...         \n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm_opt_dense_0.model...     \n",
      "\\ [7/7 files][  4.2 GiB/  4.2 GiB] 100% Done  81.3 MiB/s ETA 00:00:00           \n",
      "Operation completed over 7 objects/4.2 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(LOCAL_WORKSPACE):\n",
    "    shutil.rmtree(LOCAL_WORKSPACE)\n",
    "os.makedirs(LOCAL_WORKSPACE)\n",
    "\n",
    "!gsutil -m cp -r {WORKFLOW_MODEL_PATH} {LOCAL_WORKSPACE}\n",
    "!gsutil -m cp -r {HUGECTR_MODEL_PATH} {LOCAL_WORKSPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f1d-2e9b-4035-b982-d4c289e67dec",
   "metadata": {},
   "source": [
    "### Export the ensemble model\n",
    "\n",
    "The `src.export.export_ensemble` utility function takes a number of arguments that are required to set up a proper flow of tensors between inputs and outputs of the NVTabular workflow and the HugeCTR model.\n",
    "\n",
    "- `model_name` - The model name that will be used as a prefix for the generated ensemble artifacts.\n",
    "- `workflow_path` - The local path to the NVTabular workflow\n",
    "- `saved_model_path` - The local path to the saved HugeCTR model\n",
    "- `output_path` - The local path to the location where an ensemble will be exported\n",
    "- `model_repository_path` - The path to use as a root  in `ps.json` and other config files\n",
    "- `max_batch` - The maximum size of a serving batch that will be supported by the ensemble \n",
    "\n",
    "\n",
    "The following settings should match the settings of the NVTabular workflow\n",
    "\n",
    "- `categorical_columns` - The list of names of categorical input features to the NVTabular workflow\n",
    "- `continuous_columns` - The list of names of continuous input features to the NVTabular workflow\n",
    "\n",
    "\n",
    "The following settings should match the respective settings in the HugeCTR model\n",
    "\n",
    "- `num_outputs` - The number of outputs from the HugeCTR model\n",
    "- `embedding_vector_size` - The size of an embedding vector used by the HugeCTR model\n",
    "- `num_slots` - The number of slots used for sparse features of the HugeCTR model\n",
    "- `max_nnz` - This value controls how sparse features are coded in the embedding arrays \n",
    "\n",
    "\n",
    "As noted before, in this notebook we assume that you generated the NVTabular workflow and the HugeCTR model using the the [01-dataset-preprocessing.ipynb](01-dataset-preprocessing.ipynb) and [02-model-training-hugectr.ipynb](02-model-training-hugectr.ipynb) notebooks.\n",
    "\n",
    "The workflow captures the preprocessing logic for the Criteo dataset and the HugeCTR model is an implementation of [the DeepFM CTR model](https://arxiv.org/abs/1703.04247). The required configurations for the workflow and the model are managed in the `src.configs.EnsembleConfig()` configuration class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76abc820-3603-4b8b-9dec-a22c07d31d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_config = EnsembleConfig()\n",
    "local_workflow_path = Path(LOCAL_WORKSPACE) / Path(WORKFLOW_MODEL_PATH).parts[-1]\n",
    "local_saved_model_path = Path(LOCAL_WORKSPACE) / Path(HUGECTR_MODEL_PATH).parts[-1]\n",
    "local_ensemble_path = Path(LOCAL_WORKSPACE) / f'triton-ensemble-{time.strftime(\"%Y%m%d%H%M%S\")}'\n",
    "model_repository_path = '/models'\n",
    "\n",
    "export.export_ensemble(\n",
    "    model_name=MODEL_NAME,\n",
    "    workflow_path=local_workflow_path,\n",
    "    saved_model_path=local_saved_model_path,\n",
    "    output_path=local_ensemble_path,\n",
    "    categorical_columns=ensemble_config.categorical_columns,\n",
    "    continuous_columns=ensemble_config.continuous_columns,\n",
    "    label_columns=ensemble_config.label_columns,\n",
    "    num_slots=ensemble_config.num_slots,\n",
    "    max_nnz=ensemble_config.max_nnz,\n",
    "    num_outputs=ensemble_config.num_outputs,\n",
    "    embedding_vector_size=ensemble_config.embedding_vector_size,\n",
    "    max_batch_size=ensemble_config.max_batch_size,\n",
    "    model_repository_path=model_repository_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c48c08-1267-4117-ae54-1331d7afe4ed",
   "metadata": {},
   "source": [
    "The previous cell created the following local folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d654b85e-db4a-47c2-97d2-960bc802fef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x 5 jupyter jupyter 4096 Oct 30 20:34 .\n",
      "drwxr-xr-x 5 jupyter jupyter 4096 Oct 30 20:34 ..\n",
      "drwxr-xr-x 3 jupyter jupyter 4096 Oct 30 20:34 deepfm\n",
      "drwxr-xr-x 3 jupyter jupyter 4096 Oct 30 20:34 deepfm_ens\n",
      "drwxr-xr-x 3 jupyter jupyter 4096 Oct 30 20:34 deepfm_nvt\n",
      "-rw-r--r-- 1 jupyter jupyter  222 Oct 30 20:34 ps.json\n"
     ]
    }
   ],
   "source": [
    "! ls -la {local_ensemble_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6401c-dca4-4e45-ad64-e7dd6c8e9e4f",
   "metadata": {},
   "source": [
    "The `deepfm` folder contains artifacts and configurations for the HugeCTR model. The `deepfm_ens` folder contains a configuration for the ensemble model. And the `deepfm_nvt` contains artifacts and configurations for the NVTabular preprocessing workflow. The `ps.json` file contains information required by the Triton's HugeCTR backend.\n",
    "\n",
    "Notice that the file paths in `ps.json`  use the value from `model_repository_path`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1671792c-6566-4982-b2c3-7aab7a92e6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"supportlonglong\": true, \"models\": [{\"model\": \"deepfm\", \"sparse_files\": [\"/models/deepfm/1/deepfm0_sparse_0.model\"], \"dense_file\": \"/models/deepfm/1/deepfm_dense_0.model\", \"network_file\": \"/models/deepfm/1/deepfm.json\"}]}"
     ]
    }
   ],
   "source": [
    "! cat {local_ensemble_path}/ps.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fdc6e-1047-44a0-b6fa-2bd7c5b6e232",
   "metadata": {},
   "source": [
    "### Upload the ensemble to GCS\n",
    "\n",
    "In the later steps you will register the exported ensemble model as a Vertex AI Prediction model resource. Before doing that we need to move the ensemble to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad435708-b92b-40d6-83f9-3b685bfeaba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/1/deepfm0_opt_sparse_0.model [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/1/deepfm.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/1/deepfm_opt_dense_0.model [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/1/deepfm_dense_0.model [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/1/deepfm0_sparse_0.model/emb_vector [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/1/deepfm0_sparse_0.model/slot_id [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm/1/deepfm0_sparse_0.model/key [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_ens/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/model.py [Content-Type=text/x-python]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/metadata.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/workflow.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C23.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/column_types.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C25.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C6.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C13.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C9.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C21.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C4.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C16.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C15.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C22.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C24.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C17.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C10.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C7.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C19.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C3.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C12.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C26.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C8.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C5.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C2.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C18.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/ps.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C20.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C1.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C11.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211030203429/deepfm_nvt/1/workflow/categories/unique.C14.parquet [Content-Type=application/octet-stream]...\n",
      "\\ [41/41 files][  4.5 GiB/  4.5 GiB] 100% Done 168.2 MiB/s ETA 00:00:00         \n",
      "Operation completed over 41 objects/4.5 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "gcs_ensemble_path = '{}/{}'.format(MODEL_ARTIFACTS_REPOSITORY, Path(local_ensemble_path).parts[-1])\n",
    "\n",
    "!gsutil -m cp -r {local_ensemble_path}/* {gcs_ensemble_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f662a18-0b51-4b30-8e3f-81daf64ca5a9",
   "metadata": {},
   "source": [
    "## 2. Building a custom serving container \n",
    "\n",
    "The custom serving container is derived from the NVIDIA NGC Merlin inference container. It adds Google Cloud SDK and an entrypoint script that executes the tasks described in detail in the overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19efaa3e-bace-47d6-8023-dc5a688ae7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/merlin-on-gcp/dongm-merlin-inference-hugectr:latest\n",
      "\n",
      "EXPOSE 8000\n",
      "EXPOSE 8001\n",
      "EXPOSE 8002\n",
      "\n",
      "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
      "\n",
      "WORKDIR /src\n",
      "\n",
      "COPY serving/entrypoint.sh ./\n",
      "RUN chmod +x entrypoint.sh\n",
      "\n",
      "ENTRYPOINT [\"./entrypoint.sh\"]"
     ]
    }
   ],
   "source": [
    "! cat {DOCKERFILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0c951-23b5-4f92-b809-b6fa323beec2",
   "metadata": {},
   "source": [
    "As described in detail in the overview, the entry point script copies the ensemble artifacts to the serving container's local file system and starts Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24cd7086-111a-4336-a3c6-108a63a994b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# Copyright 2021 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#            http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "# Set up a global error handler\n",
      "err_handler() {\n",
      "    echo \"Error on line: $1\"\n",
      "    echo \"Caused by: $2\"\n",
      "    echo \"That returned exit status: $3\"\n",
      "    echo \"Aborting...\"\n",
      "    exit $3\n",
      "}\n",
      "\n",
      "trap 'err_handler \"$LINENO\" \"$BASH_COMMAND\" \"$?\"' ERR\n",
      "\n",
      "\n",
      "if [ -z \"${AIP_STORAGE_URI}\" ]\n",
      "  then\n",
      "    echo 'AIP_STORAGE_URI not set. Exiting ....'\n",
      "    exit 1\n",
      "fi\n",
      "\n",
      "if [ -z \"$1\" ]\n",
      "  then\n",
      "    MODEL_REPOSITORY=/models\n",
      "  else\n",
      "    MODEL_REPOSITORY=$1\n",
      "fi\n",
      "\n",
      "    \n",
      "echo \"Copying model ensemble from ${AIP_STORAGE_URI} to ${MODEL_REPOSITORY}\"\n",
      "mkdir ${MODEL_REPOSITORY} \n",
      "gsutil -m cp -r ${AIP_STORAGE_URI}/* ${MODEL_REPOSITORY}\n",
      "\n",
      "# gsutil does not copy empty dirs so create a version folder for the ensemble\n",
      "ENSEMBLE_DIR=$(ls ${MODEL_REPOSITORY} | grep ens)\n",
      "mkdir ${MODEL_REPOSITORY}/${ENSEMBLE_DIR}/1 \n",
      "\n",
      "echo \"Starting Triton Server\"\n",
      "LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2 tritonserver --model-repository=$MODEL_REPOSITORY \\\n",
      "--backend-config=hugectr,ps=$MODEL_REPOSITORY/ps.json "
     ]
    }
   ],
   "source": [
    "! cat src/serving/entrypoint.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113e891-7324-4bec-9381-f775dd6df4d5",
   "metadata": {},
   "source": [
    "You use [Cloud Build](https://cloud.google.com/build) to build the serving container and push it to your projects [Container Registry](https://cloud.google.com/container-registry#:~:text=Container%20Registry%20is%20a%20single,pipelines%20to%20get%20fast%20feedback.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed3fff22-3541-466a-9d2c-8e007cfb4940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 28 file(s) totalling 83.4 KiB before compression.\n",
      "Uploading tarball of [src] to [gs://jk-mlops-dev_cloudbuild/source/1635626122.507069-a442fe96d95b4980b082ca4c90be2877.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-mlops-dev/locations/global/builds/95ac0104-5506-4a60-9ec9-b83ad27eb147].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/95ac0104-5506-4a60-9ec9-b83ad27eb147?project=895222332033].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"95ac0104-5506-4a60-9ec9-b83ad27eb147\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-mlops-dev_cloudbuild/source/1635626122.507069-a442fe96d95b4980b082ca4c90be2877.tgz#1635626122742574\n",
      "Copying gs://jk-mlops-dev_cloudbuild/source/1635626122.507069-a442fe96d95b4980b082ca4c90be2877.tgz#1635626122742574...\n",
      "/ [1 files][ 18.6 KiB/ 18.6 KiB]                                                \n",
      "Operation completed over 1 objects/18.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  113.2kB\n",
      "Step 1/9 : FROM gcr.io/merlin-on-gcp/dongm-merlin-inference-hugectr:latest\n",
      "latest: Pulling from merlin-on-gcp/dongm-merlin-inference-hugectr\n",
      "f3ef4ff62e0d: Pulling fs layer\n",
      "0a5a58fe9f80: Pulling fs layer\n",
      "ffe911d7d638: Pulling fs layer\n",
      "b6ca3d91503f: Pulling fs layer\n",
      "2153f05ef7a8: Pulling fs layer\n",
      "960ad9fc339d: Pulling fs layer\n",
      "423cea99f7d2: Pulling fs layer\n",
      "ea4474a67b49: Pulling fs layer\n",
      "b397fb86b3bc: Pulling fs layer\n",
      "927627363759: Pulling fs layer\n",
      "312d920fedcc: Pulling fs layer\n",
      "7a50c2cfea6b: Pulling fs layer\n",
      "06daa3d0f46c: Pulling fs layer\n",
      "aec116761f98: Pulling fs layer\n",
      "2812dcb9b1f1: Pulling fs layer\n",
      "0b40e0143bfd: Pulling fs layer\n",
      "0617ebfc4a21: Pulling fs layer\n",
      "45a7802e347f: Pulling fs layer\n",
      "9c3aac051d0f: Pulling fs layer\n",
      "c06cf176a9ca: Pulling fs layer\n",
      "6790e5e29cc3: Pulling fs layer\n",
      "896ac25d66a9: Pulling fs layer\n",
      "6d2526d2cea7: Pulling fs layer\n",
      "acfedf2a4791: Pulling fs layer\n",
      "17c5cc3b0adc: Pulling fs layer\n",
      "4bfb0e4cc940: Pulling fs layer\n",
      "58e848323dd7: Pulling fs layer\n",
      "84d12d39fb8e: Pulling fs layer\n",
      "b4564e51587f: Pulling fs layer\n",
      "bbb32f3289e5: Pulling fs layer\n",
      "0904b13f25db: Pulling fs layer\n",
      "043bccf18a62: Pulling fs layer\n",
      "80666faa5792: Pulling fs layer\n",
      "d9c940cdfe35: Pulling fs layer\n",
      "2a19abfaf29f: Pulling fs layer\n",
      "6f306638f1f4: Pulling fs layer\n",
      "95bcd15ec1e4: Pulling fs layer\n",
      "a4a599db2167: Pulling fs layer\n",
      "588b6420e089: Pulling fs layer\n",
      "5a0ee7acb591: Pulling fs layer\n",
      "fa711c310b5e: Pulling fs layer\n",
      "f54dae1543fd: Pulling fs layer\n",
      "01f09199ca12: Pulling fs layer\n",
      "320e5792a2cc: Pulling fs layer\n",
      "dd7888d57680: Pulling fs layer\n",
      "db3ba3497665: Pulling fs layer\n",
      "b4f737bb955c: Pulling fs layer\n",
      "5fd3800cea87: Pulling fs layer\n",
      "e98147207b7d: Pulling fs layer\n",
      "b9a3e29ab76c: Pulling fs layer\n",
      "7f7e35ba03e8: Pulling fs layer\n",
      "56c8234f1ff1: Pulling fs layer\n",
      "954cbb03adfc: Pulling fs layer\n",
      "810f62c117c7: Pulling fs layer\n",
      "960ad9fc339d: Waiting\n",
      "6926159838dd: Pulling fs layer\n",
      "f05cdbdb7a3a: Pulling fs layer\n",
      "423cea99f7d2: Waiting\n",
      "5a06c1826b9b: Pulling fs layer\n",
      "a2412c74818c: Pulling fs layer\n",
      "b397fb86b3bc: Waiting\n",
      "ea4474a67b49: Waiting\n",
      "b6ca3d91503f: Waiting\n",
      "2153f05ef7a8: Waiting\n",
      "927627363759: Waiting\n",
      "312d920fedcc: Waiting\n",
      "0b40e0143bfd: Waiting\n",
      "7a50c2cfea6b: Waiting\n",
      "6790e5e29cc3: Waiting\n",
      "896ac25d66a9: Waiting\n",
      "aec116761f98: Waiting\n",
      "6d2526d2cea7: Waiting\n",
      "2812dcb9b1f1: Waiting\n",
      "0617ebfc4a21: Waiting\n",
      "45a7802e347f: Waiting\n",
      "17c5cc3b0adc: Waiting\n",
      "4bfb0e4cc940: Waiting\n",
      "84d12d39fb8e: Waiting\n",
      "58e848323dd7: Waiting\n",
      "043bccf18a62: Waiting\n",
      "588b6420e089: Waiting\n",
      "9c3aac051d0f: Waiting\n",
      "5a0ee7acb591: Waiting\n",
      "c06cf176a9ca: Waiting\n",
      "80666faa5792: Waiting\n",
      "fa711c310b5e: Waiting\n",
      "f54dae1543fd: Waiting\n",
      "d9c940cdfe35: Waiting\n",
      "2a19abfaf29f: Waiting\n",
      "6f306638f1f4: Waiting\n",
      "95bcd15ec1e4: Waiting\n",
      "a4a599db2167: Waiting\n",
      "b4564e51587f: Waiting\n",
      "01f09199ca12: Waiting\n",
      "bbb32f3289e5: Waiting\n",
      "320e5792a2cc: Waiting\n",
      "dd7888d57680: Waiting\n",
      "954cbb03adfc: Waiting\n",
      "db3ba3497665: Waiting\n",
      "f05cdbdb7a3a: Waiting\n",
      "810f62c117c7: Waiting\n",
      "6926159838dd: Waiting\n",
      "5a06c1826b9b: Waiting\n",
      "b9a3e29ab76c: Waiting\n",
      "0904b13f25db: Waiting\n",
      "b4f737bb955c: Waiting\n",
      "e98147207b7d: Waiting\n",
      "7f7e35ba03e8: Waiting\n",
      "5fd3800cea87: Waiting\n",
      "a2412c74818c: Waiting\n",
      "56c8234f1ff1: Waiting\n",
      "acfedf2a4791: Waiting\n",
      "06daa3d0f46c: Waiting\n",
      "f4804d3ccc99: Pulling fs layer\n",
      "62c207e6773c: Pulling fs layer\n",
      "5962889b0c76: Pulling fs layer\n",
      "f4804d3ccc99: Waiting\n",
      "425009f61737: Pulling fs layer\n",
      "62c207e6773c: Waiting\n",
      "5962889b0c76: Waiting\n",
      "425009f61737: Waiting\n",
      "cc4eb6f55aa9: Pulling fs layer\n",
      "cc4eb6f55aa9: Waiting\n",
      "2f1a08e2c4d0: Pulling fs layer\n",
      "7cc9ab06c351: Pulling fs layer\n",
      "fa4aa2818f5f: Pulling fs layer\n",
      "82e1ac2422cd: Pulling fs layer\n",
      "0d1a267e9d99: Pulling fs layer\n",
      "a0ba993bb711: Pulling fs layer\n",
      "08dd4105bc4d: Pulling fs layer\n",
      "41184579c3b7: Pulling fs layer\n",
      "0c0c0dd6e1be: Pulling fs layer\n",
      "82e1ac2422cd: Waiting\n",
      "0d1a267e9d99: Waiting\n",
      "a0ba993bb711: Waiting\n",
      "08dd4105bc4d: Waiting\n",
      "41184579c3b7: Waiting\n",
      "0c0c0dd6e1be: Waiting\n",
      "2f1a08e2c4d0: Waiting\n",
      "7cc9ab06c351: Waiting\n",
      "fa4aa2818f5f: Waiting\n",
      "f3ef4ff62e0d: Verifying Checksum\n",
      "f3ef4ff62e0d: Download complete\n",
      "b6ca3d91503f: Download complete\n",
      "ffe911d7d638: Verifying Checksum\n",
      "ffe911d7d638: Download complete\n",
      "2153f05ef7a8: Download complete\n",
      "960ad9fc339d: Download complete\n",
      "0a5a58fe9f80: Verifying Checksum\n",
      "0a5a58fe9f80: Download complete\n",
      "423cea99f7d2: Verifying Checksum\n",
      "423cea99f7d2: Download complete\n",
      "ea4474a67b49: Verifying Checksum\n",
      "ea4474a67b49: Download complete\n",
      "b397fb86b3bc: Verifying Checksum\n",
      "b397fb86b3bc: Download complete\n",
      "927627363759: Verifying Checksum\n",
      "927627363759: Download complete\n",
      "312d920fedcc: Download complete\n",
      "06daa3d0f46c: Verifying Checksum\n",
      "06daa3d0f46c: Download complete\n",
      "aec116761f98: Download complete\n",
      "2812dcb9b1f1: Download complete\n",
      "f3ef4ff62e0d: Pull complete\n",
      "0b40e0143bfd: Verifying Checksum\n",
      "0b40e0143bfd: Download complete\n",
      "0617ebfc4a21: Verifying Checksum\n",
      "0617ebfc4a21: Download complete\n",
      "45a7802e347f: Verifying Checksum\n",
      "45a7802e347f: Download complete\n",
      "9c3aac051d0f: Verifying Checksum\n",
      "9c3aac051d0f: Download complete\n",
      "6790e5e29cc3: Verifying Checksum\n",
      "6790e5e29cc3: Download complete\n",
      "c06cf176a9ca: Verifying Checksum\n",
      "c06cf176a9ca: Download complete\n",
      "6d2526d2cea7: Verifying Checksum\n",
      "6d2526d2cea7: Download complete\n",
      "acfedf2a4791: Verifying Checksum\n",
      "acfedf2a4791: Download complete\n",
      "896ac25d66a9: Verifying Checksum\n",
      "896ac25d66a9: Download complete\n",
      "4bfb0e4cc940: Verifying Checksum\n",
      "4bfb0e4cc940: Download complete\n",
      "17c5cc3b0adc: Verifying Checksum\n",
      "17c5cc3b0adc: Download complete\n",
      "84d12d39fb8e: Download complete\n",
      "b4564e51587f: Verifying Checksum\n",
      "b4564e51587f: Download complete\n",
      "0a5a58fe9f80: Pull complete\n",
      "58e848323dd7: Verifying Checksum\n",
      "58e848323dd7: Download complete\n",
      "bbb32f3289e5: Verifying Checksum\n",
      "bbb32f3289e5: Download complete\n",
      "0904b13f25db: Verifying Checksum\n",
      "0904b13f25db: Download complete\n",
      "043bccf18a62: Verifying Checksum\n",
      "043bccf18a62: Download complete\n",
      "80666faa5792: Verifying Checksum\n",
      "80666faa5792: Download complete\n",
      "d9c940cdfe35: Verifying Checksum\n",
      "d9c940cdfe35: Download complete\n",
      "2a19abfaf29f: Verifying Checksum\n",
      "2a19abfaf29f: Download complete\n",
      "95bcd15ec1e4: Verifying Checksum\n",
      "95bcd15ec1e4: Download complete\n",
      "a4a599db2167: Verifying Checksum\n",
      "a4a599db2167: Download complete\n",
      "588b6420e089: Download complete\n",
      "5a0ee7acb591: Verifying Checksum\n",
      "5a0ee7acb591: Download complete\n",
      "fa711c310b5e: Verifying Checksum\n",
      "fa711c310b5e: Download complete\n",
      "f54dae1543fd: Verifying Checksum\n",
      "f54dae1543fd: Download complete\n",
      "ffe911d7d638: Pull complete\n",
      "6f306638f1f4: Verifying Checksum\n",
      "6f306638f1f4: Download complete\n",
      "b6ca3d91503f: Pull complete\n",
      "320e5792a2cc: Verifying Checksum\n",
      "320e5792a2cc: Download complete\n",
      "dd7888d57680: Verifying Checksum\n",
      "dd7888d57680: Download complete\n",
      "db3ba3497665: Verifying Checksum\n",
      "db3ba3497665: Download complete\n",
      "b4f737bb955c: Verifying Checksum\n",
      "b4f737bb955c: Download complete\n",
      "5fd3800cea87: Verifying Checksum\n",
      "5fd3800cea87: Download complete\n",
      "e98147207b7d: Verifying Checksum\n",
      "e98147207b7d: Download complete\n",
      "2153f05ef7a8: Pull complete\n",
      "b9a3e29ab76c: Verifying Checksum\n",
      "b9a3e29ab76c: Download complete\n",
      "01f09199ca12: Verifying Checksum\n",
      "01f09199ca12: Download complete\n",
      "960ad9fc339d: Pull complete\n",
      "56c8234f1ff1: Download complete\n",
      "954cbb03adfc: Verifying Checksum\n",
      "954cbb03adfc: Download complete\n",
      "810f62c117c7: Verifying Checksum\n",
      "810f62c117c7: Download complete\n",
      "6926159838dd: Verifying Checksum\n",
      "6926159838dd: Download complete\n",
      "f05cdbdb7a3a: Verifying Checksum\n",
      "f05cdbdb7a3a: Download complete\n",
      "7f7e35ba03e8: Download complete\n",
      "5a06c1826b9b: Verifying Checksum\n",
      "5a06c1826b9b: Download complete\n",
      "f4804d3ccc99: Verifying Checksum\n",
      "f4804d3ccc99: Download complete\n",
      "62c207e6773c: Verifying Checksum\n",
      "62c207e6773c: Download complete\n",
      "423cea99f7d2: Pull complete\n",
      "5962889b0c76: Verifying Checksum\n",
      "5962889b0c76: Download complete\n",
      "425009f61737: Verifying Checksum\n",
      "425009f61737: Download complete\n",
      "cc4eb6f55aa9: Verifying Checksum\n",
      "cc4eb6f55aa9: Download complete\n",
      "2f1a08e2c4d0: Verifying Checksum\n",
      "2f1a08e2c4d0: Download complete\n",
      "7cc9ab06c351: Verifying Checksum\n",
      "7cc9ab06c351: Download complete\n",
      "fa4aa2818f5f: Verifying Checksum\n",
      "fa4aa2818f5f: Download complete\n",
      "ea4474a67b49: Pull complete\n",
      "b397fb86b3bc: Pull complete\n",
      "7a50c2cfea6b: Download complete\n",
      "0d1a267e9d99: Download complete\n",
      "a0ba993bb711: Verifying Checksum\n",
      "a0ba993bb711: Download complete\n",
      "08dd4105bc4d: Verifying Checksum\n",
      "08dd4105bc4d: Download complete\n",
      "927627363759: Pull complete\n",
      "41184579c3b7: Verifying Checksum\n",
      "41184579c3b7: Download complete\n",
      "0c0c0dd6e1be: Verifying Checksum\n",
      "0c0c0dd6e1be: Download complete\n",
      "312d920fedcc: Pull complete\n",
      "82e1ac2422cd: Verifying Checksum\n",
      "82e1ac2422cd: Download complete\n",
      "a2412c74818c: Verifying Checksum\n",
      "a2412c74818c: Download complete\n",
      "7a50c2cfea6b: Pull complete\n",
      "06daa3d0f46c: Pull complete\n",
      "aec116761f98: Pull complete\n",
      "2812dcb9b1f1: Pull complete\n",
      "0b40e0143bfd: Pull complete\n",
      "0617ebfc4a21: Pull complete\n",
      "45a7802e347f: Pull complete\n",
      "9c3aac051d0f: Pull complete\n",
      "c06cf176a9ca: Pull complete\n",
      "6790e5e29cc3: Pull complete\n",
      "896ac25d66a9: Pull complete\n",
      "6d2526d2cea7: Pull complete\n",
      "acfedf2a4791: Pull complete\n",
      "17c5cc3b0adc: Pull complete\n",
      "4bfb0e4cc940: Pull complete\n",
      "58e848323dd7: Pull complete\n",
      "84d12d39fb8e: Pull complete\n",
      "b4564e51587f: Pull complete\n",
      "bbb32f3289e5: Pull complete\n",
      "0904b13f25db: Pull complete\n",
      "043bccf18a62: Pull complete\n",
      "80666faa5792: Pull complete\n",
      "d9c940cdfe35: Pull complete\n",
      "2a19abfaf29f: Pull complete\n",
      "6f306638f1f4: Pull complete\n",
      "95bcd15ec1e4: Pull complete\n",
      "a4a599db2167: Pull complete\n",
      "588b6420e089: Pull complete\n",
      "5a0ee7acb591: Pull complete\n",
      "fa711c310b5e: Pull complete\n",
      "f54dae1543fd: Pull complete\n",
      "01f09199ca12: Pull complete\n",
      "320e5792a2cc: Pull complete\n",
      "dd7888d57680: Pull complete\n",
      "db3ba3497665: Pull complete\n",
      "b4f737bb955c: Pull complete\n",
      "5fd3800cea87: Pull complete\n",
      "e98147207b7d: Pull complete\n",
      "b9a3e29ab76c: Pull complete\n",
      "7f7e35ba03e8: Pull complete\n",
      "56c8234f1ff1: Pull complete\n",
      "954cbb03adfc: Pull complete\n",
      "810f62c117c7: Pull complete\n",
      "6926159838dd: Pull complete\n",
      "f05cdbdb7a3a: Pull complete\n",
      "5a06c1826b9b: Pull complete\n",
      "a2412c74818c: Pull complete\n",
      "f4804d3ccc99: Pull complete\n",
      "62c207e6773c: Pull complete\n",
      "5962889b0c76: Pull complete\n",
      "425009f61737: Pull complete\n",
      "cc4eb6f55aa9: Pull complete\n",
      "2f1a08e2c4d0: Pull complete\n",
      "7cc9ab06c351: Pull complete\n",
      "fa4aa2818f5f: Pull complete\n",
      "82e1ac2422cd: Pull complete\n",
      "0d1a267e9d99: Pull complete\n",
      "a0ba993bb711: Pull complete\n",
      "08dd4105bc4d: Pull complete\n",
      "41184579c3b7: Pull complete\n",
      "0c0c0dd6e1be: Pull complete\n",
      "Digest: sha256:100d13339bf00f469074224cd214907cdfb22f2718eb52d1290144541cdfe0a0\n",
      "Status: Downloaded newer image for gcr.io/merlin-on-gcp/dongm-merlin-inference-hugectr:latest\n",
      " ---> 363e54b006c3\n",
      "Step 2/9 : EXPOSE 8000\n",
      " ---> Running in edaed2f22d49\n",
      "Removing intermediate container edaed2f22d49\n",
      " ---> 78f4fbe99c43\n",
      "Step 3/9 : EXPOSE 8001\n",
      " ---> Running in 75a287a75379\n",
      "Removing intermediate container 75a287a75379\n",
      " ---> 4b57b02a4572\n",
      "Step 4/9 : EXPOSE 8002\n",
      " ---> Running in 72a5399da4c4\n",
      "Removing intermediate container 72a5399da4c4\n",
      " ---> c76da31907de\n",
      "Step 5/9 : RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
      " ---> Running in 8a984f7c24bf\n",
      "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    T\u001b[0m\u001b[91mime     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2537  100  2537    0     0  70472      0 --:--:-- --:--:-- --:--:-- 70472:--     0\u001b[0m\u001b[91m\n",
      "\u001b[0m\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mOK\n",
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Release\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk InRelease [6739 B]\n",
      "Hit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:8 https://apt.kitware.com/ubuntu focal InRelease [11.0 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:10 http://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [194 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [101 kB]\n",
      "Get:12 https://apt.kitware.com/ubuntu focal/main amd64 Packages [37.1 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1636 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1085 kB]\n",
      "Fetched 3298 kB in 2s (1509 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  python3-crcmod\n",
      "Suggested packages:\n",
      "  google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-python\n",
      "  google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator\n",
      "  google-cloud-sdk-datastore-emulator kubectl\n",
      "The following NEW packages will be installed:\n",
      "  google-cloud-sdk python3-crcmod\n",
      "0 upgraded, 2 newly installed, 0 to remove and 8 not upgraded.\n",
      "Need to get 75.8 MB of archives.\n",
      "After this operation, 481 MB of additional disk space will be used.\n",
      "Get:1 http://packages.cloud.google.com/apt cloud-sdk/main amd64 google-cloud-sdk all 362.0.0-0 [75.7 MB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 python3-crcmod amd64 1.7+dfsg-2build2 [18.8 kB]\n",
      "Fetched 75.8 MB in 1s (74.9 MB/s)\n",
      "Selecting previously unselected package python3-crcmod.\n",
      "(Reading database ... 47558 files and directories currently installed.)\n",
      "Preparing to unpack .../python3-crcmod_1.7+dfsg-2build2_amd64.deb ...\n",
      "Unpacking python3-crcmod (1.7+dfsg-2build2) ...\n",
      "Selecting previously unselected package google-cloud-sdk.\n",
      "Preparing to unpack .../google-cloud-sdk_362.0.0-0_all.deb ...\n",
      "Unpacking google-cloud-sdk (362.0.0-0) ...\n",
      "Setting up python3-crcmod (1.7+dfsg-2build2) ...\n",
      "Setting up google-cloud-sdk (362.0.0-0) ...\n",
      "Removing intermediate container 8a984f7c24bf\n",
      " ---> a62ce2b9521f\n",
      "Step 6/9 : WORKDIR /src\n",
      " ---> Running in 6de254377c42\n",
      "Removing intermediate container 6de254377c42\n",
      " ---> dbea189cb491\n",
      "Step 7/9 : COPY serving/entrypoint.sh ./\n",
      " ---> dd4dea0f2485\n",
      "Step 8/9 : RUN chmod +x entrypoint.sh\n",
      " ---> Running in 782e08850601\n",
      "Removing intermediate container 782e08850601\n",
      " ---> 859d482e2046\n",
      "Step 9/9 : ENTRYPOINT [\"./entrypoint.sh\"]\n",
      " ---> Running in 93aad0124bf8\n",
      "Removing intermediate container 93aad0124bf8\n",
      " ---> 7df4e40311e0\n",
      "Successfully built 7df4e40311e0\n",
      "Successfully tagged gcr.io/jk-mlops-dev/triton-deploy-hugectr:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-mlops-dev/triton-deploy-hugectr\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/triton-deploy-hugectr]\n",
      "1e567fc2489c: Preparing\n",
      "d71d88584752: Preparing\n",
      "6221b2678641: Preparing\n",
      "75df2229c228: Preparing\n",
      "3ce2ac71b3c1: Preparing\n",
      "a33767c33d91: Preparing\n",
      "0681b1f0f245: Preparing\n",
      "e8438e9e2007: Preparing\n",
      "b3cdece61ac5: Preparing\n",
      "5e967d546b4d: Preparing\n",
      "9d230c808dc6: Preparing\n",
      "1f3eb77d215c: Preparing\n",
      "56ef7b2ce960: Preparing\n",
      "3cd92dff0914: Preparing\n",
      "d8e19c3cfae8: Preparing\n",
      "0565e452465e: Preparing\n",
      "b51ef398a13a: Preparing\n",
      "54dd12864152: Preparing\n",
      "10f0d6f62932: Preparing\n",
      "fd530c4f7455: Preparing\n",
      "36ba3d52bc90: Preparing\n",
      "e766eb2e6212: Preparing\n",
      "d90b409bd299: Preparing\n",
      "d14ac3a9c479: Preparing\n",
      "e5334449bce7: Preparing\n",
      "f126b9815719: Preparing\n",
      "2620f7668dfa: Preparing\n",
      "d45e3faa0a87: Preparing\n",
      "7b541f8976f9: Preparing\n",
      "e3c5956c9625: Preparing\n",
      "3995468f76ac: Preparing\n",
      "a3f2aec045ec: Preparing\n",
      "8d0cfe64a340: Preparing\n",
      "ac30c9682363: Preparing\n",
      "0e9d4363d3f5: Preparing\n",
      "0711ba34b762: Preparing\n",
      "536f82eb28ad: Preparing\n",
      "db4eb6cf452c: Preparing\n",
      "5997a0974e26: Preparing\n",
      "b62202d06e00: Preparing\n",
      "6837ffee73bf: Preparing\n",
      "ac67f734bb8d: Preparing\n",
      "c80b58351969: Preparing\n",
      "70f39615fb82: Preparing\n",
      "0209e2deb656: Preparing\n",
      "d04aa8b8997a: Preparing\n",
      "2620f7668dfa: Waiting\n",
      "106a74485afe: Preparing\n",
      "b113f60db0af: Preparing\n",
      "d45e3faa0a87: Waiting\n",
      "1ae133f14a6c: Preparing\n",
      "f3c214cf223b: Preparing\n",
      "7b541f8976f9: Waiting\n",
      "0e9d4363d3f5: Waiting\n",
      "0ea50bfd6c33: Preparing\n",
      "e3c5956c9625: Waiting\n",
      "80dfc75e1ed3: Preparing\n",
      "0711ba34b762: Waiting\n",
      "3995468f76ac: Waiting\n",
      "9f69d8b563c8: Preparing\n",
      "536f82eb28ad: Waiting\n",
      "a3f2aec045ec: Waiting\n",
      "db4eb6cf452c: Waiting\n",
      "33f10a67d2ae: Preparing\n",
      "5997a0974e26: Waiting\n",
      "ac67f734bb8d: Waiting\n",
      "8d0cfe64a340: Waiting\n",
      "3cd92dff0914: Waiting\n",
      "45c6aceb9bfb: Preparing\n",
      "ac30c9682363: Waiting\n",
      "b7871e54a044: Preparing\n",
      "c80b58351969: Waiting\n",
      "b62202d06e00: Waiting\n",
      "d8e19c3cfae8: Waiting\n",
      "6837ffee73bf: Waiting\n",
      "70f39615fb82: Waiting\n",
      "18690565e7e8: Preparing\n",
      "d5a8e494ae7a: Preparing\n",
      "0565e452465e: Waiting\n",
      "a33767c33d91: Waiting\n",
      "0209e2deb656: Waiting\n",
      "c695310163ba: Preparing\n",
      "16fe0630f74c: Preparing\n",
      "901c8940db67: Preparing\n",
      "b51ef398a13a: Waiting\n",
      "2bf4f7a42374: Preparing\n",
      "5e967d546b4d: Waiting\n",
      "9d230c808dc6: Waiting\n",
      "82ef1083a881: Preparing\n",
      "5fc8a72a640c: Preparing\n",
      "54dd12864152: Waiting\n",
      "1f3eb77d215c: Waiting\n",
      "56ef7b2ce960: Waiting\n",
      "f1de1b7eef1f: Preparing\n",
      "10f0d6f62932: Waiting\n",
      "b85730dafc1f: Preparing\n",
      "e8438e9e2007: Waiting\n",
      "d14ac3a9c479: Waiting\n",
      "ae70e300bdb5: Preparing\n",
      "fd530c4f7455: Waiting\n",
      "1b548163cd10: Preparing\n",
      "36ba3d52bc90: Waiting\n",
      "f0f7e28d05f2: Preparing\n",
      "e5334449bce7: Waiting\n",
      "a33b6bbde9d4: Preparing\n",
      "d04aa8b8997a: Waiting\n",
      "e766eb2e6212: Waiting\n",
      "d6b42408a5cf: Preparing\n",
      "80dfc75e1ed3: Waiting\n",
      "106a74485afe: Waiting\n",
      "d90b409bd299: Waiting\n",
      "422a68779176: Preparing\n",
      "9f69d8b563c8: Waiting\n",
      "18690565e7e8: Waiting\n",
      "b113f60db0af: Waiting\n",
      "f1de1b7eef1f: Waiting\n",
      "33f10a67d2ae: Waiting\n",
      "d5a8e494ae7a: Waiting\n",
      "b85730dafc1f: Waiting\n",
      "1ae133f14a6c: Waiting\n",
      "45c6aceb9bfb: Waiting\n",
      "ae70e300bdb5: Waiting\n",
      "c695310163ba: Waiting\n",
      "b7871e54a044: Waiting\n",
      "f3c214cf223b: Waiting\n",
      "1b548163cd10: Waiting\n",
      "16fe0630f74c: Waiting\n",
      "f0f7e28d05f2: Waiting\n",
      "901c8940db67: Waiting\n",
      "0ea50bfd6c33: Waiting\n",
      "9ace6acda497: Preparing\n",
      "2bf4f7a42374: Waiting\n",
      "422a68779176: Waiting\n",
      "d6b42408a5cf: Waiting\n",
      "82ef1083a881: Waiting\n",
      "5fc8a72a640c: Waiting\n",
      "b3cdece61ac5: Waiting\n",
      "41c8f8f9279b: Preparing\n",
      "aa52050c370d: Preparing\n",
      "da55b45d310b: Preparing\n",
      "9ace6acda497: Waiting\n",
      "aa52050c370d: Waiting\n",
      "41c8f8f9279b: Waiting\n",
      "da55b45d310b: Waiting\n",
      "3ce2ac71b3c1: Layer already exists\n",
      "a33767c33d91: Layer already exists\n",
      "0681b1f0f245: Layer already exists\n",
      "e8438e9e2007: Layer already exists\n",
      "b3cdece61ac5: Layer already exists\n",
      "5e967d546b4d: Layer already exists\n",
      "9d230c808dc6: Layer already exists\n",
      "1f3eb77d215c: Layer already exists\n",
      "56ef7b2ce960: Layer already exists\n",
      "3cd92dff0914: Layer already exists\n",
      "d8e19c3cfae8: Layer already exists\n",
      "0565e452465e: Layer already exists\n",
      "b51ef398a13a: Layer already exists\n",
      "54dd12864152: Layer already exists\n",
      "10f0d6f62932: Layer already exists\n",
      "fd530c4f7455: Layer already exists\n",
      "36ba3d52bc90: Layer already exists\n",
      "1e567fc2489c: Pushed\n",
      "d71d88584752: Pushed\n",
      "e766eb2e6212: Layer already exists\n",
      "d90b409bd299: Layer already exists\n",
      "d14ac3a9c479: Layer already exists\n",
      "6221b2678641: Pushed\n",
      "e5334449bce7: Layer already exists\n",
      "f126b9815719: Layer already exists\n",
      "2620f7668dfa: Layer already exists\n",
      "7b541f8976f9: Layer already exists\n",
      "d45e3faa0a87: Layer already exists\n",
      "e3c5956c9625: Layer already exists\n",
      "3995468f76ac: Layer already exists\n",
      "a3f2aec045ec: Layer already exists\n",
      "8d0cfe64a340: Layer already exists\n",
      "ac30c9682363: Layer already exists\n",
      "536f82eb28ad: Layer already exists\n",
      "0711ba34b762: Layer already exists\n",
      "db4eb6cf452c: Layer already exists\n",
      "0e9d4363d3f5: Layer already exists\n",
      "b62202d06e00: Layer already exists\n",
      "5997a0974e26: Layer already exists\n",
      "6837ffee73bf: Layer already exists\n",
      "ac67f734bb8d: Layer already exists\n",
      "c80b58351969: Layer already exists\n",
      "0209e2deb656: Layer already exists\n",
      "70f39615fb82: Layer already exists\n",
      "d04aa8b8997a: Layer already exists\n",
      "106a74485afe: Layer already exists\n",
      "b113f60db0af: Layer already exists\n",
      "f3c214cf223b: Layer already exists\n",
      "1ae133f14a6c: Layer already exists\n",
      "0ea50bfd6c33: Layer already exists\n",
      "80dfc75e1ed3: Layer already exists\n",
      "9f69d8b563c8: Layer already exists\n",
      "33f10a67d2ae: Layer already exists\n",
      "45c6aceb9bfb: Layer already exists\n",
      "b7871e54a044: Layer already exists\n",
      "d5a8e494ae7a: Layer already exists\n",
      "18690565e7e8: Layer already exists\n",
      "16fe0630f74c: Layer already exists\n",
      "c695310163ba: Layer already exists\n",
      "901c8940db67: Layer already exists\n",
      "2bf4f7a42374: Layer already exists\n",
      "82ef1083a881: Layer already exists\n",
      "b85730dafc1f: Layer already exists\n",
      "5fc8a72a640c: Layer already exists\n",
      "f1de1b7eef1f: Layer already exists\n",
      "ae70e300bdb5: Layer already exists\n",
      "a33b6bbde9d4: Layer already exists\n",
      "1b548163cd10: Layer already exists\n",
      "f0f7e28d05f2: Layer already exists\n",
      "d6b42408a5cf: Layer already exists\n",
      "9ace6acda497: Layer already exists\n",
      "422a68779176: Layer already exists\n",
      "41c8f8f9279b: Layer already exists\n",
      "aa52050c370d: Layer already exists\n",
      "da55b45d310b: Layer already exists\n",
      "75df2229c228: Pushed\n",
      "latest: digest: sha256:7508b5bb435a50e19fd4124ba16aa83d8606c9b86f49bb15887c4f748323629c size: 16288\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                               STATUS\n",
      "95ac0104-5506-4a60-9ec9-b83ad27eb147  2021-10-30T20:35:22+00:00  6M8S      gs://jk-mlops-dev_cloudbuild/source/1635626122.507069-a442fe96d95b4980b082ca4c90be2877.tgz  gcr.io/jk-mlops-dev/triton-deploy-hugectr (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! cp {DOCKERFILE} src/Dockerfile\n",
    "! gcloud builds submit --timeout \"2h\" --tag {IMAGE_URI} src --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f7ae3-76c4-4bfe-8a80-44d3567b0b6d",
   "metadata": {},
   "source": [
    "## 3. Uploading the model and its metadata to Vertex Models.\n",
    "\n",
    "In the following cell you will register (upload) the ensemble model as a Vertex AI Prediction `Model` resource. \n",
    "\n",
    "Refer to [Use a custom container for prediction guide](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container) for detailed information about creating Vertex AI Prediction `Model` resources.\n",
    "\n",
    "Notice that the value of  `model_repository_path`that was used when exporting the ensemble is passed as a command line parameter to the serving container. The entrypoint script in the container will copy the ensemble artifacts to this location when the container starts. This ensures that the locations of the artifacts in the container's local file system and the paths in the `ps.json` and other configuration files used by Triton match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9e3c2ac-90fb-4175-b185-d2981cbdebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/895222332033/locations/us-central1/models/8315056685056000000/operations/1341226477319880704\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/895222332033/locations/us-central1/models/8315056685056000000\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/895222332033/locations/us-central1/models/8315056685056000000')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projects/895222332033/locations/us-central1/models/8315056685056000000'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_route = \"/v2/health/ready\"\n",
    "predict_route = f\"/v2/models/{MODEL_NAME}_ens/infer\"\n",
    "serving_container_ports = [8000]\n",
    "serving_container_args = [model_repository_path]\n",
    "\n",
    "\n",
    "model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    description=MODEL_DESCRIPTION,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    "    artifact_uri=gcs_ensemble_path,\n",
    "    serving_container_args=serving_container_args,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c572a7c-c5e5-4383-bbd5-ce11b6659945",
   "metadata": {},
   "source": [
    "## 4. Deploying the model to Vertex AI Prediction.\n",
    "\n",
    "Deploying a Vertex AI Prediction `Model` is a two step process. First you create an endpoint that will expose an external interface to clients consuming the model. After the endpoint is ready you can deploy multiple versions of a model to the endpoint.\n",
    "\n",
    "Refer to [Deploy a model using the Vertex AI API guide](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) for more information about the APIs used in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the Vertex Endpoint\n",
    "\n",
    "Before deploying the ensemble model you need to create a Vertex AI Prediction endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/895222332033/locations/us-central1/endpoints/1127316077819199488/operations/3905182040177246208\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/895222332033/locations/us-central1/endpoints/1127316077819199488\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/895222332033/locations/us-central1/endpoints/1127316077819199488')\n"
     ]
    }
   ],
   "source": [
    "endpoint = vertex_ai.Endpoint.create(\n",
    "    display_name=ENDPOINT_DISPLAY_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Deploy the model to Vertex Prediction endpoint\n",
    "\n",
    "After the endpoint is ready, you can deploy your ensemble model to the endpoint. You will run the ensemble on a GPU node equipped with the NVIDIA Tesla T4 GPUs. \n",
    "\n",
    "Refer to [Deploy a model using the Vertex AI API guide](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd27dda-7c1b-4ce5-9477-e62a9f1199bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/895222332033/locations/us-central1/endpoints/1127316077819199488\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/895222332033/locations/us-central1/endpoints/1127316077819199488/operations/4979290551305109504\n"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "## 5. Invoking the model\n",
    "\n",
    "To invoke the ensemble through Vertex AI Prediction endpoint you need to format your request using a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference) or a [Inference Request JSON Object with a binary extension](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md) and submit a request to Vertex AI Prediction [REST rawPredict endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/rawPredict). You need to use the `rawPredict` rather than `predict` endpoint because inference request formats used by Triton are not compatible with the Vertex AI Prediction [standard input format](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#formatting-prediction-input).\n",
    "\n",
    "The below cell shows a sample request body formatted as a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference). The request encapsulates a batch of three records from the Criteo dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "924669cf-5442-4454-b881-e2a792a96f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'id': '1',\n",
    "    'inputs': [\n",
    "        {'name': 'I1','shape': [3, 1], 'datatype': 'INT32', 'data': [5, 32, 0]},\n",
    "        {'name': 'I2', 'shape': [3, 1], 'datatype': 'INT32', 'data': [110, 3, 233]},\n",
    "        {'name': 'I3', 'shape': [3, 1], 'datatype': 'INT32', 'data': [0, 5, 1]},\n",
    "        {'name': 'I4', 'shape': [3, 1], 'datatype': 'INT32', 'data': [16, 0, 146]},\n",
    "        {'name': 'I5', 'shape': [3, 1], 'datatype': 'INT32', 'data': [0, 1, 1]},\n",
    "        {'name': 'I6', 'shape': [3, 1], 'datatype': 'INT32', 'data': [1, 0, 0]},\n",
    "        {'name': 'I7', 'shape': [3, 1], 'datatype': 'INT32', 'data': [0, 0, 0]},\n",
    "        {'name': 'I8', 'shape': [3, 1], 'datatype': 'INT32', 'data': [14, 61, 99]},\n",
    "        {'name': 'I9', 'shape': [3, 1], 'datatype': 'INT32', 'data': [7, 5, 7]},\n",
    "        {'name': 'I10', 'shape': [3, 1], 'datatype': 'INT32', 'data': [1, 0, 0]},\n",
    "        {'name': 'I11', 'shape': [3, 1], 'datatype': 'INT32', 'data': [0, 1, 1]},\n",
    "        {'name': 'I12', 'shape': [3, 1], 'datatype': 'INT32', 'data': [306, 3157, 3101]},\n",
    "        {'name': 'I13', 'shape': [3, 1], 'datatype': 'INT32', 'data': [0, 5, 1]},\n",
    "        {'name': 'C1', 'shape': [3, 1], 'datatype': 'INT32', 'data': [1651969401, -436994675, 1651969401]},\n",
    "        {'name': 'C2', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-501260968, -1599406170, -1382530557]},\n",
    "        {'name': 'C3', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1343601617, 1873417685, 1656669709]},\n",
    "        {'name': 'C4', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1805877297, -628476895, 946620910]},\n",
    "        {'name': 'C5', 'shape': [3, 1], 'datatype': 'INT32', 'data': [951068488, 1020698403, -413858227]},\n",
    "        {'name': 'C6', 'shape': [3, 1], 'datatype': 'INT32', 'data': [1875733963, 1875733963, 1875733963]},\n",
    "        {'name': 'C7', 'shape': [3, 1], 'datatype': 'INT32', 'data': [897624609, -1424560767, -1242174622]},\n",
    "        {'name': 'C8', 'shape': [3, 1], 'datatype': 'INT32', 'data': [679512323, 1128426537, -772617077]},\n",
    "        {'name': 'C9', 'shape': [3, 1], 'datatype': 'INT32', 'data': [1189011366, 502653268, 776897055]},\n",
    "        {'name': 'C10', 'shape': [3, 1], 'datatype': 'INT32', 'data': [771915201, 2112471209, 771915201]},\n",
    "        {'name': 'C11', 'shape': [3, 1], 'datatype': 'INT32', 'data': [209470001, 1716706404, 209470001]},\n",
    "        {'name': 'C12', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1785193185, -1712632281, 309420420]},\n",
    "        {'name': 'C13', 'shape': [3, 1], 'datatype': 'INT32', 'data': [12976055, 12976055, 12976055]},\n",
    "        {'name': 'C14', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1102125769, -1102125769, -1102125769]},\n",
    "        {'name': 'C15', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1978960692, -205783399, -150008565]},\n",
    "        {'name': 'C16', 'shape': [3, 1], 'datatype': 'INT32', 'data': [1289502458, 1289502458, 1289502458]},\n",
    "        {'name': 'C17', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-771205462, -771205462, -771205462]},\n",
    "        {'name': 'C18', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1206449222, -1578429167, 1653545869]},\n",
    "        {'name': 'C19', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1793932789, -1793932789, -1793932789]},\n",
    "        {'name': 'C20', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-1014091992, -20981661, -1014091992]},\n",
    "        {'name': 'C21', 'shape': [3, 1], 'datatype': 'INT32', 'data': [351689309, -1556988767, 351689309]},\n",
    "        {'name': 'C22', 'shape': [3, 1], 'datatype': 'INT32', 'data': [632402057, -924717482, 632402057]},\n",
    "        {'name': 'C23', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-675152885, 391309800, -675152885]},\n",
    "        {'name': 'C24', 'shape': [3, 1], 'datatype': 'INT32', 'data': [2091868316, 1966410890, 883538181]},\n",
    "        {'name': 'C25', 'shape': [3, 1], 'datatype': 'INT32', 'data': [809724924, -1726799382, -10139646]},\n",
    "        {'name': 'C26', 'shape': [3, 1], 'datatype': 'INT32', 'data': [-317696227, -1218975401, -317696227]}]\n",
    "}\n",
    "\n",
    "with open('criteo_payload.json', 'w') as f:\n",
    "    json.dump(payload, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27197c74-94c7-454f-a469-151c680de97b",
   "metadata": {},
   "source": [
    "You can invoke the Vertex AI Prediction `rawPredict` endpoint using any HTTP tool or library, including `curl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f98d37f-87bf-47d8-951e-a88dee48421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"1\",\"model_name\":\"deepfm_ens\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[3],\"data\":[0.07325490564107895,0.04518262296915054,0.04656011983752251]}]}"
     ]
    }
   ],
   "source": [
    "uri = f'https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:rawPredict'\n",
    "\n",
    "! curl -X POST \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\"  \\\n",
    "{uri} \\\n",
    "-d @criteo_payload.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf2954-7807-4282-8143-ba4c314bf632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m82",
   "type": "gcloud",
   "uri": "gcr.io/jk-mlops-dev/merlin-dev:latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
