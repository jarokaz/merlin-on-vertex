{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Using Vertex AI for online serving with NVIDIA Triton\n",
    "\n",
    "- This notebooks demonstrates serving of ensemble models - NVTabular preprocessing + HugeCTR recommender on Triton server \n",
    "\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "- Building a custom container derived from NVIDIA NGC Merlin inference image and the model artifacts\n",
    "- Creating Vertex model using the custome container\n",
    "- Creating a Vertex endpoint and deploying the model to that endpoint\n",
    "- Getting the inference on a sample dataset using hte endpoint\n",
    "\n",
    "## Model serving\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\n",
    "Triton can load models from local storage or cloud platforms. As models are retrained with new data, developers can easily make updates without restarting the inference server or disrupting the application.\n",
    "\n",
    "Triton runs multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding.\n",
    "\n",
    "It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensemble for use cases that require multiple models to perform end-to-end inference, such as conversational AI.\n",
    "\n",
    "%Users can also use shared memory. The Inputs and outputs that pass to and from Triton are stored in shared memory, reducing HTTP/gRPC overhead and increasing performance.\n",
    "\n",
    "The following figure shows the Triton Inference Server high-level architecture. The model repository is a file-system based repository of the models that Triton will make available for inferencing. Inference requests arrive at the server via either HTTP/REST or GRPC or by the C API and are then routed to the appropriate per-model scheduler. Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis. Each model's scheduler optionally performs batching of inference requests and then passes the requests to the backend corresponding to the model type. The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs. The outputs are then returned.\n",
    "\n",
    "<img src=\"./images/triton-architecture.png\" alt=\"Triton Architecture\" />\n",
    "\n",
    "Triton supports a backend C API that allows Triton to be extended with new functionality such as custom pre- and post-processing operations or even a new deep-learning framework.\n",
    "\n",
    "The models being served by Triton can be queried and controlled by a dedicated model management API that is available by HTTP/REST or GRPC protocol, or by the C API.\n",
    "\n",
    "Readiness and liveness health endpoints and utilization, throughput and latency metrics ease the integration of Triton into deployment framework such as Kubernetes.\n",
    "\n",
    "Here we use Triton to serve an ensemble model that contains data processing operations using NVTabular and HugeCTR model trained on Criteo data. The model is deployed into Google's Vertex AI and served via a Vertex Endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## Notebook flow\n",
    "\n",
    "This notebook assumes that the emsemble model containg the Hugectr trained model asn the NVTabular preprocessed wrokflow is created using ... notebook.\n",
    "\n",
    "As you walk through the notebook you will execute the following steps:\n",
    "- Configure notebook environment settings like GCP project and compute region.\n",
    "- Build a custom Vertex container based on NVIDIA NGC Merlin Inference container\n",
    "- Configure and submit the model based on the custom container \n",
    "- Create the endoint\n",
    "- Configure the deployment of the model and submit the deployment job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6a68a-ed54-4810-9e2b-85972ae24e06",
   "metadata": {},
   "source": [
    "## Configure notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp'\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"gs://workshop-datasets/merlin/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9c710-537a-49c9-86d5-fae7e9f2b4be",
   "metadata": {},
   "source": [
    "## Create the ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7cb7eb-320e-4565-b78b-072358dedf86",
   "metadata": {},
   "source": [
    "### Copying the nvtabular workflow created in step 1 artifact to a temporary local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a0a6e6-5b49-46e1-9520-c27d5569b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C1.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C10.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C11.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C12.parquet...\n",
      "| [4 files][131.4 MiB/131.4 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C13.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C14.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C15.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C16.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C17.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C18.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C19.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C2.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C20.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C21.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C22.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C23.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C24.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C25.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C26.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C3.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C4.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C5.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C6.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C7.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C8.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/categories/unique.C9.parquet...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/metadata.json...\n",
      "Copying gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/workflow.pkl...\n",
      "- [28 files][301.7 MiB/301.7 MiB]                                               \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 28 objects/301.7 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://workshop-datasets/merlin/criteo_processed_parquet_0.6/workflow/ ./src/tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f60b9-743b-44c8-a2c3-6293b597d71e",
   "metadata": {},
   "source": [
    "### Copying the hugectr model artifact created in step 2 to a temporary local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76abc820-3603-4b8b-9dec-a22c07d31d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://workshop-datasets/merlin/model_21.09/deepfm.json...\n",
      "Copying gs://workshop-datasets/merlin/model_21.09/deepfm0_opt_sparse_0.model... \n",
      "Copying gs://workshop-datasets/merlin/model_21.09/deepfm0_sparse_0.model/emb_vector...\n",
      "==> NOTE: You are downloading one or more large file(s), which would            \n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "Copying gs://workshop-datasets/merlin/model_21.09/deepfm0_sparse_0.model/key... \n",
      "\\ [4 files][  3.7 GiB/  3.7 GiB]   18.1 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://workshop-datasets/merlin/model_21.09/deepfm0_sparse_0.model/slot_id...\n",
      "Copying gs://workshop-datasets/merlin/model_21.09/deepfm_dense_0.model...       \n",
      "Copying gs://workshop-datasets/merlin/model_21.09/deepfm_opt_dense_0.model...   \n",
      "/ [7 files][  4.2 GiB/  4.2 GiB]   15.9 MiB/s                                   \n",
      "Operation completed over 7 objects/4.2 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://workshop-datasets/merlin/model_21.09/ ./src/tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f1d-2e9b-4035-b982-d4c289e67dec",
   "metadata": {},
   "source": [
    "### Exporting the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7a6f4c-f009-489c-8e39-00400986a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.serving import export\n",
    "\n",
    "# workflow_path = \"./src/tmp/criteo_processed_parquet_0.6/workflow/\"\n",
    "# saved_model_path = \"./src/tmp/model_21.09\"\n",
    "# output_path = \"./src/tmp/models\"\n",
    "# label_columns=[\"label\"],\n",
    "# categotical_columns=[\"C\" + str(x) for x in range(1, 27)],\n",
    "# continuous_columns=[\"I\" + str(x) for x in range(1, 14)]\n",
    "\n",
    "\n",
    "# export.export_ensemble( workflow_path, saved_model_path, output_path, categotical_columns, continuous_columns, label_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d2a9f-312e-4e53-86cf-ab4bb9129365",
   "metadata": {},
   "source": [
    "### Copying the ensemble model to the ensemble model location on gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23f6fb2-c38b-4de9-b97e-9a3108c926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ensemble_location = f\"{BUCKET_NAME}/models/\"\n",
    "#!gsutil cp -r ${output_path} ${model_ensemble_location}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f5255-c846-4eee-b0f3-bea76e3b8030",
   "metadata": {},
   "source": [
    "## Submit a Vertex custom training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71a3ac-10dc-42fc-9789-daf4fb37727e",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3f0679-1102-45a5-a581-90b391396566",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb83ba-e6a2-49aa-8c14-8c1d4f4c6a73",
   "metadata": {},
   "source": [
    "### Build a custom prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa33dc62-9210-41fe-9733-66d4d2c77566",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'triton_deploy-hugectr'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9672b9b-9e73-471b-8600-a30660432be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  10.88GB\n",
      "Step 1/9 : FROM gcr.io/merlin-on-gcp/dongm-merlin-inference-hugectr:v0.6.1\n",
      " ---> fb6f7db2d7fd\n",
      "Step 2/9 : EXPOSE 8000\n",
      " ---> Using cache\n",
      " ---> 6483e4a811d5\n",
      "Step 3/9 : EXPOSE 8001\n",
      " ---> Using cache\n",
      " ---> 36f81f5b7f47\n",
      "Step 4/9 : EXPOSE 8002\n",
      " ---> Using cache\n",
      " ---> 541852b52454\n",
      "Step 5/9 : RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
      " ---> Using cache\n",
      " ---> 0935960c0e99\n",
      "Step 6/9 : WORKDIR /src\n",
      " ---> Using cache\n",
      " ---> f2a6be6fafb9\n",
      "Step 7/9 : COPY serving/entrypoint.sh ./\n",
      " ---> Using cache\n",
      " ---> 0733b8a1e25d\n",
      "Step 8/9 : RUN chmod +x entrypoint.sh\n",
      " ---> Using cache\n",
      " ---> 6b36dbe0f14e\n",
      "Step 9/9 : ENTRYPOINT [\"./entrypoint.sh\"]\n",
      " ---> Using cache\n",
      " ---> b90b7adbe460\n",
      "Successfully built b90b7adbe460\n",
      "Successfully tagged gcr.io/merlin-on-gcp/triton_deploy-hugectr:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/merlin-on-gcp/triton_deploy-hugectr]\n",
      "\n",
      "\u001b[1B6ca0ae9a: Preparing \n",
      "\u001b[1B16f4daba: Preparing \n",
      "\u001b[1Bd5b17ce2: Preparing \n",
      "\u001b[1Bf185cdfb: Preparing \n",
      "\u001b[1Be58e8598: Preparing \n",
      "\u001b[1Bc9824aed: Preparing \n",
      "\u001b[1B23fe2ec9: Preparing \n",
      "\u001b[1Beb0eea4e: Preparing \n",
      "\u001b[1B08de1536: Preparing \n",
      "\u001b[1Bd698577a: Preparing \n",
      "\u001b[1B21cc7b5f: Preparing \n",
      "\u001b[1B646f53eb: Preparing \n",
      "\u001b[1Bf0fe3ed8: Preparing \n",
      "\u001b[1B7d455483: Preparing \n",
      "\u001b[1B026e0fab: Preparing \n",
      "\u001b[1B9b492bd7: Preparing \n",
      "\u001b[1B2784440c: Preparing \n",
      "\u001b[1B2f6421c3: Preparing \n",
      "\u001b[1B8a55152d: Preparing \n",
      "\u001b[1B589ca15a: Preparing \n",
      "\u001b[1B0a50ea64: Preparing \n",
      "\u001b[1B64c9831d: Preparing \n",
      "\u001b[1B42df9148: Preparing \n",
      "\u001b[1B4e7260d1: Preparing \n",
      "\u001b[1B01b2e862: Preparing \n",
      "\u001b[1Bda137864: Preparing \n",
      "\u001b[1B5d96cc97: Preparing \n",
      "\u001b[1B41d8a723: Preparing \n",
      "\u001b[1B8118241f: Preparing \n",
      "\u001b[1B75c0649e: Preparing \n",
      "\u001b[1B357bd24c: Preparing \n",
      "\u001b[1Bb05908c8: Preparing \n",
      "\u001b[1B4217793b: Preparing \n",
      "\u001b[1Bd25bd59b: Preparing \n",
      "\u001b[1B7c006461: Preparing \n",
      "\u001b[1Bd42ad1cc: Preparing \n",
      "\u001b[1B8046cdd7: Preparing \n",
      "\u001b[1B08382770: Preparing \n",
      "\u001b[1B08abd40a: Preparing \n",
      "\u001b[1B325c8271: Preparing \n",
      "\u001b[31B1cc7b5f: Waiting g \n",
      "\u001b[1B6c27cf6e: Preparing \n",
      "\u001b[32B46f53eb: Waiting g \n",
      "\u001b[1Bd199c000: Preparing \n",
      "\u001b[33B0fe3ed8: Waiting g \n",
      "\u001b[17B5c0649e: Waiting g \n",
      "\u001b[1B6a18e033: Preparing \n",
      "\u001b[41Bb0eea4e: Waiting g \n",
      "\u001b[1B428c2607: Preparing \n",
      "\u001b[5Bf46ba1c7: Waiting g \n",
      "\u001b[1Bc3489c5c: Preparing \n",
      "\u001b[1Bd4750603: Preparing \n",
      "\u001b[6Bb0b86b83: Waiting g \n",
      "\u001b[1B2b1df38f: Preparing \n",
      "\u001b[1B9a293c2f: Preparing \n",
      "\u001b[1B60a9947e: Preparing \n",
      "\u001b[9B428c2607: Waiting g \n",
      "\u001b[9Be1f4e2bf: Waiting g \n",
      "\u001b[7B4d6d50a7: Waiting g \n",
      "\u001b[1B98d0b77d: Preparing \n",
      "\u001b[8B2b1df38f: Waiting g \n",
      "\u001b[1Baf4cc8e8: Preparing \n",
      "\u001b[8B60a9947e: Waiting g \n",
      "\u001b[1Bbcfb7e1f: Preparing \n",
      "\u001b[11Ba293c2f: Waiting g \n",
      "\u001b[1Ba057adac: Preparing \n",
      "\u001b[1Ba85aee62: Preparing \n",
      "\u001b[9B98d0b77d: Waiting g \n",
      "\u001b[1B762bfd28: Preparing \n",
      "\u001b[1B316f3ae7: Preparing \n",
      "\u001b[11Bd5c90ce: Waiting g \n",
      "\u001b[11Bf4cc8e8: Waiting g \n",
      "\u001b[1B96ef8c35: Preparing \n",
      "\u001b[1B7a6c588e: Preparing \n",
      "\u001b[11B75e907e: Waiting g \n",
      "\u001b[13Bcfb7e1f: Waiting g \n",
      "\u001b[12B057adac: Waiting g \n",
      "\u001b[1B8f6f0e57: Preparing \n",
      "\u001b[2B8f6f0e57: Layer already exists \u001b[79A\u001b[2K\u001b[74A\u001b[2K\u001b[67A\u001b[2K\u001b[69A\u001b[2K\u001b[66A\u001b[2K\u001b[60A\u001b[2K\u001b[58A\u001b[2K\u001b[56A\u001b[2K\u001b[54A\u001b[2K\u001b[52A\u001b[2K\u001b[46A\u001b[2K\u001b[41A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[21A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:ee533c87ee54f0ce50d0a6830ab5f2329e6b8351777e9a0e1a8070fd6bbe668f size: 16916\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {IMAGE_URI} -f {DOCKERFILE} src\n",
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dead7-e920-4417-b61a-d8a9ccc9c7d7",
   "metadata": {},
   "source": [
    "### Configure a custom prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247c919b-1855-4040-bac2-af1d41fa1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 3\n",
    "model_display_name = f\"{IMAGE_NAME}-deepfm-v{VERSION}\"\n",
    "model_description = \"Serving with Triton inference server using a custom container\"\n",
    "\n",
    "health_route = \"/v2/health/ready\"\n",
    "predict_route = f\"/v2/models/deepfm_ens/infer\"\n",
    "serving_container_ports = [8000]\n",
    "in_container_model_repository = '/models' # this should match the paths in ps.json and config.pbtxt in the ensemble\n",
    "serving_container_args = [in_container_model_repository]\n",
    "\n",
    "model_ensemble_location = f\"{BUCKET_NAME}/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c1220-bf18-4256-90e8-47b125734c4c",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa84abf4-3b8a-4d79-9490-58cc569350bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/659831510405/locations/us-central1/models/7127513758313742336/operations/7054833428377632768\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/659831510405/locations/us-central1/models/7127513758313742336\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/659831510405/locations/us-central1/models/7127513758313742336')\n",
      "triton_deploy-hugectr-deepfm-v3\n",
      "projects/659831510405/locations/us-central1/models/7127513758313742336\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    "    artifact_uri=model_ensemble_location,\n",
    "    serving_container_args=serving_container_args,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/659831510405/locations/us-central1/endpoints/5534369788177940480/operations/8858525079139516416\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/659831510405/locations/us-central1/endpoints/5534369788177940480\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/659831510405/locations/us-central1/endpoints/5534369788177940480')\n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{IMAGE_NAME}-endpoint-v{VERSION}\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Set deployment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16274eb5-feb6-48d4-9fa7-fea87b1da376",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd27dda-7c1b-4ce5-9477-e62a9f1199bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/659831510405/locations/us-central1/endpoints/5534369788177940480\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/659831510405/locations/us-central1/endpoints/5534369788177940480/operations/1685522325761425408\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/659831510405/locations/us-central1/endpoints/5534369788177940480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f66e546bf10> \n",
       "resource name: projects/659831510405/locations/us-central1/endpoints/5534369788177940480"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "### Getting inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aafdee-283d-4377-ac5a-ef92a7de6d38",
   "metadata": {},
   "source": [
    "### Reading CSV data file, getting the input data, and generating the inference request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50394a33-e4b3-4fe8-950f-1cbd76f9b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.serving import inference\n",
    "\n",
    "data = pd.read_csv('gs://diman-criteo/data/criteo.csv', encoding='utf-8', index_col=[0])\n",
    "\n",
    "# Defining whether to set the request_data in json or binary format\n",
    "binary_data = False\n",
    "\n",
    "data = data[[x for x in data.columns if x != \"label\"]].fillna(0)\n",
    "\n",
    "# Converting the data into triton's InferInput object format \n",
    "# The format matches KF Serving V2 protocol\n",
    "inputs = inference.get_inference_input(data, binary_data)\n",
    "\n",
    "# Greating the request_body to be sent to the inference request  \n",
    "if (binary_data):\n",
    "    request_body, json_size = inference.get_inference_request(inputs, '1')\n",
    "    with open('criteo.dat', 'wb') as output_file:\n",
    "        output_file.write(request_body)\n",
    "else:\n",
    "    infer_request, request_body, json_size = inference.get_inference_request(inputs, '1')\n",
    "    json_obj = json.loads(request_body)\n",
    "    with open('criteo.json', 'w') as output_file:\n",
    "        json.dump(json_obj, output_file)\n",
    "         \n",
    "output_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee823b73-0540-4f13-af88-508334004d07",
   "metadata": {},
   "source": [
    "### Getting inference for a json input using curl command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2316dbbd-7dc4-4e93-9b39-6c18537353c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGION = us-central1\n",
      "ENDPOINT DISPLAY NAME = triton_deploy-hugectr-endpoint-v3\n",
      "ENDPOINT_ID = 5534369788177940480\n",
      "{\"id\":\"1\",\"model_name\":\"deepfm_ens\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[3],\"data\":[0.07456179708242417,0.03542599454522133,0.02267574332654476]}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3765    0   261  100  3504    289   3884 --:--:-- --:--:-- --:--:--  4169\n"
     ]
    }
   ],
   "source": [
    "%%bash -s  $PROJECT_ID $REGION $endpoint_display_name\n",
    "\n",
    "PROJECT_ID=$1\n",
    "REGION=$2\n",
    "endpoint_display_name=$3\n",
    "\n",
    "# get endpoint id\n",
    "echo \"REGION = ${REGION}\"\n",
    "echo \"ENDPOINT DISPLAY NAME = ${endpoint_display_name}\"\n",
    "endpoint_id=$(gcloud beta ai endpoints list --region ${REGION} --filter \"display_name=${endpoint_display_name}\" --format \"value(ENDPOINT_ID)\")\n",
    "echo \"ENDPOINT_ID = ${endpoint_id}\"\n",
    "\n",
    "curl -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\"  \\\n",
    "  https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/endpoints/${endpoint_id}:rawPredict \\\n",
    "  -d @criteo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43b51e-f8cd-4bb7-b689-02dfdeed6814",
   "metadata": {},
   "source": [
    "### Getting inference for a binary input using curl command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca3133e-73a3-4fb4-a90e-03450a4f6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently it is not working\n",
    "\n",
    "#Infer-Header-Content-Length = json_size\n",
    "\n",
    "# !curl \\\n",
    "# -X POST https://us-central1-aiplatform.googleapis.com/v1/projects/merlin-on-gcp/locations/us-central1/endpoints/5806274615680434176:rawPredict \\\n",
    "# -k -H \"Content-Type: application/octet-stream\" \\\n",
    "# -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "# -H \"Infer-Header-Content-Length: 3812\" \\\n",
    "# --data-binary \"@criteo.dat\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
