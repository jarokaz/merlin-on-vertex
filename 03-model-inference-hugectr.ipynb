{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Serving Recommender Models for Online Prediction using NVIDIA Triton and Vertex AI\n",
    "\n",
    "This notebooks demonstrates serving a HugeCTR model using Triton server on Vertex AI prediction.\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "1. Exporting the Triton ensemble model consisting of NVTabular preprocessing workflow HugeCTR model.\n",
    "2. Uploading the model and its metadata to Vertex Models.\n",
    "3. Building a custom container derived from NVIDIA NGC Merlin inference image.\n",
    "4. Deploy the model to Vertex AI Prediction.\n",
    "5. Getting the inference on a sample data points using hte endpoint.\n",
    "\n",
    "## Triton Inference Server Overview\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides an inferencing solution optimized for both CPUs and GPUs. Triton can run multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding.It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensemble for use cases that require multiple models to perform end-to-end inference.\n",
    "\n",
    "At a high-level, the Triton Inference Server high-level architecture works as follows:\n",
    "- The model repository is a file-system based repository of the models that Triton will make available for inferencing. \n",
    "- Inference requests arrive at the server via either HTTP/REST or gRPC or then routed to the appropriate per-model scheduler. \n",
    "- Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis.\n",
    "- The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs.\n",
    "\n",
    "Triton server provides readiness and liveness health endpoints, as well as utilization, throughput, and latency metrics, which enables the integration of Triton into deployment environment, such as Vertex AI Prediction.\n",
    "\n",
    "In this example, we use Triton to serve an ensemble model that contains data processing workflow and HugeCTR model trained on Criteo data. The model is deployed into Vertex AI Prediction. This is shown in the following figure:\n",
    "\n",
    "<img src=\"./images/triton-vertex.png\" alt=\"Triton Architecture\" style=\"width:70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a GCS compute region, and a GCP Bucket. \n",
    "You also set the locations of the saved NVTaubular workflow, created in [01-dataset-preprocessing.ipynb](01-dataset-preprocessing.ipynb) and the trained HugeCTR model, created in [02-model-training-hugectr.ipynb](02-model-training-hugectr.ipynb) notebook.\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from src.serving import export\n",
    "from src import feature_utils\n",
    "\n",
    "from google.cloud import aiplatform as vetex_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp' # Change to your project.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "BUCKET = 'merlin-on-gcp' # Change to your bucket.\n",
    "\n",
    "VERSION = 'v01'\n",
    "ENDPOINT_DISPLAY_NAME = 'criteo-merlin-recommender'\n",
    "MODEL_DISPLAY_NAME = f'{ENDPOINT_DISPLAY_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "EXPORTED_MODELS_DIR = os.path.join(WORKSPACE, \"model_registry\")\n",
    "\n",
    "IMAGE_NAME = 'triton_deploy-hugectr'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton'\n",
    "\n",
    "WORKFLOW_MODEL_DIR = \"\" # Change to GCS path of the nvt workflow.\n",
    "HUGECTR_MODEL_DIR = \"\" # Change to GCS path of the hugectr trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e3f45-2bab-48fb-9b81-4e0e65635ece",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0cca3-11e6-4454-af41-c5c632dbbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'stg') \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9c710-537a-49c9-86d5-fae7e9f2b4be",
   "metadata": {},
   "source": [
    "## 1. Exporting the Triton ensemble model\n",
    "\n",
    "The Triton ensemble model consists of NVTabular preprocessing workflow HugeCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8f2b3-bb59-49e9-a727-4c3953add12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r {WORKFLOW_MODEL_DIR} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a66224-acc2-4d7c-b4b0-ca256acc4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r {HUGECTR_MODEL_DIR} ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f1d-2e9b-4035-b982-d4c289e67dec",
   "metadata": {},
   "source": [
    "### Exporting the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76abc820-3603-4b8b-9dec-a22c07d31d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "categotical_columns = feature_utils.categotical_columns()\n",
    "continuous_columns = feature_utils.continuous_columns()\n",
    "label_columns = feature_utils.label_columns()\n",
    "\n",
    "local_workflow_path = Path(WORKFLOW_MODEL_DIR).parts[-1]\n",
    "local_saved_model_path = Path(HUGECTR_MODEL_DIR).parts[-1]\n",
    "local_exported_ensemble_path = f'triton-ensemble-{time.strftime(\"%Y%m%d%H%M%S\")}'\n",
    "\n",
    "export.export_ensemble(\n",
    "    workflow_path=local_workflow_path,\n",
    "    saved_model_path=local_saved_model_path,\n",
    "    output_path=local_exported_ensemble_path,\n",
    "    categotical_columns,\n",
    "    continuous_columns,\n",
    "    label_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e252c2a-8fd8-4962-a9f1-d2682cb3e718",
   "metadata": {},
   "source": [
    "## 2. Uploading the model and its metadata to Vertex Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f6fb2-c38b-4de9-b97e-9a3108c926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r {local_exported_ensemble_path} {EXPORTED_MODELS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce6460-4132-43ef-8764-8a8111e7263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_route = \"/v2/health/ready\"\n",
    "predict_route = \"/v2/models/deepfm_ens/infer\"\n",
    "serving_container_ports = [8000]\n",
    "in_container_model_repository = '/models'\n",
    "serving_container_args = [in_container_model_repository]\n",
    "\n",
    "model_ensemble_location = os.path.join(EXPORTED_MODELS_DIR, local_exported_ensemble_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3c2ac-90fb-4175-b185-d2981cbdebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vertex_ai.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    "    artifact_uri=model_ensemble_location,\n",
    "    serving_container_args=serving_container_args,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c89562-ea7e-4ad0-a601-7a504002f1ac",
   "metadata": {},
   "source": [
    "## 3. Building a custom container derived from NVIDIA NGC Merlin inference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9672b9b-9e73-471b-8600-a30660432be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --timeout \"2h\" --tag {IMAGE_URI} {DOCKERFILE} --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c572a7c-c5e5-4383-bbd5-ce11b6659945",
   "metadata": {},
   "source": [
    "## 4. Deploy the model to Vertex AI Prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the Vertex Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.Endpoint.create(\n",
    "    display_name=ENDPOINT_DISPLAY_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Deploy the model to Vertex Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd27dda-7c1b-4ce5-9477-e62a9f1199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "## 5. Getting the inference on a sample data points using hte endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50394a33-e4b3-4fe8-950f-1cbd76f9b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.serving import inference\n",
    "\n",
    "data = pd.read_csv('gs://diman-criteo/data/criteo.csv', encoding='utf-8', index_col=[0])\n",
    "\n",
    "# Defining whether to set the request_data in json or binary format\n",
    "binary_data = False\n",
    "\n",
    "data = data[[x for x in data.columns if x != \"label\"]].fillna(0)\n",
    "\n",
    "# Converting the data into triton's InferInput object format \n",
    "# The format matches KF Serving V2 protocol\n",
    "inputs = inference.get_inference_input(data, binary_data)\n",
    "\n",
    "# Greating the request_body to be sent to the inference request  \n",
    "if (binary_data):\n",
    "    request_body, json_size = inference.get_inference_request(inputs, '1')\n",
    "    with open('criteo.dat', 'wb') as output_file:\n",
    "        output_file.write(request_body)\n",
    "else:\n",
    "    infer_request, request_body, json_size = inference.get_inference_request(inputs, '1')\n",
    "    json_obj = json.loads(request_body)\n",
    "    with open('criteo.json', 'w') as output_file:\n",
    "        json.dump(json_obj, output_file)\n",
    "         \n",
    "output_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee823b73-0540-4f13-af88-508334004d07",
   "metadata": {},
   "source": [
    "### Getting inference for a json input using curl command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316dbbd-7dc4-4e93-9b39-6c18537353c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s  $PROJECT_ID $REGION $ENDPOINT_DISPLAY_NAME\n",
    "\n",
    "PROJECT_ID=$1\n",
    "REGION=$2\n",
    "endpoint_display_name=$3\n",
    "\n",
    "# get endpoint id\n",
    "echo \"REGION = ${REGION}\"\n",
    "echo \"ENDPOINT DISPLAY NAME = ${ENDPOINT_DISPLAY_NAME}\"\n",
    "ENDPOINT_ID=$(gcloud beta ai endpoints list --region ${REGION} --filter \"display_name=${ENDPOINT_DISPLAY_NAME}\" --format \"value(ENDPOINT_ID)\")\n",
    "echo \"ENDPOINT_ID = ${ENDPOINT_ID}\"\n",
    "\n",
    "curl -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\"  \\\n",
    "  https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/endpoints/${ENDPOINT_ID}:rawPredict \\\n",
    "  -d @criteo.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
