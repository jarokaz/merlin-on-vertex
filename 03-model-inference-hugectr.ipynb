{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Serving Recommender Models for Online Prediction using NVIDIA Triton and Vertex AI\n",
    "\n",
    "This notebooks demonstrates serving a HugeCTR model using Triton server on Vertex AI prediction.\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "1. Exporting the Triton ensemble model consisting of NVTabular preprocessing workflow HugeCTR model.\n",
    "2. Uploading the model and its metadata to Vertex Models.\n",
    "3. Building a custom container derived from NVIDIA NGC Merlin inference image.\n",
    "4. Deploy the model to Vertex AI Prediction.\n",
    "5. Getting the inference on a sample data points using the endpoint.\n",
    "\n",
    "## Triton Inference Server Overview\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides an inferencing solution optimized for both CPUs and GPUs. Triton can run multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding.It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensemble for use cases that require multiple models to perform end-to-end inference.\n",
    "\n",
    "At a high-level, the Triton Inference Server high-level architecture works as follows:\n",
    "- The model repository is a file-system based repository of the models that Triton will make available for inferencing. \n",
    "- Inference requests arrive at the server via either HTTP/REST or gRPC or then routed to the appropriate per-model scheduler. \n",
    "- Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis.\n",
    "- The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs.\n",
    "\n",
    "Triton server provides readiness and liveness health endpoints, as well as utilization, throughput, and latency metrics, which enables the integration of Triton into deployment environment, such as Vertex AI Prediction.\n",
    "\n",
    "In this example, we use Triton to serve an ensemble model that contains data processing workflow and HugeCTR model trained on Criteo data. The model is deployed into Vertex AI Prediction. This is shown in the following figure:\n",
    "\n",
    "<img src=\"./images/triton-vertex.png\" alt=\"Triton Architecture\" style=\"width:70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a GCS compute region, and a GCP Bucket. \n",
    "You also set the locations of the saved NVTaubular workflow, created in [01-dataset-preprocessing.ipynb](01-dataset-preprocessing.ipynb) and the trained HugeCTR model, created in [02-model-training-hugectr.ipynb](02-model-training-hugectr.ipynb) notebook.\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from src.serving import export\n",
    "#from src import feature_utils\n",
    "from src.feature_utils import FeatureSpecs\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp' # Change to your project.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "STAGING_BUCKET = 'jk-merlin-staging' # Change to your bucket.\n",
    "MODEL_REPOSITORY_BUCKET = 'merlin_model_repository'\n",
    "LOCAL_WORKSPACE = '/home/jupyter/staging'\n",
    "\n",
    "MODEL_NAME = 'deepfm'\n",
    "MODEL_VERSION = 'v01'\n",
    "MODEL_DISPLAY_NAME = f'{MODEL_NAME}-{MODEL_VERSION}'\n",
    "MODEL_DESCRIPTION = 'HugeCTR DeepFM model'\n",
    "\n",
    "EXPORTED_MODELS_DIR = f'gs://{MODEL_REPOSITORY_BUCKET}/hugectr_models'\n",
    "\n",
    "IMAGE_NAME = 'triton-deploy-hugectr'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton'\n",
    "\n",
    "WORKFLOW_MODEL_DIR = \"gs://criteo-datasets/criteo_processed_parquet/workflow\" # Change to GCS path of the nvt workflow.\n",
    "HUGECTR_MODEL_DIR = \"gs://merlin-models/hugectr_deepfm_21.09\" # Change to GCS path of the hugectr trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e3f45-2bab-48fb-9b81-4e0e65635ece",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e0cca3-11e6-4454-af41-c5c632dbbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20334367-9ed8-4d31-a3d8-39d122413afb",
   "metadata": {},
   "source": [
    "### Build a custom prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb8160ca-74cc-427b-9a20-fbbdd23cbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: docker: command not found\n",
      "/bin/bash: docker: command not found\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME = 'triton-hugectr-inference'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"\n",
    "DOCKERFILE = 'src/Dockerfile.triton'\n",
    "\n",
    "! gcloud builds submit --timeout \"2h\" --tag {IMAGE_URI} {DOCKERFILE} --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9c710-537a-49c9-86d5-fae7e9f2b4be",
   "metadata": {},
   "source": [
    "## 1. Exporting the Triton ensemble model\n",
    "\n",
    "The Triton ensemble model consists of NVTabular preprocessing workflow HugeCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc4fc36f-621e-4471-97ac-0580d2bca48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(LOCAL_WORKSPACE):\n",
    "    shutil.rmtree(LOCAL_WORKSPACE)\n",
    "os.makedirs(LOCAL_WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61e8f2b3-bb59-49e9-a727-4c3953add12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C1.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C10.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C11.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C12.parquet...\n",
      "| [4 files][126.2 MiB/126.2 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C13.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C14.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C15.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C16.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C17.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C18.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C19.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C2.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C20.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C21.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C22.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C23.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C24.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C25.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C26.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C3.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C4.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C5.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C6.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C7.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C8.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/categories/unique.C9.parquet...\n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/metadata.json... \n",
      "Copying gs://criteo-datasets/criteo_processed_parquet/workflow/workflow.pkl...  \n",
      "- [28 files][289.7 MiB/289.7 MiB]                                               \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 28 objects/289.7 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r {WORKFLOW_MODEL_DIR} {LOCAL_WORKSPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1a66224-acc2-4d7c-b4b0-ca256acc4cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm.json...\n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_opt_sparse_0.model...   \n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_sparse_0.model/emb_vector...\n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_sparse_0.model/key...   \n",
      "\\ [4 files][  3.7 GiB/  3.7 GiB]  133.2 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm0_sparse_0.model/slot_id...\n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm_dense_0.model...         \n",
      "Copying gs://merlin-models/hugectr_deepfm_21.09/deepfm_opt_dense_0.model...     \n",
      "\\ [7 files][  4.2 GiB/  4.2 GiB]  164.5 MiB/s                                   \n",
      "Operation completed over 7 objects/4.2 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r {HUGECTR_MODEL_DIR} {LOCAL_WORKSPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb6237c1-aa71-408a-a6c6-c86e6e4155bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugectr_deepfm_21.09  workflow\n"
     ]
    }
   ],
   "source": [
    "!ls {LOCAL_WORKSPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f1d-2e9b-4035-b982-d4c289e67dec",
   "metadata": {},
   "source": [
    "### Exporting the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76abc820-3603-4b8b-9dec-a22c07d31d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_specs = FeatureSpecs()\n",
    "local_workflow_path = Path(LOCAL_WORKSPACE) / Path(WORKFLOW_MODEL_DIR).parts[-1]\n",
    "local_saved_model_path = Path(LOCAL_WORKSPACE) / Path(HUGECTR_MODEL_DIR).parts[-1]\n",
    "local_exported_ensemble_path = Path(LOCAL_WORKSPACE) / f'triton-ensemble-{time.strftime(\"%Y%m%d%H%M%S\")}'\n",
    "\n",
    "export.export_ensemble(\n",
    "    workflow_path=local_workflow_path,\n",
    "    saved_model_path=local_saved_model_path,\n",
    "    output_path=local_exported_ensemble_path,\n",
    "    categorical_columns=feature_specs.categorical_columns,\n",
    "    continuous_columns=feature_specs.continuous_columns,\n",
    "    label_columns=feature_specs.label_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e252c2a-8fd8-4962-a9f1-d2682cb3e718",
   "metadata": {},
   "source": [
    "## 2. Uploading the model and its metadata to Vertex Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f23f6fb2-c38b-4de9-b97e-9a3108c926bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/1/deepfm.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/1/deepfm0_opt_sparse_0.model [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/1/deepfm_opt_dense_0.model [Content-Type=application/octet-stream]...\n",
      "/ [4 files][  6.2 KiB/  6.2 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/1/deepfm_dense_0.model [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/1/deepfm0_sparse_0.model/key [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/1/deepfm0_sparse_0.model/slot_id [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm/1/deepfm0_sparse_0.model/emb_vector [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_ens/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/model.py [Content-Type=text/x-python]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/column_types.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/workflow.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/metadata.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C13.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C6.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C10.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C12.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C23.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C16.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C21.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C4.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C22.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C11.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C19.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C5.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C25.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C7.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C3.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C14.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C1.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C17.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C24.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C26.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C15.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C18.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C9.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C20.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C8.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/deepfm_nvt/1/workflow/categories/unique.C2.parquet [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/staging/triton-ensemble-20211028203607/ps.json [Content-Type=application/json]...\n",
      "| [41 files][  4.5 GiB/  4.5 GiB]   47.6 MiB/s                                  \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 41 objects/4.5 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "gcs_exported_ensemble_path = '{}/{}'.format(EXPORTED_MODELS_DIR, Path(local_exported_ensemble_path).parts[-1])\n",
    "\n",
    "!gsutil cp -r {local_exported_ensemble_path}/* {gcs_exported_ensemble_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9e3c2ac-90fb-4175-b185-d2981cbdebf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Image gcr.io/merlin-on-gcp/triton-deploy-hugectr not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"Image gcr.io/merlin-on-gcp/triton-deploy-hugectr not found\"\n\tdebug_error_string = \"{\"created\":\"@1635454543.693540584\",\"description\":\"Error received from peer ipv4:74.125.69.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1069,\"grpc_message\":\"Image gcr.io/merlin-on-gcp/triton-deploy-hugectr not found\",\"grpc_status\":5}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/3682196627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = vertex_ai.Model.upload(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_DISPLAY_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_DESCRIPTION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(cls, display_name, serving_container_image_uri, artifact_uri, serving_container_predict_route, serving_container_health_route, description, serving_container_command, serving_container_args, serving_container_environment_variables, serving_container_ports, instance_schema_uri, parameters_schema_uri, prediction_schema_uri, explanation_metadata, explanation_parameters, project, location, credentials, labels, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0mmanaged_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplanation_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplanation_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m         lro = api_client.upload_model(\n\u001b[0m\u001b[1;32m   1676\u001b[0m             \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_location_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanaged_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform_v1/services/model_service/client.py\u001b[0m in \u001b[0;36mupload_model\u001b[0;34m(self, request, parent, model, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Wrap the response in an operation future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 Image gcr.io/merlin-on-gcp/triton-deploy-hugectr not found"
     ]
    }
   ],
   "source": [
    "health_route = \"/v2/health/ready\"\n",
    "predict_route = f\"/v2/models/{MODEL_NAME}_ens/infer\"\n",
    "serving_container_ports = [8000]\n",
    "in_container_model_repository = '/models'\n",
    "serving_container_args = [in_container_model_repository]\n",
    "\n",
    "\n",
    "model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    description=MODEL_DESCRIPTION,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    "    artifact_uri=model_ensemble_location,\n",
    "    serving_container_args=serving_container_args,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c89562-ea7e-4ad0-a601-7a504002f1ac",
   "metadata": {},
   "source": [
    "## 3. Building a custom container derived from NVIDIA NGC Merlin inference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9672b9b-9e73-471b-8600-a30660432be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --timeout \"2h\" --tag {IMAGE_URI} {DOCKERFILE} --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c572a7c-c5e5-4383-bbd5-ce11b6659945",
   "metadata": {},
   "source": [
    "## 4. Deploy the model to Vertex AI Prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the Vertex Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.Endpoint.create(\n",
    "    display_name=ENDPOINT_DISPLAY_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Deploy the model to Vertex Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd27dda-7c1b-4ce5-9477-e62a9f1199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "## 5. Getting the inference on a sample data points using hte endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50394a33-e4b3-4fe8-950f-1cbd76f9b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.serving import inference\n",
    "from src import feature_utils\n",
    "\n",
    "is_binary = False\n",
    "\n",
    "data = {\n",
    "    'I1': [5, 32, 0], \n",
    "    'I2': [110, 3, 233], \n",
    "    'I3': [0, 5, 1], \n",
    "    'I4': [16, 0, 146], \n",
    "    'I5': [0, 1, 1], \n",
    "    'I6': [1, 0, 0], \n",
    "    'I7': [0, 0, 0], \n",
    "    'I8': [14, 61, 99], \n",
    "    'I9': [7, 5, 7], \n",
    "    'I10': [1, 0, 0], \n",
    "    'I11': [0, 1, 1], \n",
    "    'I12': [306, 3157, 3101], \n",
    "    'I13': [0, 5, 1], \n",
    "    'C1': [1651969401, -436994675, 1651969401], \n",
    "    'C2': [-501260968, -1599406170, -1382530557], \n",
    "    'C3': [-1343601617, 1873417685, 1656669709], \n",
    "    'C4': [-1805877297, -628476895, 946620910], \n",
    "    'C5': [951068488, 1020698403, -413858227], \n",
    "    'C6': [1875733963, 1875733963, 1875733963], \n",
    "    'C7': [897624609, -1424560767, -1242174622], \n",
    "    'C8': [679512323, 1128426537, -772617077], \n",
    "    'C9': [1189011366, 502653268, 776897055], \n",
    "    'C10': [771915201, 2112471209, 771915201], \n",
    "    'C11': [209470001, 1716706404, 209470001], \n",
    "    'C12': [-1785193185, -1712632281, 309420420], \n",
    "    'C13': [12976055, 12976055, 12976055], \n",
    "    'C14': [-1102125769, -1102125769, -1102125769], \n",
    "    'C15': [-1978960692, -205783399, -150008565], \n",
    "    'C16': [1289502458, 1289502458, 1289502458], \n",
    "    'C17': [-771205462, -771205462, -771205462], \n",
    "    'C18': [-1206449222, -1578429167, 1653545869], \n",
    "    'C19': [-1793932789, -1793932789, -1793932789], \n",
    "    'C20': [-1014091992, -20981661, -1014091992], \n",
    "    'C21': [351689309, -1556988767, 351689309], \n",
    "    'C22': [632402057, -924717482, 632402057], \n",
    "    'C23': [-675152885, 391309800, -675152885], \n",
    "    'C24': [2091868316, 1966410890, 883538181], \n",
    "    'C25': [809724924, -1726799382, -10139646], \n",
    "    'C26': [-317696227, -1218975401, -317696227]\n",
    "}\n",
    "\n",
    "inputs = inference.get_inference_input(data, is_binary)\n",
    "\n",
    "# Greating the request_body to be sent to the inference request  \n",
    "if is_binary:\n",
    "    request_body, json_size = inference.get_inference_request(inputs, '1')\n",
    "    with open('criteo.dat', 'wb') as output_file:\n",
    "        output_file.write(request_body)\n",
    "else:\n",
    "    infer_request, request_body, json_size = inference.get_inference_request(inputs, '1')\n",
    "    json_obj = json.loads(request_body)\n",
    "    with open('criteo.json', 'w') as output_file:\n",
    "        json.dump(json_obj, output_file)\n",
    "         \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee823b73-0540-4f13-af88-508334004d07",
   "metadata": {},
   "source": [
    "### Getting inference for a json input using curl command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316dbbd-7dc4-4e93-9b39-6c18537353c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s  $PROJECT_ID $REGION $ENDPOINT_DISPLAY_NAME\n",
    "\n",
    "PROJECT_ID=$1\n",
    "REGION=$2\n",
    "endpoint_display_name=$3\n",
    "\n",
    "# get endpoint id\n",
    "echo \"REGION = ${REGION}\"\n",
    "echo \"ENDPOINT DISPLAY NAME = ${ENDPOINT_DISPLAY_NAME}\"\n",
    "ENDPOINT_ID=$(gcloud beta ai endpoints list --region ${REGION} --filter \"display_name=${ENDPOINT_DISPLAY_NAME}\" --format \"value(ENDPOINT_ID)\")\n",
    "echo \"ENDPOINT_ID = ${ENDPOINT_ID}\"\n",
    "\n",
    "curl -X POST \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json\"  \\\n",
    "  https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/endpoints/${ENDPOINT_ID}:rawPredict \\\n",
    "  -d @criteo.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m82",
   "type": "gcloud",
   "uri": "gcr.io/merlin-on-gcp/merlin-dev-vertex:latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
